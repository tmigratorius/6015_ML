{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Feasture Scaling\n",
    "\n",
    "1. Rescaling Data\n",
    "2. Normalizing Data\n",
    "3. Standardizing Data\n",
    "\n",
    "[More Information](https://machinelearningmastery.com/prepare-data-machine-learning-python-scikit-learn/)\n",
    "\n",
    "\n",
    "[Data Description](https://www.kaggle.com/uciml/pima-indians-diabetes-database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='red'>numerical inputs only. Have to code in data (no categories), Then need to decode on the back side. Each row would have a category row with all options. Applicable cat is 1 and non-applicables are 0. If there are three categories, there are 3 colums, but only one would ever have a 1 in it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.353 0.744 0.59  0.354 0.    0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 0.293 0.    0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 0.    0.    0.347 0.254 0.183]\n",
      " [0.059 0.447 0.541 0.232 0.111 0.419 0.038 0.   ]\n",
      " [0.    0.688 0.328 0.354 0.199 0.642 0.944 0.2  ]]\n"
     ]
    }
   ],
   "source": [
    "# Rescale data (between 0 and 1)\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataframe = pandas.read_csv(\"./files/prims.csv\")\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,:8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX=scaler.fit_transform(X)\n",
    "\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>min max scaler makes every value in set between the range. why is that important? Disadvantes: may loose some information, scaling may cause all values to be in one cluster because they are so tiny and close. Scaling with clustering can be problematic</font>\n",
    "\n",
    "### <font color='red'>also, scaling happens on the training data, not on all the data, and the scaled values in the test set may be different.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 0.196 0.    0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 0.244 0.    0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 0.    0.    0.118 0.003 0.162]\n",
      " [0.007 0.588 0.436 0.152 0.622 0.186 0.001 0.139]\n",
      " [0.    0.596 0.174 0.152 0.731 0.188 0.01  0.144]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data (length of 1)\n",
    "#Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra).\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "dataframe = pandas.read_csv(\"./files/prims.csv\")\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.646  0.853  0.16   0.912 -0.692  0.215  0.448  1.433]\n",
      " [-0.841 -1.104 -0.147  0.538 -0.692 -0.666 -0.371 -0.18 ]\n",
      " [ 1.241  1.941 -0.25  -1.272 -0.692 -1.081  0.581 -0.095]\n",
      " [-0.841 -0.98  -0.147  0.163  0.122 -0.477 -0.917 -1.03 ]\n",
      " [-1.138  0.511 -1.479  0.912  0.763  1.411  5.375 -0.011]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "\n",
      "[[-0.543  0.045  0.365  0.413  1.04   0.505  0.021 -0.605]\n",
      " [ 0.646  0.139  0.467  0.663 -0.692 -0.54   0.264  1.348]\n",
      " [-0.841  1.475  0.979  0.538 -0.692  0.391  1.273  1.603]\n",
      " [-0.543  0.263 -3.527 -1.272 -0.692  0.832 -0.51   0.669]\n",
      " [ 0.051 -0.328  0.365 -0.024  0.174 -0.439 -1.062 -0.52 ]]\n",
      "[1.016 0.907 0.879 0.937 0.966 0.892 0.764 0.971]\n"
     ]
    }
   ],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "# Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "\"\"\"\n",
    "scaled_train =  (train - train_mean) / train_std_deviation\n",
    "scaled_test = (test - train_mean) / train_std_deviation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "dataframe = pandas.read_csv(\"./files/prims.csv\")\n",
    "array = dataframe.values\n",
    "\n",
    " \n",
    "X = array[:700,:8]\n",
    " \n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "\n",
    "std = numpy.std(rescaledX,axis=0)\n",
    "print(std)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "test=scaler.transform(array[700:,:8])\n",
    "\n",
    "print(test[0:5,:])\n",
    "\n",
    "std = numpy.std(test,axis=0)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n",
    "## Feedforward\n",
    "\n",
    "[Code](https://enlight.nyc/projects/neural-network/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.192]\n",
      " [0.216]\n",
      " [0.204]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X = (hours sleeping, hours studying), y = score on test\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "\n",
    "# scale units\n",
    "X = X/np.amax(X, axis=0) # maximum of X array\n",
    "y = y/100 # max test score is 100\n",
    "\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "    #parameters\n",
    "    #2 input values\n",
    "    self.inputSize = 2\n",
    "    #1 output var, score\n",
    "    self.outputSize = 1\n",
    "    #3 nodes\n",
    "    self.hiddenSize = 3\n",
    "\n",
    "    #weights\n",
    "    #initialize weights randomly, is a matrix\n",
    "    self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
    "    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
    "\n",
    "  def forward(self, X):\n",
    "    #forward propagation through our network\n",
    "    self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "    self.z2 = self.sigmoid(self.z) # activation function\n",
    "    self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "    o = self.sigmoid(self.z3) # final activation function\n",
    "    return o\n",
    "\n",
    "  def sigmoid(self, s):\n",
    "    # activation function\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "NN = Neural_Network()\n",
    "\n",
    "#defining our output\n",
    "o = NN.forward(X)\n",
    "\n",
    "print(\"Predicted Output: \\n\" + str(o))\n",
    "print (\"Actual Output: \\n\" + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n",
    "## Feedforward and Backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.598]\n",
      " [0.633]\n",
      " [0.598]]\n",
      "Loss: \n",
      "0.08013009871075595\n",
      "\n",
      "\n",
      "# 1\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.633]\n",
      " [0.664]\n",
      " [0.633]]\n",
      "Loss: \n",
      "0.06230983870082934\n",
      "\n",
      "\n",
      "# 2\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.662]\n",
      " [0.69 ]\n",
      " [0.663]]\n",
      "Loss: \n",
      "0.049021730757290154\n",
      "\n",
      "\n",
      "# 3\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.686]\n",
      " [0.712]\n",
      " [0.688]]\n",
      "Loss: \n",
      "0.03903490551930175\n",
      "\n",
      "\n",
      "# 4\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.708]\n",
      " [0.73 ]\n",
      " [0.71 ]]\n",
      "Loss: \n",
      "0.031456141566098106\n",
      "\n",
      "\n",
      "# 5\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.726]\n",
      " [0.746]\n",
      " [0.728]]\n",
      "Loss: \n",
      "0.02564289518319218\n",
      "\n",
      "\n",
      "# 6\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.741]\n",
      " [0.759]\n",
      " [0.744]]\n",
      "Loss: \n",
      "0.02113386486844149\n",
      "\n",
      "\n",
      "# 7\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.755]\n",
      " [0.771]\n",
      " [0.758]]\n",
      "Loss: \n",
      "0.01759712358418009\n",
      "\n",
      "\n",
      "# 8\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.766]\n",
      " [0.781]\n",
      " [0.77 ]]\n",
      "Loss: \n",
      "0.014792581749226116\n",
      "\n",
      "\n",
      "# 9\n",
      "\n",
      "Input (scaled): \n",
      "[[0.667 1.   ]\n",
      " [0.333 0.556]\n",
      " [1.    0.667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.777]\n",
      " [0.789]\n",
      " [0.78 ]]\n",
      "Loss: \n",
      "0.012545296686582824\n",
      "\n",
      "\n",
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "[0.5 1. ]\n",
      "Output: \n",
      "[0.787]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#this is the test data\n",
    "# X = (hours studying, hours sleeping), y = score on test, xPredicted = 4 hours studying & 8 hours sleeping (input data for prediction)\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "xPredicted = np.array(([4,8]), dtype=float)\n",
    "\n",
    "# scale units\n",
    "X = X/np.amax(X, axis=0) # maximum of X array\n",
    "xPredicted = xPredicted/np.amax(xPredicted, axis=0) # maximum of xPredicted (our input data for the prediction)\n",
    "y = y/100 # max test score is 100\n",
    "\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "  #parameters\n",
    "    self.inputSize = 2\n",
    "    self.outputSize = 1\n",
    "    self.hiddenSize = 3\n",
    "\n",
    "  #weights\n",
    "    self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
    "    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
    "\n",
    "  def forward(self, X):\n",
    "    #forward propagation through our network\n",
    "    self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "    self.z2 = self.sigmoid(self.z) # activation function\n",
    "    self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "    o = self.sigmoid(self.z3) # final activation function\n",
    "    return o\n",
    "\n",
    "  def sigmoid(self, s):\n",
    "    # activation function\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "  def sigmoidPrime(self, s):\n",
    "    #derivative of sigmoid\n",
    "    return s * (1 - s)\n",
    "\n",
    "  def backward(self, X, y, o):\n",
    "    # backward propagate through the network\n",
    "    self.o_error = y - o # error in output\n",
    "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "    self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "    self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "    self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "\n",
    "  def train(self, X, y):\n",
    "    o = self.forward(X)\n",
    "    self.backward(X, y, o)\n",
    "\n",
    "  def saveWeights(self):\n",
    "    np.savetxt(\"w1.txt\", self.W1, fmt=\"%s\")\n",
    "    np.savetxt(\"w2.txt\", self.W2, fmt=\"%s\")\n",
    "\n",
    "  def predict(self):\n",
    "    print (\"Predicted data based on trained weights: \")\n",
    "    print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "    print (\"Output: \\n\" + str(self.forward(xPredicted)));\n",
    "\n",
    "NN = Neural_Network()\n",
    "for i in range(0,10): # trains the NN 1,000 times\n",
    "  print(\"# \" + str(i) + \"\\n\")\n",
    "  print(\"Input (scaled): \\n\" + str(X))\n",
    "  print(\"Actual Output: \\n\" + str(y))\n",
    "  print(\"Predicted Output: \\n\" + str(NN.forward(X)))\n",
    "  print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n",
    "  print (\"\\n\")\n",
    "  NN.train(X, y)\n",
    "\n",
    "NN.saveWeights()\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPClassifier in module sklearn.neural_network.multilayer_perceptron:\n",
      "\n",
      "class MLPClassifier(BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)\n",
      " |  Multi-layer Perceptron classifier.\n",
      " |  \n",
      " |  This model optimizes the log-loss function using LBFGS or stochastic\n",
      " |  gradient descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
      " |        by Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, optional, default 0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, optional, default 'auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      " |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when ``solver='sgd'``.\n",
      " |  \n",
      " |  learning_rate_init : double, optional, default 0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : double, optional, default 0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, optional, default 200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, optional, default True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least tol for two consecutive iterations, unless `learning_rate`\n",
      " |      is set to 'adaptive', convergence is considered to be reached and\n",
      " |      training stops.\n",
      " |  \n",
      " |  verbose : bool, optional, default False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, optional, default False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution.\n",
      " |  \n",
      " |  momentum : float, default 0.9\n",
      " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : boolean, default True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for two consecutive\n",
      " |      epochs.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True\n",
      " |  \n",
      " |  beta_1 : float, optional, default 0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  beta_2 : float, optional, default 0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  epsilon : float, optional, default 1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array or list of array of shape (n_classes,)\n",
      " |      Class labels for each output.\n",
      " |  \n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  coefs_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_iter_ : int,\n",
      " |      The number of iterations the solver has ran.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : string\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPClassifier trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays or\n",
      " |  sparse scipy arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPClassifier\n",
      " |      BaseMultilayerPerceptron\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_classes)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return the log of probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_y_prob : array-like, shape (n_samples, n_classes)\n",
      " |          The predicted log-probability of the sample for each class\n",
      " |          in the model, where classes are ordered as they are in\n",
      " |          `self.classes_`. Equivalent to log(predict_proba(X))\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_prob : array-like, shape (n_samples, n_classes)\n",
      " |          The predicted probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in `self.classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  partial_fit\n",
      " |      Fit the model to data matrix X and target y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      classes : array, shape (n_classes)\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "help(MLPClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>what is the advantage or disadvantage when num iter is small or large</font>\n",
    "\n",
    "### <font color='red'>small num of iterations, fast but may not converge</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69225859\n",
      "Iteration 2, loss = 0.68286383\n",
      "Iteration 3, loss = 0.67564925\n",
      "Iteration 4, loss = 0.66941684\n",
      "Iteration 5, loss = 0.66401914\n",
      "Iteration 6, loss = 0.66004404\n",
      "Iteration 7, loss = 0.65619876\n",
      "Iteration 8, loss = 0.65268342\n",
      "Iteration 9, loss = 0.64974644\n",
      "Iteration 10, loss = 0.64698598\n",
      "Iteration 11, loss = 0.64431285\n",
      "Iteration 12, loss = 0.64173960\n",
      "Iteration 13, loss = 0.63922889\n",
      "Iteration 14, loss = 0.63679691\n",
      "Iteration 15, loss = 0.63425251\n",
      "Iteration 16, loss = 0.63186714\n",
      "Iteration 17, loss = 0.62935345\n",
      "Iteration 18, loss = 0.62690308\n",
      "Iteration 19, loss = 0.62411275\n",
      "Iteration 20, loss = 0.62153795\n",
      "Iteration 21, loss = 0.61873835\n",
      "Iteration 22, loss = 0.61591680\n",
      "Iteration 23, loss = 0.61303832\n",
      "Iteration 24, loss = 0.61015721\n",
      "Iteration 25, loss = 0.60733694\n",
      "Iteration 26, loss = 0.60470102\n",
      "Iteration 27, loss = 0.60239766\n",
      "Iteration 28, loss = 0.59998877\n",
      "Iteration 29, loss = 0.59782174\n",
      "Iteration 30, loss = 0.59573785\n",
      "Iteration 31, loss = 0.59376595\n",
      "Iteration 32, loss = 0.59211323\n",
      "Iteration 33, loss = 0.59001078\n",
      "Iteration 34, loss = 0.58813692\n",
      "Iteration 35, loss = 0.58615512\n",
      "Iteration 36, loss = 0.58434587\n",
      "Iteration 37, loss = 0.58265595\n",
      "Iteration 38, loss = 0.58073111\n",
      "Iteration 39, loss = 0.57909680\n",
      "Iteration 40, loss = 0.57709015\n",
      "Iteration 41, loss = 0.57547678\n",
      "Iteration 42, loss = 0.57361414\n",
      "Iteration 43, loss = 0.57196319\n",
      "Iteration 44, loss = 0.57040082\n",
      "Iteration 45, loss = 0.56871326\n",
      "Iteration 46, loss = 0.56717427\n",
      "Iteration 47, loss = 0.56551319\n",
      "Iteration 48, loss = 0.56401813\n",
      "Iteration 49, loss = 0.56248878\n",
      "Iteration 50, loss = 0.56097352\n",
      "Iteration 51, loss = 0.55947382\n",
      "Iteration 52, loss = 0.55800981\n",
      "Iteration 53, loss = 0.55671929\n",
      "Iteration 54, loss = 0.55516038\n",
      "Iteration 55, loss = 0.55383384\n",
      "Iteration 56, loss = 0.55244165\n",
      "Iteration 57, loss = 0.55114551\n",
      "Iteration 58, loss = 0.54988338\n",
      "Iteration 59, loss = 0.54865214\n",
      "Iteration 60, loss = 0.54744167\n",
      "Iteration 61, loss = 0.54597220\n",
      "Iteration 62, loss = 0.54478675\n",
      "Iteration 63, loss = 0.54356295\n",
      "Iteration 64, loss = 0.54239272\n",
      "Iteration 65, loss = 0.54119360\n",
      "Iteration 66, loss = 0.54015023\n",
      "Iteration 67, loss = 0.53914195\n",
      "Iteration 68, loss = 0.53786330\n",
      "Iteration 69, loss = 0.53687967\n",
      "Iteration 70, loss = 0.53583072\n",
      "Iteration 71, loss = 0.53473390\n",
      "Iteration 72, loss = 0.53375777\n",
      "Iteration 73, loss = 0.53275281\n",
      "Iteration 74, loss = 0.53187432\n",
      "Iteration 75, loss = 0.53089048\n",
      "Iteration 76, loss = 0.52987853\n",
      "Iteration 77, loss = 0.52876701\n",
      "Iteration 78, loss = 0.52787622\n",
      "Iteration 79, loss = 0.52693827\n",
      "Iteration 80, loss = 0.52611279\n",
      "Iteration 81, loss = 0.52508624\n",
      "Iteration 82, loss = 0.52422805\n",
      "Iteration 83, loss = 0.52334383\n",
      "Iteration 84, loss = 0.52247765\n",
      "Iteration 85, loss = 0.52150410\n",
      "Iteration 86, loss = 0.52061613\n",
      "Iteration 87, loss = 0.51981645\n",
      "Iteration 88, loss = 0.51916807\n",
      "Iteration 89, loss = 0.51811986\n",
      "Iteration 90, loss = 0.51721264\n",
      "Iteration 91, loss = 0.51634883\n",
      "Iteration 92, loss = 0.51550605\n",
      "Iteration 93, loss = 0.51467773\n",
      "Iteration 94, loss = 0.51384576\n",
      "Iteration 95, loss = 0.51311184\n",
      "Iteration 96, loss = 0.51226965\n",
      "Iteration 97, loss = 0.51146854\n",
      "Iteration 98, loss = 0.51057985\n",
      "Iteration 99, loss = 0.50987077\n",
      "Iteration 100, loss = 0.50902887\n",
      "Iteration 101, loss = 0.50828216\n",
      "Iteration 102, loss = 0.50744591\n",
      "Iteration 103, loss = 0.50669901\n",
      "Iteration 104, loss = 0.50604421\n",
      "Iteration 105, loss = 0.50526205\n",
      "Iteration 106, loss = 0.50457172\n",
      "Iteration 107, loss = 0.50383729\n",
      "Iteration 108, loss = 0.50306276\n",
      "Iteration 109, loss = 0.50237849\n",
      "Iteration 110, loss = 0.50197427\n",
      "Iteration 111, loss = 0.50125721\n",
      "Iteration 112, loss = 0.50042321\n",
      "Iteration 113, loss = 0.49959835\n",
      "Iteration 114, loss = 0.49883649\n",
      "Iteration 115, loss = 0.49811503\n",
      "Iteration 116, loss = 0.49712047\n",
      "Iteration 117, loss = 0.49678081\n",
      "Iteration 118, loss = 0.49621683\n",
      "Iteration 119, loss = 0.49519641\n",
      "Iteration 120, loss = 0.49458220\n",
      "Iteration 121, loss = 0.49382379\n",
      "Iteration 122, loss = 0.49318897\n",
      "Iteration 123, loss = 0.49253116\n",
      "Iteration 124, loss = 0.49194417\n",
      "Iteration 125, loss = 0.49123320\n",
      "Iteration 126, loss = 0.49097438\n",
      "Iteration 127, loss = 0.49026243\n",
      "Iteration 128, loss = 0.48966675\n",
      "Iteration 129, loss = 0.48895449\n",
      "Iteration 130, loss = 0.48839765\n",
      "Iteration 131, loss = 0.48768157\n",
      "Iteration 132, loss = 0.48743613\n",
      "Iteration 133, loss = 0.48685266\n",
      "Iteration 134, loss = 0.48621033\n",
      "Iteration 135, loss = 0.48580916\n",
      "Iteration 136, loss = 0.48513393\n",
      "Iteration 137, loss = 0.48461708\n",
      "Iteration 138, loss = 0.48423711\n",
      "Iteration 139, loss = 0.48351635\n",
      "Iteration 140, loss = 0.48315703\n",
      "Iteration 141, loss = 0.48245629\n",
      "Iteration 142, loss = 0.48196230\n",
      "Iteration 143, loss = 0.48163177\n",
      "Iteration 144, loss = 0.48114423\n",
      "Iteration 145, loss = 0.48042696\n",
      "Iteration 146, loss = 0.48024942\n",
      "Iteration 147, loss = 0.47979875\n",
      "Iteration 148, loss = 0.47943441\n",
      "Iteration 149, loss = 0.47918742\n",
      "Iteration 150, loss = 0.47823566\n",
      "Iteration 151, loss = 0.47785567\n",
      "Iteration 152, loss = 0.47724345\n",
      "Iteration 153, loss = 0.47685757\n",
      "Iteration 154, loss = 0.47634188\n",
      "Iteration 155, loss = 0.47617390\n",
      "Iteration 156, loss = 0.47569148\n",
      "Iteration 157, loss = 0.47549644\n",
      "Iteration 158, loss = 0.47516032\n",
      "Iteration 159, loss = 0.47459239\n",
      "Iteration 160, loss = 0.47426290\n",
      "Iteration 161, loss = 0.47377252\n",
      "Iteration 162, loss = 0.47348595\n",
      "Iteration 163, loss = 0.47298732\n",
      "Iteration 164, loss = 0.47288676\n",
      "Iteration 165, loss = 0.47263148\n",
      "Iteration 166, loss = 0.47204165\n",
      "Iteration 167, loss = 0.47159980\n",
      "Iteration 168, loss = 0.47128761\n",
      "Iteration 169, loss = 0.47108203\n",
      "Iteration 170, loss = 0.47066679\n",
      "Iteration 171, loss = 0.47041136\n",
      "Iteration 172, loss = 0.47000112\n",
      "Iteration 173, loss = 0.46980145\n",
      "Iteration 174, loss = 0.46960149\n",
      "Iteration 175, loss = 0.46922246\n",
      "Iteration 176, loss = 0.46902738\n",
      "Iteration 177, loss = 0.46892669\n",
      "Iteration 178, loss = 0.46842897\n",
      "Iteration 179, loss = 0.46836653\n",
      "Iteration 180, loss = 0.46770885\n",
      "Iteration 181, loss = 0.46740985\n",
      "Iteration 182, loss = 0.46769289\n",
      "Iteration 183, loss = 0.46707656\n",
      "Iteration 184, loss = 0.46662271\n",
      "Iteration 185, loss = 0.46638076\n",
      "Iteration 186, loss = 0.46621965\n",
      "Iteration 187, loss = 0.46600801\n",
      "Iteration 188, loss = 0.46567952\n",
      "Iteration 189, loss = 0.46543345\n",
      "Iteration 190, loss = 0.46524097\n",
      "Iteration 191, loss = 0.46506568\n",
      "Iteration 192, loss = 0.46484214\n",
      "Iteration 193, loss = 0.46465235\n",
      "Iteration 194, loss = 0.46426545\n",
      "Iteration 195, loss = 0.46415589\n",
      "Iteration 196, loss = 0.46408837\n",
      "Iteration 197, loss = 0.46380908\n",
      "Iteration 198, loss = 0.46347328\n",
      "Iteration 199, loss = 0.46318547\n",
      "Iteration 200, loss = 0.46316388\n",
      "Iteration 201, loss = 0.46305083\n",
      "Iteration 202, loss = 0.46281926\n",
      "Iteration 203, loss = 0.46237775\n",
      "Iteration 204, loss = 0.46233476\n",
      "Iteration 205, loss = 0.46191951\n",
      "Iteration 206, loss = 0.46166659\n",
      "Iteration 207, loss = 0.46154500\n",
      "Iteration 208, loss = 0.46149260\n",
      "Iteration 209, loss = 0.46139699\n",
      "Iteration 210, loss = 0.46146017\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Training set score: 0.785714\n",
      "Test set score: 0.794118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "dataframe = pandas.read_csv(\"./files/prims.csv\")\n",
    "array = dataframe.values\n",
    "X=array[:,:8]\n",
    "y = array[:,8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X_train, X_test = X[:700], X[700:]\n",
    "y_train, y_test = y[:700], y[700:]\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10000, alpha=1e-4,\n",
    "                    solver='adam', verbose=10, tol=1e-4, random_state=1)\n",
    "#mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "#                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "#                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the above code, play on the hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>any classifier can be made into a regressor</font>\n",
    "\n",
    "### <font color='red'>output comes with ranges instead of binary/classes. With regression, there are thresholds, because the answer is numeric, not classed, it has ranges</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPRegressor in module sklearn.neural_network.multilayer_perceptron:\n",
      "\n",
      "class MLPRegressor(BaseMultilayerPerceptron, sklearn.base.RegressorMixin)\n",
      " |  Multi-layer Perceptron regressor.\n",
      " |  \n",
      " |  This model optimizes the squared-loss using LBFGS or stochastic gradient\n",
      " |  descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
      " |        Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, optional, default 0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, optional, default 'auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      " |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when solver='sgd'.\n",
      " |  \n",
      " |  learning_rate_init : double, optional, default 0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : double, optional, default 0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, optional, default 200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, optional, default True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least tol for two consecutive iterations, unless `learning_rate`\n",
      " |      is set to 'adaptive', convergence is considered to be reached and\n",
      " |      training stops.\n",
      " |  \n",
      " |  verbose : bool, optional, default False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, optional, default False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution.\n",
      " |  \n",
      " |  momentum : float, default 0.9\n",
      " |      Momentum for gradient descent update.  Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : boolean, default True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for two consecutive\n",
      " |      epochs.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True\n",
      " |  \n",
      " |  beta_1 : float, optional, default 0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  beta_2 : float, optional, default 0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  epsilon : float, optional, default 1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  coefs_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_iter_ : int,\n",
      " |      The number of iterations the solver has ran.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : string\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPRegressor trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense and sparse numpy\n",
      " |  arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPRegressor\n",
      " |      BaseMultilayerPerceptron\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like, shape (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  partial_fit\n",
      " |      Fit the model to data matrix X and target y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "help(MLPRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>must put seed if not, there are different outpust every time. Don't forget the seed</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4848.52190857\n",
      "Iteration 2, loss = 4843.89158197\n",
      "Iteration 3, loss = 4839.23966432\n",
      "Iteration 4, loss = 4834.64868045\n",
      "Iteration 5, loss = 4830.04038263\n",
      "Iteration 6, loss = 4825.46677005\n",
      "Iteration 7, loss = 4820.92877977\n",
      "Iteration 8, loss = 4816.43800048\n",
      "Iteration 9, loss = 4811.91425205\n",
      "Iteration 10, loss = 4807.38578216\n",
      "Iteration 11, loss = 4802.92887248\n",
      "Iteration 12, loss = 4798.42461852\n",
      "Iteration 13, loss = 4793.90009629\n",
      "Iteration 14, loss = 4789.34307257\n",
      "Iteration 15, loss = 4784.79121397\n",
      "Iteration 16, loss = 4780.15660823\n",
      "Iteration 17, loss = 4775.49576785\n",
      "Iteration 18, loss = 4770.75743582\n",
      "Iteration 19, loss = 4765.94968190\n",
      "Iteration 20, loss = 4761.07802531\n",
      "Iteration 21, loss = 4756.25792219\n",
      "Iteration 22, loss = 4751.42509539\n",
      "Iteration 23, loss = 4746.55002318\n",
      "Iteration 24, loss = 4741.64422567\n",
      "Iteration 25, loss = 4736.75596641\n",
      "Iteration 26, loss = 4731.91045983\n",
      "Iteration 27, loss = 4726.82709834\n",
      "Iteration 28, loss = 4721.73534818\n",
      "Iteration 29, loss = 4716.43143436\n",
      "Iteration 30, loss = 4711.10801334\n",
      "Iteration 31, loss = 4705.78253791\n",
      "Iteration 32, loss = 4700.40458315\n",
      "Iteration 33, loss = 4695.02753921\n",
      "Iteration 34, loss = 4689.70945740\n",
      "Iteration 35, loss = 4684.50494436\n",
      "Iteration 36, loss = 4679.19268524\n",
      "Iteration 37, loss = 4673.89911814\n",
      "Iteration 38, loss = 4668.59015926\n",
      "Iteration 39, loss = 4663.22215818\n",
      "Iteration 40, loss = 4657.75412141\n",
      "Iteration 41, loss = 4652.30027672\n",
      "Iteration 42, loss = 4646.71581002\n",
      "Iteration 43, loss = 4641.05467136\n",
      "Iteration 44, loss = 4635.35269828\n",
      "Iteration 45, loss = 4629.53138007\n",
      "Iteration 46, loss = 4623.81772458\n",
      "Iteration 47, loss = 4618.01696374\n",
      "Iteration 48, loss = 4612.26239715\n",
      "Iteration 49, loss = 4606.39625689\n",
      "Iteration 50, loss = 4600.46930502\n",
      "Iteration 51, loss = 4594.52401577\n",
      "Iteration 52, loss = 4588.45929601\n",
      "Iteration 53, loss = 4582.26663806\n",
      "Iteration 54, loss = 4575.97208723\n",
      "Iteration 55, loss = 4569.51247885\n",
      "Iteration 56, loss = 4562.67761850\n",
      "Iteration 57, loss = 4555.71134300\n",
      "Iteration 58, loss = 4548.51743993\n",
      "Iteration 59, loss = 4541.22073472\n",
      "Iteration 60, loss = 4534.05463091\n",
      "Iteration 61, loss = 4526.85907425\n",
      "Iteration 62, loss = 4519.76175449\n",
      "Iteration 63, loss = 4512.46280346\n",
      "Iteration 64, loss = 4505.07456115\n",
      "Iteration 65, loss = 4497.52534142\n",
      "Iteration 66, loss = 4489.74302145\n",
      "Iteration 67, loss = 4482.19213600\n",
      "Iteration 68, loss = 4474.54642982\n",
      "Iteration 69, loss = 4466.93430374\n",
      "Iteration 70, loss = 4459.61635943\n",
      "Iteration 71, loss = 4451.99401699\n",
      "Iteration 72, loss = 4444.57352586\n",
      "Iteration 73, loss = 4437.27265635\n",
      "Iteration 74, loss = 4429.71627100\n",
      "Iteration 75, loss = 4422.33504471\n",
      "Iteration 76, loss = 4414.95638158\n",
      "Iteration 77, loss = 4407.49560312\n",
      "Iteration 78, loss = 4400.02407259\n",
      "Iteration 79, loss = 4392.67356659\n",
      "Iteration 80, loss = 4385.19668992\n",
      "Iteration 81, loss = 4377.79192946\n",
      "Iteration 82, loss = 4370.38580995\n",
      "Iteration 83, loss = 4363.13714488\n",
      "Iteration 84, loss = 4355.50525117\n",
      "Iteration 85, loss = 4348.08697061\n",
      "Iteration 86, loss = 4340.74847718\n",
      "Iteration 87, loss = 4333.22361612\n",
      "Iteration 88, loss = 4325.85150545\n",
      "Iteration 89, loss = 4318.46778642\n",
      "Iteration 90, loss = 4311.01986106\n",
      "Iteration 91, loss = 4303.62006387\n",
      "Iteration 92, loss = 4296.25725673\n",
      "Iteration 93, loss = 4288.86298320\n",
      "Iteration 94, loss = 4281.35950199\n",
      "Iteration 95, loss = 4274.07756081\n",
      "Iteration 96, loss = 4266.66333801\n",
      "Iteration 97, loss = 4259.33471216\n",
      "Iteration 98, loss = 4251.92943752\n",
      "Iteration 99, loss = 4244.58908050\n",
      "Iteration 100, loss = 4237.20701161\n",
      "Iteration 101, loss = 4229.90849934\n",
      "Iteration 102, loss = 4222.56008757\n",
      "Iteration 103, loss = 4215.23004975\n",
      "Iteration 104, loss = 4207.97391630\n",
      "Iteration 105, loss = 4200.60885733\n",
      "Iteration 106, loss = 4193.35388463\n",
      "Iteration 107, loss = 4186.10294579\n",
      "Iteration 108, loss = 4178.75608816\n",
      "Iteration 109, loss = 4171.57764608\n",
      "Iteration 110, loss = 4164.25894387\n",
      "Iteration 111, loss = 4157.13869900\n",
      "Iteration 112, loss = 4149.88318869\n",
      "Iteration 113, loss = 4142.64019066\n",
      "Iteration 114, loss = 4135.54888862\n",
      "Iteration 115, loss = 4128.30145171\n",
      "Iteration 116, loss = 4121.16832173\n",
      "Iteration 117, loss = 4113.95438891\n",
      "Iteration 118, loss = 4106.93016207\n",
      "Iteration 119, loss = 4099.78883568\n",
      "Iteration 120, loss = 4092.64004468\n",
      "Iteration 121, loss = 4085.52682883\n",
      "Iteration 122, loss = 4078.49358670\n",
      "Iteration 123, loss = 4071.40762946\n",
      "Iteration 124, loss = 4064.48119087\n",
      "Iteration 125, loss = 4057.44264844\n",
      "Iteration 126, loss = 4050.41716975\n",
      "Iteration 127, loss = 4043.42946375\n",
      "Iteration 128, loss = 4036.38574601\n",
      "Iteration 129, loss = 4029.51214207\n",
      "Iteration 130, loss = 4022.58750976\n",
      "Iteration 131, loss = 4015.58508746\n",
      "Iteration 132, loss = 4008.70842313\n",
      "Iteration 133, loss = 4001.93266033\n",
      "Iteration 134, loss = 3994.93777893\n",
      "Iteration 135, loss = 3988.15934296\n",
      "Iteration 136, loss = 3981.24858538\n",
      "Iteration 137, loss = 3974.52502555\n",
      "Iteration 138, loss = 3967.72084858\n",
      "Iteration 139, loss = 3960.88227896\n",
      "Iteration 140, loss = 3954.15438054\n",
      "Iteration 141, loss = 3947.35308705\n",
      "Iteration 142, loss = 3940.75587967\n",
      "Iteration 143, loss = 3933.97475439\n",
      "Iteration 144, loss = 3927.43313976\n",
      "Iteration 145, loss = 3920.58639495\n",
      "Iteration 146, loss = 3914.04557401\n",
      "Iteration 147, loss = 3907.36912352\n",
      "Iteration 148, loss = 3900.68686796\n",
      "Iteration 149, loss = 3894.23710091\n",
      "Iteration 150, loss = 3887.61699601\n",
      "Iteration 151, loss = 3881.05404823\n",
      "Iteration 152, loss = 3874.54758782\n",
      "Iteration 153, loss = 3867.95961501\n",
      "Iteration 154, loss = 3861.49811680\n",
      "Iteration 155, loss = 3855.05730991\n",
      "Iteration 156, loss = 3848.49944715\n",
      "Iteration 157, loss = 3842.15354763\n",
      "Iteration 158, loss = 3835.71914756\n",
      "Iteration 159, loss = 3829.32858637\n",
      "Iteration 160, loss = 3822.92290683\n",
      "Iteration 161, loss = 3816.64499919\n",
      "Iteration 162, loss = 3810.20533775\n",
      "Iteration 163, loss = 3803.94153751\n",
      "Iteration 164, loss = 3797.61429220\n",
      "Iteration 165, loss = 3791.39606008\n",
      "Iteration 166, loss = 3785.04459542\n",
      "Iteration 167, loss = 3778.77616386\n",
      "Iteration 168, loss = 3772.63370206\n",
      "Iteration 169, loss = 3766.40994428\n",
      "Iteration 170, loss = 3760.17637764\n",
      "Iteration 171, loss = 3754.04378060\n",
      "Iteration 172, loss = 3747.85181796\n",
      "Iteration 173, loss = 3741.81352962\n",
      "Iteration 174, loss = 3735.65885376\n",
      "Iteration 175, loss = 3729.54093233\n",
      "Iteration 176, loss = 3723.45623439\n",
      "Iteration 177, loss = 3717.46387059\n",
      "Iteration 178, loss = 3711.31306094\n",
      "Iteration 179, loss = 3705.32168816\n",
      "Iteration 180, loss = 3699.36749983\n",
      "Iteration 181, loss = 3693.36051404\n",
      "Iteration 182, loss = 3687.39892565\n",
      "Iteration 183, loss = 3681.48784331\n",
      "Iteration 184, loss = 3675.54703751\n",
      "Iteration 185, loss = 3669.61421135\n",
      "Iteration 186, loss = 3663.69717905\n",
      "Iteration 187, loss = 3657.84521501\n",
      "Iteration 188, loss = 3652.06080011\n",
      "Iteration 189, loss = 3646.15733989\n",
      "Iteration 190, loss = 3640.33044609\n",
      "Iteration 191, loss = 3634.56170314\n",
      "Iteration 192, loss = 3628.79020246\n",
      "Iteration 193, loss = 3623.10199942\n",
      "Iteration 194, loss = 3617.23679955\n",
      "Iteration 195, loss = 3611.56498638\n",
      "Iteration 196, loss = 3605.76437342\n",
      "Iteration 197, loss = 3600.14616144\n",
      "Iteration 198, loss = 3594.43947774\n",
      "Iteration 199, loss = 3588.78703406\n",
      "Iteration 200, loss = 3583.17519269\n",
      "Iteration 201, loss = 3577.53885713\n",
      "Iteration 202, loss = 3571.99160441\n",
      "Iteration 203, loss = 3566.29577219\n",
      "Iteration 204, loss = 3560.71396680\n",
      "Iteration 205, loss = 3555.26706517\n",
      "Iteration 206, loss = 3549.70491850\n",
      "Iteration 207, loss = 3544.14490815\n",
      "Iteration 208, loss = 3538.64693538\n",
      "Iteration 209, loss = 3533.22458967\n",
      "Iteration 210, loss = 3527.71574169\n",
      "Iteration 211, loss = 3522.26025873\n",
      "Iteration 212, loss = 3516.86311751\n",
      "Iteration 213, loss = 3511.41599311\n",
      "Iteration 214, loss = 3506.06942486\n",
      "Iteration 215, loss = 3500.62983563\n",
      "Iteration 216, loss = 3495.33591611\n",
      "Iteration 217, loss = 3489.95156587\n",
      "Iteration 218, loss = 3484.62422624\n",
      "Iteration 219, loss = 3479.26582829\n",
      "Iteration 220, loss = 3474.03387906\n",
      "Iteration 221, loss = 3468.68987566\n",
      "Iteration 222, loss = 3463.44746423\n",
      "Iteration 223, loss = 3458.23934621\n",
      "Iteration 224, loss = 3452.99825759\n",
      "Iteration 225, loss = 3447.79457690\n",
      "Iteration 226, loss = 3442.54707337\n",
      "Iteration 227, loss = 3437.42791604\n",
      "Iteration 228, loss = 3432.27649210\n",
      "Iteration 229, loss = 3427.03793555\n",
      "Iteration 230, loss = 3421.94705187\n",
      "Iteration 231, loss = 3416.79732938\n",
      "Iteration 232, loss = 3411.79492058\n",
      "Iteration 233, loss = 3406.70019456\n",
      "Iteration 234, loss = 3401.55235787\n",
      "Iteration 235, loss = 3396.54661574\n",
      "Iteration 236, loss = 3391.49755772\n",
      "Iteration 237, loss = 3386.49460590\n",
      "Iteration 238, loss = 3381.48732053\n",
      "Iteration 239, loss = 3376.53448411\n",
      "Iteration 240, loss = 3371.46697040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 241, loss = 3366.58303711\n",
      "Iteration 242, loss = 3361.57797745\n",
      "Iteration 243, loss = 3356.73563915\n",
      "Iteration 244, loss = 3351.77012019\n",
      "Iteration 245, loss = 3346.90983847\n",
      "Iteration 246, loss = 3342.01865302\n",
      "Iteration 247, loss = 3337.13298801\n",
      "Iteration 248, loss = 3332.24604637\n",
      "Iteration 249, loss = 3327.42095771\n",
      "Iteration 250, loss = 3322.67342604\n",
      "Iteration 251, loss = 3317.85820764\n",
      "Iteration 252, loss = 3313.00747957\n",
      "Iteration 253, loss = 3308.30140800\n",
      "Iteration 254, loss = 3303.52339138\n",
      "Iteration 255, loss = 3298.72650968\n",
      "Iteration 256, loss = 3294.04357466\n",
      "Iteration 257, loss = 3289.30469300\n",
      "Iteration 258, loss = 3284.60944585\n",
      "Iteration 259, loss = 3280.04214316\n",
      "Iteration 260, loss = 3275.22437285\n",
      "Iteration 261, loss = 3270.57821145\n",
      "Iteration 262, loss = 3265.98976229\n",
      "Iteration 263, loss = 3261.33601994\n",
      "Iteration 264, loss = 3256.68582452\n",
      "Iteration 265, loss = 3252.22167249\n",
      "Iteration 266, loss = 3247.53239155\n",
      "Iteration 267, loss = 3243.00337251\n",
      "Iteration 268, loss = 3238.41089031\n",
      "Iteration 269, loss = 3233.84946048\n",
      "Iteration 270, loss = 3229.39133091\n",
      "Iteration 271, loss = 3224.82793799\n",
      "Iteration 272, loss = 3220.33941265\n",
      "Iteration 273, loss = 3215.83113440\n",
      "Iteration 274, loss = 3211.35771929\n",
      "Iteration 275, loss = 3206.99119966\n",
      "Iteration 276, loss = 3202.49616547\n",
      "Iteration 277, loss = 3198.12841108\n",
      "Iteration 278, loss = 3193.66038537\n",
      "Iteration 279, loss = 3189.15427086\n",
      "Iteration 280, loss = 3184.93255190\n",
      "Iteration 281, loss = 3180.48063102\n",
      "Iteration 282, loss = 3176.12174186\n",
      "Iteration 283, loss = 3171.78398541\n",
      "Iteration 284, loss = 3167.44770285\n",
      "Iteration 285, loss = 3163.14014853\n",
      "Iteration 286, loss = 3158.85737297\n",
      "Iteration 287, loss = 3154.54085455\n",
      "Iteration 288, loss = 3150.27340912\n",
      "Iteration 289, loss = 3145.98795212\n",
      "Iteration 290, loss = 3141.71371470\n",
      "Iteration 291, loss = 3137.57312112\n",
      "Iteration 292, loss = 3133.27341305\n",
      "Iteration 293, loss = 3129.04824165\n",
      "Iteration 294, loss = 3124.93442304\n",
      "Iteration 295, loss = 3120.63139614\n",
      "Iteration 296, loss = 3116.58210223\n",
      "Iteration 297, loss = 3112.33144595\n",
      "Iteration 298, loss = 3108.27260385\n",
      "Iteration 299, loss = 3104.09693453\n",
      "Iteration 300, loss = 3099.97332326\n",
      "Iteration 301, loss = 3095.87944236\n",
      "Iteration 302, loss = 3091.72550347\n",
      "Iteration 303, loss = 3087.65625547\n",
      "Iteration 304, loss = 3083.63641898\n",
      "Iteration 305, loss = 3079.53532049\n",
      "Iteration 306, loss = 3075.52210742\n",
      "Iteration 307, loss = 3071.49439785\n",
      "Iteration 308, loss = 3067.46339645\n",
      "Iteration 309, loss = 3063.45619233\n",
      "Iteration 310, loss = 3059.48749063\n",
      "Iteration 311, loss = 3055.49880630\n",
      "Iteration 312, loss = 3051.50546424\n",
      "Iteration 313, loss = 3047.58336775\n",
      "Iteration 314, loss = 3043.66147879\n",
      "Iteration 315, loss = 3039.62185916\n",
      "Iteration 316, loss = 3035.75610445\n",
      "Iteration 317, loss = 3031.94738690\n",
      "Iteration 318, loss = 3027.95943981\n",
      "Iteration 319, loss = 3024.12194231\n",
      "Iteration 320, loss = 3020.16584296\n",
      "Iteration 321, loss = 3016.40351038\n",
      "Iteration 322, loss = 3012.44866471\n",
      "Iteration 323, loss = 3008.69751426\n",
      "Iteration 324, loss = 3004.88221931\n",
      "Iteration 325, loss = 3001.00006519\n",
      "Iteration 326, loss = 2997.30578707\n",
      "Iteration 327, loss = 2993.48538852\n",
      "Iteration 328, loss = 2989.70696377\n",
      "Iteration 329, loss = 2985.95207620\n",
      "Iteration 330, loss = 2982.17861843\n",
      "Iteration 331, loss = 2978.45843524\n",
      "Iteration 332, loss = 2974.73494544\n",
      "Iteration 333, loss = 2971.03174877\n",
      "Iteration 334, loss = 2967.30392248\n",
      "Iteration 335, loss = 2963.67684615\n",
      "Iteration 336, loss = 2959.93082768\n",
      "Iteration 337, loss = 2956.20381901\n",
      "Iteration 338, loss = 2952.62903766\n",
      "Iteration 339, loss = 2948.99960985\n",
      "Iteration 340, loss = 2945.32837374\n",
      "Iteration 341, loss = 2941.74362017\n",
      "Iteration 342, loss = 2938.12242158\n",
      "Iteration 343, loss = 2934.53632503\n",
      "Iteration 344, loss = 2930.93024380\n",
      "Iteration 345, loss = 2927.35149785\n",
      "Iteration 346, loss = 2923.76818439\n",
      "Iteration 347, loss = 2920.20011301\n",
      "Iteration 348, loss = 2916.72428891\n",
      "Iteration 349, loss = 2913.16300154\n",
      "Iteration 350, loss = 2909.65866911\n",
      "Iteration 351, loss = 2906.14268200\n",
      "Iteration 352, loss = 2902.58521216\n",
      "Iteration 353, loss = 2899.15932151\n",
      "Iteration 354, loss = 2895.70161922\n",
      "Iteration 355, loss = 2892.19044141\n",
      "Iteration 356, loss = 2888.77051829\n",
      "Iteration 357, loss = 2885.33667234\n",
      "Iteration 358, loss = 2881.91221964\n",
      "Iteration 359, loss = 2878.46887012\n",
      "Iteration 360, loss = 2875.05766349\n",
      "Iteration 361, loss = 2871.65493705\n",
      "Iteration 362, loss = 2868.27214235\n",
      "Iteration 363, loss = 2864.93366830\n",
      "Iteration 364, loss = 2861.53643262\n",
      "Iteration 365, loss = 2858.14433052\n",
      "Iteration 366, loss = 2854.82472007\n",
      "Iteration 367, loss = 2851.53307817\n",
      "Iteration 368, loss = 2848.19743167\n",
      "Iteration 369, loss = 2844.85471258\n",
      "Iteration 370, loss = 2841.54115814\n",
      "Iteration 371, loss = 2838.27086366\n",
      "Iteration 372, loss = 2835.00365553\n",
      "Iteration 373, loss = 2831.71549824\n",
      "Iteration 374, loss = 2828.47649649\n",
      "Iteration 375, loss = 2825.25367107\n",
      "Iteration 376, loss = 2821.97448930\n",
      "Iteration 377, loss = 2818.75334869\n",
      "Iteration 378, loss = 2815.54842954\n",
      "Iteration 379, loss = 2812.29597345\n",
      "Iteration 380, loss = 2809.10577716\n",
      "Iteration 381, loss = 2805.94693958\n",
      "Iteration 382, loss = 2802.80908880\n",
      "Iteration 383, loss = 2799.62718496\n",
      "Iteration 384, loss = 2796.44874270\n",
      "Iteration 385, loss = 2793.31114781\n",
      "Iteration 386, loss = 2790.13689166\n",
      "Iteration 387, loss = 2787.07060513\n",
      "Iteration 388, loss = 2783.89144120\n",
      "Iteration 389, loss = 2780.86880898\n",
      "Iteration 390, loss = 2777.74040329\n",
      "Iteration 391, loss = 2774.65422228\n",
      "Iteration 392, loss = 2771.58139276\n",
      "Iteration 393, loss = 2768.51148407\n",
      "Iteration 394, loss = 2765.43361022\n",
      "Iteration 395, loss = 2762.44993104\n",
      "Iteration 396, loss = 2759.40377799\n",
      "Iteration 397, loss = 2756.40737398\n",
      "Iteration 398, loss = 2753.37096396\n",
      "Iteration 399, loss = 2750.30898717\n",
      "Iteration 400, loss = 2747.37806280\n",
      "Iteration 401, loss = 2744.34695912\n",
      "Iteration 402, loss = 2741.39533678\n",
      "Iteration 403, loss = 2738.45745153\n",
      "Iteration 404, loss = 2735.47981862\n",
      "Iteration 405, loss = 2732.55866722\n",
      "Iteration 406, loss = 2729.60648849\n",
      "Iteration 407, loss = 2726.67409375\n",
      "Iteration 408, loss = 2723.80712533\n",
      "Iteration 409, loss = 2720.89624850\n",
      "Iteration 410, loss = 2717.90955827\n",
      "Iteration 411, loss = 2715.07733843\n",
      "Iteration 412, loss = 2712.25152379\n",
      "Iteration 413, loss = 2709.35100845\n",
      "Iteration 414, loss = 2706.47749457\n",
      "Iteration 415, loss = 2703.68849193\n",
      "Iteration 416, loss = 2700.77100198\n",
      "Iteration 417, loss = 2697.97828034\n",
      "Iteration 418, loss = 2695.16609132\n",
      "Iteration 419, loss = 2692.38072144\n",
      "Iteration 420, loss = 2689.53808447\n",
      "Iteration 421, loss = 2686.75889609\n",
      "Iteration 422, loss = 2684.00303362\n",
      "Iteration 423, loss = 2681.18897416\n",
      "Iteration 424, loss = 2678.45823016\n",
      "Iteration 425, loss = 2675.71995978\n",
      "Iteration 426, loss = 2672.95500755\n",
      "Iteration 427, loss = 2670.19884120\n",
      "Iteration 428, loss = 2667.51097648\n",
      "Iteration 429, loss = 2664.77935357\n",
      "Iteration 430, loss = 2662.07975466\n",
      "Iteration 431, loss = 2659.36930204\n",
      "Iteration 432, loss = 2656.70828652\n",
      "Iteration 433, loss = 2654.04594391\n",
      "Iteration 434, loss = 2651.36488839\n",
      "Iteration 435, loss = 2648.65847470\n",
      "Iteration 436, loss = 2646.05751275\n",
      "Iteration 437, loss = 2643.38086805\n",
      "Iteration 438, loss = 2640.76313474\n",
      "Iteration 439, loss = 2638.14808746\n",
      "Iteration 440, loss = 2635.56270076\n",
      "Iteration 441, loss = 2632.97973200\n",
      "Iteration 442, loss = 2630.33855548\n",
      "Iteration 443, loss = 2627.76525481\n",
      "Iteration 444, loss = 2625.12842303\n",
      "Iteration 445, loss = 2622.59277290\n",
      "Iteration 446, loss = 2620.02356545\n",
      "Iteration 447, loss = 2617.55192291\n",
      "Iteration 448, loss = 2614.95477903\n",
      "Iteration 449, loss = 2612.38311223\n",
      "Iteration 450, loss = 2609.89073343\n",
      "Iteration 451, loss = 2607.34821224\n",
      "Iteration 452, loss = 2604.89327388\n",
      "Iteration 453, loss = 2602.37842581\n",
      "Iteration 454, loss = 2599.91817579\n",
      "Iteration 455, loss = 2597.37626303\n",
      "Iteration 456, loss = 2594.95861483\n",
      "Iteration 457, loss = 2592.47439796\n",
      "Iteration 458, loss = 2590.00975172\n",
      "Iteration 459, loss = 2587.56822404\n",
      "Iteration 460, loss = 2585.22250812\n",
      "Iteration 461, loss = 2582.64692900\n",
      "Iteration 462, loss = 2580.30681801\n",
      "Iteration 463, loss = 2577.90361568\n",
      "Iteration 464, loss = 2575.49520272\n",
      "Iteration 465, loss = 2573.10498618\n",
      "Iteration 466, loss = 2570.69773532\n",
      "Iteration 467, loss = 2568.36189243\n",
      "Iteration 468, loss = 2565.98655886\n",
      "Iteration 469, loss = 2563.60524816\n",
      "Iteration 470, loss = 2561.26694019\n",
      "Iteration 471, loss = 2558.99004360\n",
      "Iteration 472, loss = 2556.58646125\n",
      "Iteration 473, loss = 2554.24843603\n",
      "Iteration 474, loss = 2551.99470185\n",
      "Iteration 475, loss = 2549.65942073\n",
      "Iteration 476, loss = 2547.37032692\n",
      "Iteration 477, loss = 2545.04843570\n",
      "Iteration 478, loss = 2542.83307224\n",
      "Iteration 479, loss = 2540.51786933\n",
      "Iteration 480, loss = 2538.23947511\n",
      "Iteration 481, loss = 2535.96566507\n",
      "Iteration 482, loss = 2533.75314721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 483, loss = 2531.50181561\n",
      "Iteration 484, loss = 2529.32395928\n",
      "Iteration 485, loss = 2527.03555184\n",
      "Iteration 486, loss = 2524.80335171\n",
      "Iteration 487, loss = 2522.67426085\n",
      "Iteration 488, loss = 2520.45293748\n",
      "Iteration 489, loss = 2518.24747836\n",
      "Iteration 490, loss = 2516.07418463\n",
      "Iteration 491, loss = 2513.88770753\n",
      "Iteration 492, loss = 2511.78412372\n",
      "Iteration 493, loss = 2509.52677165\n",
      "Iteration 494, loss = 2507.42426162\n",
      "Iteration 495, loss = 2505.31041094\n",
      "Iteration 496, loss = 2503.16405630\n",
      "Iteration 497, loss = 2501.01136655\n",
      "Iteration 498, loss = 2498.92817765\n",
      "Iteration 499, loss = 2496.79389463\n",
      "Iteration 500, loss = 2494.73912224\n",
      "Iteration 501, loss = 2492.66524511\n",
      "Iteration 502, loss = 2490.51160950\n",
      "Iteration 503, loss = 2488.44724632\n",
      "Iteration 504, loss = 2486.38507528\n",
      "Iteration 505, loss = 2484.34870503\n",
      "Iteration 506, loss = 2482.25815729\n",
      "Iteration 507, loss = 2480.23072804\n",
      "Iteration 508, loss = 2478.24785142\n",
      "Iteration 509, loss = 2476.12858141\n",
      "Iteration 510, loss = 2474.14729439\n",
      "Iteration 511, loss = 2472.14604727\n",
      "Iteration 512, loss = 2470.13569971\n",
      "Iteration 513, loss = 2468.14633469\n",
      "Iteration 514, loss = 2466.12691736\n",
      "Iteration 515, loss = 2464.12154031\n",
      "Iteration 516, loss = 2462.21164609\n",
      "Iteration 517, loss = 2460.28325660\n",
      "Iteration 518, loss = 2458.27248744\n",
      "Iteration 519, loss = 2456.27478747\n",
      "Iteration 520, loss = 2454.36587756\n",
      "Iteration 521, loss = 2452.46284849\n",
      "Iteration 522, loss = 2450.47158776\n",
      "Iteration 523, loss = 2448.63635841\n",
      "Iteration 524, loss = 2446.67406330\n",
      "Iteration 525, loss = 2444.74776579\n",
      "Iteration 526, loss = 2442.84454909\n",
      "Iteration 527, loss = 2441.02954229\n",
      "Iteration 528, loss = 2439.13827478\n",
      "Iteration 529, loss = 2437.23126143\n",
      "Iteration 530, loss = 2435.38613726\n",
      "Iteration 531, loss = 2433.48640276\n",
      "Iteration 532, loss = 2431.68467574\n",
      "Iteration 533, loss = 2429.79742084\n",
      "Iteration 534, loss = 2427.94243187\n",
      "Iteration 535, loss = 2426.17503201\n",
      "Iteration 536, loss = 2424.35827352\n",
      "Iteration 537, loss = 2422.51207084\n",
      "Iteration 538, loss = 2420.70245165\n",
      "Iteration 539, loss = 2418.95421567\n",
      "Iteration 540, loss = 2417.10880631\n",
      "Iteration 541, loss = 2415.31325284\n",
      "Iteration 542, loss = 2413.53336549\n",
      "Iteration 543, loss = 2411.78893048\n",
      "Iteration 544, loss = 2410.02536895\n",
      "Iteration 545, loss = 2408.27221696\n",
      "Iteration 546, loss = 2406.54931210\n",
      "Iteration 547, loss = 2404.79538071\n",
      "Iteration 548, loss = 2403.02865240\n",
      "Iteration 549, loss = 2401.34058298\n",
      "Iteration 550, loss = 2399.59058824\n",
      "Iteration 551, loss = 2397.93319434\n",
      "Iteration 552, loss = 2396.16793373\n",
      "Iteration 553, loss = 2394.50959300\n",
      "Iteration 554, loss = 2392.81217993\n",
      "Iteration 555, loss = 2391.13870095\n",
      "Iteration 556, loss = 2389.43115933\n",
      "Iteration 557, loss = 2387.79507222\n",
      "Iteration 558, loss = 2386.11659296\n",
      "Iteration 559, loss = 2384.42751688\n",
      "Iteration 560, loss = 2382.86424039\n",
      "Iteration 561, loss = 2381.19113383\n",
      "Iteration 562, loss = 2379.55127662\n",
      "Iteration 563, loss = 2377.90887927\n",
      "Iteration 564, loss = 2376.33505614\n",
      "Iteration 565, loss = 2374.73948755\n",
      "Iteration 566, loss = 2373.06023522\n",
      "Iteration 567, loss = 2371.46637669\n",
      "Iteration 568, loss = 2369.93024896\n",
      "Iteration 569, loss = 2368.37599605\n",
      "Iteration 570, loss = 2366.78022131\n",
      "Iteration 571, loss = 2365.16094723\n",
      "Iteration 572, loss = 2363.68730584\n",
      "Iteration 573, loss = 2362.05267876\n",
      "Iteration 574, loss = 2360.50810191\n",
      "Iteration 575, loss = 2358.98070274\n",
      "Iteration 576, loss = 2357.46251068\n",
      "Iteration 577, loss = 2355.93180869\n",
      "Iteration 578, loss = 2354.38554833\n",
      "Iteration 579, loss = 2352.91395994\n",
      "Iteration 580, loss = 2351.38275170\n",
      "Iteration 581, loss = 2349.90504034\n",
      "Iteration 582, loss = 2348.45684022\n",
      "Iteration 583, loss = 2346.89593120\n",
      "Iteration 584, loss = 2345.42260653\n",
      "Iteration 585, loss = 2343.94505918\n",
      "Iteration 586, loss = 2342.53105672\n",
      "Iteration 587, loss = 2341.03646131\n",
      "Iteration 588, loss = 2339.54302706\n",
      "Iteration 589, loss = 2338.17789495\n",
      "Iteration 590, loss = 2336.71149354\n",
      "Iteration 591, loss = 2335.25605896\n",
      "Iteration 592, loss = 2333.85901798\n",
      "Iteration 593, loss = 2332.43550814\n",
      "Iteration 594, loss = 2331.01032352\n",
      "Iteration 595, loss = 2329.64567144\n",
      "Iteration 596, loss = 2328.23812562\n",
      "Iteration 597, loss = 2326.80514784\n",
      "Iteration 598, loss = 2325.43778124\n",
      "Iteration 599, loss = 2324.05772373\n",
      "Iteration 600, loss = 2322.69967458\n",
      "Iteration 601, loss = 2321.36749542\n",
      "Iteration 602, loss = 2319.95695420\n",
      "Iteration 603, loss = 2318.61834531\n",
      "Iteration 604, loss = 2317.23572393\n",
      "Iteration 605, loss = 2315.92378103\n",
      "Iteration 606, loss = 2314.60620792\n",
      "Iteration 607, loss = 2313.26118175\n",
      "Iteration 608, loss = 2311.96838404\n",
      "Iteration 609, loss = 2310.64883836\n",
      "Iteration 610, loss = 2309.30752621\n",
      "Iteration 611, loss = 2308.04344765\n",
      "Iteration 612, loss = 2306.73376415\n",
      "Iteration 613, loss = 2305.48307190\n",
      "Iteration 614, loss = 2304.13195652\n",
      "Iteration 615, loss = 2302.88559537\n",
      "Iteration 616, loss = 2301.63842746\n",
      "Iteration 617, loss = 2300.32332501\n",
      "Iteration 618, loss = 2299.09224113\n",
      "Iteration 619, loss = 2297.87675137\n",
      "Iteration 620, loss = 2296.59742325\n",
      "Iteration 621, loss = 2295.36624640\n",
      "Iteration 622, loss = 2294.15074544\n",
      "Iteration 623, loss = 2292.90303668\n",
      "Iteration 624, loss = 2291.66862869\n",
      "Iteration 625, loss = 2290.45783165\n",
      "Iteration 626, loss = 2289.25266168\n",
      "Iteration 627, loss = 2288.08471990\n",
      "Iteration 628, loss = 2286.87687786\n",
      "Iteration 629, loss = 2285.69741308\n",
      "Iteration 630, loss = 2284.49849779\n",
      "Iteration 631, loss = 2283.28880708\n",
      "Iteration 632, loss = 2282.20610427\n",
      "Iteration 633, loss = 2280.97891728\n",
      "Iteration 634, loss = 2279.82952672\n",
      "Iteration 635, loss = 2278.66682730\n",
      "Iteration 636, loss = 2277.52146383\n",
      "Iteration 637, loss = 2276.44945051\n",
      "Iteration 638, loss = 2275.24763852\n",
      "Iteration 639, loss = 2274.13050069\n",
      "Iteration 640, loss = 2273.02483402\n",
      "Iteration 641, loss = 2271.86099311\n",
      "Iteration 642, loss = 2270.75856823\n",
      "Iteration 643, loss = 2269.67588115\n",
      "Iteration 644, loss = 2268.59531397\n",
      "Iteration 645, loss = 2267.48500756\n",
      "Iteration 646, loss = 2266.38600049\n",
      "Iteration 647, loss = 2265.32768265\n",
      "Iteration 648, loss = 2264.28649324\n",
      "Iteration 649, loss = 2263.10577416\n",
      "Iteration 650, loss = 2262.13626530\n",
      "Iteration 651, loss = 2261.02468627\n",
      "Iteration 652, loss = 2259.98433552\n",
      "Iteration 653, loss = 2258.97984662\n",
      "Iteration 654, loss = 2257.92799337\n",
      "Iteration 655, loss = 2256.85658145\n",
      "Iteration 656, loss = 2255.86500009\n",
      "Iteration 657, loss = 2254.82353191\n",
      "Iteration 658, loss = 2253.76942767\n",
      "Iteration 659, loss = 2252.80823561\n",
      "Iteration 660, loss = 2251.75536101\n",
      "Iteration 661, loss = 2250.79052146\n",
      "Iteration 662, loss = 2249.81772548\n",
      "Iteration 663, loss = 2248.80741237\n",
      "Iteration 664, loss = 2247.79414621\n",
      "Iteration 665, loss = 2246.82877072\n",
      "Iteration 666, loss = 2245.84752235\n",
      "Iteration 667, loss = 2244.87386875\n",
      "Iteration 668, loss = 2243.95601539\n",
      "Iteration 669, loss = 2242.96623547\n",
      "Iteration 670, loss = 2242.01512065\n",
      "Iteration 671, loss = 2241.07876371\n",
      "Iteration 672, loss = 2240.14684589\n",
      "Iteration 673, loss = 2239.17309590\n",
      "Iteration 674, loss = 2238.23433551\n",
      "Iteration 675, loss = 2237.38385904\n",
      "Iteration 676, loss = 2236.41039286\n",
      "Iteration 677, loss = 2235.50528527\n",
      "Iteration 678, loss = 2234.60817082\n",
      "Iteration 679, loss = 2233.65904722\n",
      "Iteration 680, loss = 2232.78848257\n",
      "Iteration 681, loss = 2231.90748555\n",
      "Iteration 682, loss = 2231.01690962\n",
      "Iteration 683, loss = 2230.11160946\n",
      "Iteration 684, loss = 2229.24770434\n",
      "Iteration 685, loss = 2228.35712448\n",
      "Iteration 686, loss = 2227.55784612\n",
      "Iteration 687, loss = 2226.61649553\n",
      "Iteration 688, loss = 2225.78435911\n",
      "Iteration 689, loss = 2224.93487099\n",
      "Iteration 690, loss = 2224.12061224\n",
      "Iteration 691, loss = 2223.24611086\n",
      "Iteration 692, loss = 2222.47130585\n",
      "Iteration 693, loss = 2221.56584494\n",
      "Iteration 694, loss = 2220.72334090\n",
      "Iteration 695, loss = 2219.93188519\n",
      "Iteration 696, loss = 2219.13668584\n",
      "Iteration 697, loss = 2218.30341821\n",
      "Iteration 698, loss = 2217.52975296\n",
      "Iteration 699, loss = 2216.69080113\n",
      "Iteration 700, loss = 2215.87281287\n",
      "Iteration 701, loss = 2215.09822557\n",
      "Iteration 702, loss = 2214.34101100\n",
      "Iteration 703, loss = 2213.54561877\n",
      "Iteration 704, loss = 2212.77388798\n",
      "Iteration 705, loss = 2212.02555541\n",
      "Iteration 706, loss = 2211.19917745\n",
      "Iteration 707, loss = 2210.44437744\n",
      "Iteration 708, loss = 2209.72273517\n",
      "Iteration 709, loss = 2208.96400248\n",
      "Iteration 710, loss = 2208.22747967\n",
      "Iteration 711, loss = 2207.45980993\n",
      "Iteration 712, loss = 2206.73029138\n",
      "Iteration 713, loss = 2205.95540103\n",
      "Iteration 714, loss = 2205.26865222\n",
      "Iteration 715, loss = 2204.56840775\n",
      "Iteration 716, loss = 2203.80396832\n",
      "Iteration 717, loss = 2203.12038964\n",
      "Iteration 718, loss = 2202.37562753\n",
      "Iteration 719, loss = 2201.71691849\n",
      "Iteration 720, loss = 2200.96310742\n",
      "Iteration 721, loss = 2200.28757237\n",
      "Iteration 722, loss = 2199.60268881\n",
      "Iteration 723, loss = 2198.91672509\n",
      "Iteration 724, loss = 2198.20698847\n",
      "Iteration 725, loss = 2197.56125908\n",
      "Iteration 726, loss = 2196.87614959\n",
      "Iteration 727, loss = 2196.23724167\n",
      "Iteration 728, loss = 2195.53530648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 729, loss = 2194.85987136\n",
      "Iteration 730, loss = 2194.21416840\n",
      "Iteration 731, loss = 2193.56766335\n",
      "Iteration 732, loss = 2192.92086497\n",
      "Iteration 733, loss = 2192.29420366\n",
      "Iteration 734, loss = 2191.63269924\n",
      "Iteration 735, loss = 2191.02004412\n",
      "Iteration 736, loss = 2190.38096179\n",
      "Iteration 737, loss = 2189.77185685\n",
      "Iteration 738, loss = 2189.12936344\n",
      "Iteration 739, loss = 2188.49773090\n",
      "Iteration 740, loss = 2187.90739786\n",
      "Iteration 741, loss = 2187.29199190\n",
      "Iteration 742, loss = 2186.66617062\n",
      "Iteration 743, loss = 2186.11402926\n",
      "Iteration 744, loss = 2185.51362356\n",
      "Iteration 745, loss = 2184.88033557\n",
      "Iteration 746, loss = 2184.31383226\n",
      "Iteration 747, loss = 2183.73755647\n",
      "Iteration 748, loss = 2183.13602016\n",
      "Iteration 749, loss = 2182.57794713\n",
      "Iteration 750, loss = 2182.00355843\n",
      "Iteration 751, loss = 2181.42843260\n",
      "Iteration 752, loss = 2180.85641194\n",
      "Iteration 753, loss = 2180.31588941\n",
      "Iteration 754, loss = 2179.74364105\n",
      "Iteration 755, loss = 2179.21322833\n",
      "Iteration 756, loss = 2178.64621021\n",
      "Iteration 757, loss = 2178.11930257\n",
      "Iteration 758, loss = 2177.58704528\n",
      "Iteration 759, loss = 2177.00055173\n",
      "Iteration 760, loss = 2176.52465990\n",
      "Iteration 761, loss = 2175.95619219\n",
      "Iteration 762, loss = 2175.43086792\n",
      "Iteration 763, loss = 2174.95297854\n",
      "Iteration 764, loss = 2174.40391145\n",
      "Iteration 765, loss = 2173.88223422\n",
      "Iteration 766, loss = 2173.39486427\n",
      "Iteration 767, loss = 2172.86629739\n",
      "Iteration 768, loss = 2172.39031387\n",
      "Iteration 769, loss = 2171.89553669\n",
      "Iteration 770, loss = 2171.38973882\n",
      "Iteration 771, loss = 2170.89278964\n",
      "Iteration 772, loss = 2170.40146405\n",
      "Iteration 773, loss = 2169.90049234\n",
      "Iteration 774, loss = 2169.44560326\n",
      "Iteration 775, loss = 2168.95014305\n",
      "Iteration 776, loss = 2168.48503988\n",
      "Iteration 777, loss = 2168.03194397\n",
      "Iteration 778, loss = 2167.56033467\n",
      "Iteration 779, loss = 2167.05771933\n",
      "Iteration 780, loss = 2166.63230510\n",
      "Iteration 781, loss = 2166.16820071\n",
      "Iteration 782, loss = 2165.71661866\n",
      "Iteration 783, loss = 2165.26215221\n",
      "Iteration 784, loss = 2164.85200998\n",
      "Iteration 785, loss = 2164.35900796\n",
      "Iteration 786, loss = 2163.94303456\n",
      "Iteration 787, loss = 2163.52057334\n",
      "Iteration 788, loss = 2163.04633603\n",
      "Iteration 789, loss = 2162.63673716\n",
      "Iteration 790, loss = 2162.20700718\n",
      "Iteration 791, loss = 2161.80176637\n",
      "Iteration 792, loss = 2161.36832846\n",
      "Iteration 793, loss = 2160.96844483\n",
      "Iteration 794, loss = 2160.54662670\n",
      "Iteration 795, loss = 2160.12597824\n",
      "Iteration 796, loss = 2159.70205110\n",
      "Iteration 797, loss = 2159.33156352\n",
      "Iteration 798, loss = 2158.92039335\n",
      "Iteration 799, loss = 2158.51748329\n",
      "Iteration 800, loss = 2158.11290690\n",
      "Iteration 801, loss = 2157.72676536\n",
      "Iteration 802, loss = 2157.36724383\n",
      "Iteration 803, loss = 2156.95503292\n",
      "Iteration 804, loss = 2156.54907923\n",
      "Iteration 805, loss = 2156.18818170\n",
      "Iteration 806, loss = 2155.81945385\n",
      "Iteration 807, loss = 2155.43974984\n",
      "Iteration 808, loss = 2155.05992724\n",
      "Iteration 809, loss = 2154.69231252\n",
      "Iteration 810, loss = 2154.34049774\n",
      "Iteration 811, loss = 2153.97233617\n",
      "Iteration 812, loss = 2153.62093897\n",
      "Iteration 813, loss = 2153.24761452\n",
      "Iteration 814, loss = 2152.90448212\n",
      "Iteration 815, loss = 2152.54041932\n",
      "Iteration 816, loss = 2152.18508980\n",
      "Iteration 817, loss = 2151.87445594\n",
      "Iteration 818, loss = 2151.48981278\n",
      "Iteration 819, loss = 2151.18309251\n",
      "Iteration 820, loss = 2150.82755277\n",
      "Iteration 821, loss = 2150.50199002\n",
      "Iteration 822, loss = 2150.12929718\n",
      "Iteration 823, loss = 2149.82176641\n",
      "Iteration 824, loss = 2149.49472698\n",
      "Iteration 825, loss = 2149.18567757\n",
      "Iteration 826, loss = 2148.84577683\n",
      "Iteration 827, loss = 2148.53816700\n",
      "Iteration 828, loss = 2148.21441166\n",
      "Iteration 829, loss = 2147.89872839\n",
      "Iteration 830, loss = 2147.58829537\n",
      "Iteration 831, loss = 2147.28436461\n",
      "Iteration 832, loss = 2146.96228701\n",
      "Iteration 833, loss = 2146.67667768\n",
      "Iteration 834, loss = 2146.36370936\n",
      "Iteration 835, loss = 2146.04189017\n",
      "Iteration 836, loss = 2145.75219970\n",
      "Iteration 837, loss = 2145.48474263\n",
      "Iteration 838, loss = 2145.15695240\n",
      "Iteration 839, loss = 2144.87241094\n",
      "Iteration 840, loss = 2144.62140293\n",
      "Iteration 841, loss = 2144.29144375\n",
      "Iteration 842, loss = 2144.01020181\n",
      "Iteration 843, loss = 2143.75142285\n",
      "Iteration 844, loss = 2143.47356643\n",
      "Iteration 845, loss = 2143.15987963\n",
      "Iteration 846, loss = 2142.91111505\n",
      "Iteration 847, loss = 2142.63846800\n",
      "Iteration 848, loss = 2142.36463102\n",
      "Iteration 849, loss = 2142.10898266\n",
      "Iteration 850, loss = 2141.82554442\n",
      "Iteration 851, loss = 2141.53633356\n",
      "Iteration 852, loss = 2141.29209384\n",
      "Iteration 853, loss = 2141.05936387\n",
      "Iteration 854, loss = 2140.77350866\n",
      "Iteration 855, loss = 2140.52727075\n",
      "Iteration 856, loss = 2140.26105662\n",
      "Iteration 857, loss = 2139.99422950\n",
      "Iteration 858, loss = 2139.75724993\n",
      "Iteration 859, loss = 2139.52517179\n",
      "Iteration 860, loss = 2139.25731052\n",
      "Iteration 861, loss = 2139.01932554\n",
      "Iteration 862, loss = 2138.76763906\n",
      "Iteration 863, loss = 2138.55473798\n",
      "Iteration 864, loss = 2138.29178748\n",
      "Iteration 865, loss = 2138.04472902\n",
      "Iteration 866, loss = 2137.83960091\n",
      "Iteration 867, loss = 2137.56781149\n",
      "Iteration 868, loss = 2137.36026833\n",
      "Iteration 869, loss = 2137.11914589\n",
      "Iteration 870, loss = 2136.88476249\n",
      "Iteration 871, loss = 2136.66187093\n",
      "Iteration 872, loss = 2136.44031766\n",
      "Iteration 873, loss = 2136.20848486\n",
      "Iteration 874, loss = 2136.01179976\n",
      "Iteration 875, loss = 2135.75611619\n",
      "Iteration 876, loss = 2135.55799988\n",
      "Iteration 877, loss = 2135.32144114\n",
      "Iteration 878, loss = 2135.11030912\n",
      "Iteration 879, loss = 2134.91343689\n",
      "Iteration 880, loss = 2134.67488895\n",
      "Iteration 881, loss = 2134.48610137\n",
      "Iteration 882, loss = 2134.26543811\n",
      "Iteration 883, loss = 2134.05577000\n",
      "Iteration 884, loss = 2133.85667250\n",
      "Iteration 885, loss = 2133.63554235\n",
      "Iteration 886, loss = 2133.44187844\n",
      "Iteration 887, loss = 2133.22501349\n",
      "Iteration 888, loss = 2133.03716069\n",
      "Iteration 889, loss = 2132.82958996\n",
      "Iteration 890, loss = 2132.65513401\n",
      "Iteration 891, loss = 2132.43660258\n",
      "Iteration 892, loss = 2132.23012448\n",
      "Iteration 893, loss = 2132.05805661\n",
      "Iteration 894, loss = 2131.86180559\n",
      "Iteration 895, loss = 2131.65306388\n",
      "Iteration 896, loss = 2131.46306962\n",
      "Iteration 897, loss = 2131.27277109\n",
      "Iteration 898, loss = 2131.10883817\n",
      "Iteration 899, loss = 2130.89980051\n",
      "Iteration 900, loss = 2130.71909291\n",
      "Iteration 901, loss = 2130.52834920\n",
      "Iteration 902, loss = 2130.34551382\n",
      "Iteration 903, loss = 2130.16725441\n",
      "Iteration 904, loss = 2129.98076752\n",
      "Iteration 905, loss = 2129.81048476\n",
      "Iteration 906, loss = 2129.62701999\n",
      "Iteration 907, loss = 2129.43980725\n",
      "Iteration 908, loss = 2129.25591227\n",
      "Iteration 909, loss = 2129.10120389\n",
      "Iteration 910, loss = 2128.91151752\n",
      "Iteration 911, loss = 2128.75589792\n",
      "Iteration 912, loss = 2128.57223824\n",
      "Iteration 913, loss = 2128.39185044\n",
      "Iteration 914, loss = 2128.22142804\n",
      "Iteration 915, loss = 2128.06133181\n",
      "Iteration 916, loss = 2127.87864217\n",
      "Iteration 917, loss = 2127.71480020\n",
      "Iteration 918, loss = 2127.55020971\n",
      "Iteration 919, loss = 2127.38324285\n",
      "Iteration 920, loss = 2127.23678269\n",
      "Iteration 921, loss = 2127.04956792\n",
      "Iteration 922, loss = 2126.90363940\n",
      "Iteration 923, loss = 2126.72575250\n",
      "Iteration 924, loss = 2126.56857490\n",
      "Iteration 925, loss = 2126.40272586\n",
      "Iteration 926, loss = 2126.26522048\n",
      "Iteration 927, loss = 2126.09480381\n",
      "Iteration 928, loss = 2125.91544965\n",
      "Iteration 929, loss = 2125.77119070\n",
      "Iteration 930, loss = 2125.63545488\n",
      "Iteration 931, loss = 2125.44964865\n",
      "Iteration 932, loss = 2125.30159399\n",
      "Iteration 933, loss = 2125.14322777\n",
      "Iteration 934, loss = 2124.99355096\n",
      "Iteration 935, loss = 2124.83776929\n",
      "Iteration 936, loss = 2124.69636336\n",
      "Iteration 937, loss = 2124.53053061\n",
      "Iteration 938, loss = 2124.38513177\n",
      "Iteration 939, loss = 2124.22946748\n",
      "Iteration 940, loss = 2124.09215491\n",
      "Iteration 941, loss = 2123.93907891\n",
      "Iteration 942, loss = 2123.78098197\n",
      "Iteration 943, loss = 2123.63684848\n",
      "Iteration 944, loss = 2123.48578293\n",
      "Iteration 945, loss = 2123.34293840\n",
      "Iteration 946, loss = 2123.19653367\n",
      "Iteration 947, loss = 2123.06040065\n",
      "Iteration 948, loss = 2122.90179574\n",
      "Iteration 949, loss = 2122.76042164\n",
      "Iteration 950, loss = 2122.61612180\n",
      "Iteration 951, loss = 2122.47659814\n",
      "Iteration 952, loss = 2122.33264165\n",
      "Iteration 953, loss = 2122.18620027\n",
      "Iteration 954, loss = 2122.04529964\n",
      "Iteration 955, loss = 2121.93200085\n",
      "Iteration 956, loss = 2121.76083396\n",
      "Iteration 957, loss = 2121.62549185\n",
      "Iteration 958, loss = 2121.48128724\n",
      "Iteration 959, loss = 2121.34164680\n",
      "Iteration 960, loss = 2121.21199206\n",
      "Iteration 961, loss = 2121.07246270\n",
      "Iteration 962, loss = 2120.92432937\n",
      "Iteration 963, loss = 2120.78465583\n",
      "Iteration 964, loss = 2120.67120600\n",
      "Iteration 965, loss = 2120.52689901\n",
      "Iteration 966, loss = 2120.37861363\n",
      "Iteration 967, loss = 2120.23713849\n",
      "Iteration 968, loss = 2120.10445914\n",
      "Iteration 969, loss = 2119.97421797\n",
      "Iteration 970, loss = 2119.84122190\n",
      "Iteration 971, loss = 2119.70106076\n",
      "Iteration 972, loss = 2119.57195594\n",
      "Iteration 973, loss = 2119.44493388\n",
      "Iteration 974, loss = 2119.30858325\n",
      "Iteration 975, loss = 2119.17215772\n",
      "Iteration 976, loss = 2119.04236404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 977, loss = 2118.90307573\n",
      "Iteration 978, loss = 2118.77306243\n",
      "Iteration 979, loss = 2118.64150404\n",
      "Iteration 980, loss = 2118.51995297\n",
      "Iteration 981, loss = 2118.39672333\n",
      "Iteration 982, loss = 2118.24394695\n",
      "Iteration 983, loss = 2118.11303967\n",
      "Iteration 984, loss = 2117.98160887\n",
      "Iteration 985, loss = 2117.85589691\n",
      "Iteration 986, loss = 2117.72125184\n",
      "Iteration 987, loss = 2117.59011944\n",
      "Iteration 988, loss = 2117.46539387\n",
      "Iteration 989, loss = 2117.33020204\n",
      "Iteration 990, loss = 2117.20817397\n",
      "Iteration 991, loss = 2117.09400731\n",
      "Iteration 992, loss = 2116.95421582\n",
      "Iteration 993, loss = 2116.81879754\n",
      "Iteration 994, loss = 2116.69955178\n",
      "Iteration 995, loss = 2116.55962355\n",
      "Iteration 996, loss = 2116.44305919\n",
      "Iteration 997, loss = 2116.31753481\n",
      "Iteration 998, loss = 2116.18341608\n",
      "Iteration 999, loss = 2116.05809418\n",
      "Iteration 1000, loss = 2115.92966690\n",
      "Iteration 1001, loss = 2115.80525508\n",
      "Iteration 1002, loss = 2115.67360818\n",
      "Iteration 1003, loss = 2115.54857275\n",
      "Iteration 1004, loss = 2115.42483345\n",
      "Iteration 1005, loss = 2115.30057553\n",
      "Iteration 1006, loss = 2115.16980307\n",
      "Iteration 1007, loss = 2115.05366390\n",
      "Iteration 1008, loss = 2114.91736210\n",
      "Iteration 1009, loss = 2114.79623595\n",
      "Iteration 1010, loss = 2114.67054479\n",
      "Iteration 1011, loss = 2114.55416608\n",
      "Iteration 1012, loss = 2114.42916823\n",
      "Iteration 1013, loss = 2114.29382839\n",
      "Iteration 1014, loss = 2114.17034704\n",
      "Iteration 1015, loss = 2114.04652527\n",
      "Iteration 1016, loss = 2113.92016031\n",
      "Iteration 1017, loss = 2113.79908759\n",
      "Iteration 1018, loss = 2113.67015520\n",
      "Iteration 1019, loss = 2113.54924125\n",
      "Iteration 1020, loss = 2113.42108943\n",
      "Iteration 1021, loss = 2113.30085798\n",
      "Iteration 1022, loss = 2113.17624769\n",
      "Iteration 1023, loss = 2113.05245217\n",
      "Iteration 1024, loss = 2112.93418729\n",
      "Iteration 1025, loss = 2112.80306257\n",
      "Iteration 1026, loss = 2112.68810728\n",
      "Iteration 1027, loss = 2112.55899266\n",
      "Iteration 1028, loss = 2112.43500159\n",
      "Iteration 1029, loss = 2112.32032170\n",
      "Iteration 1030, loss = 2112.18925901\n",
      "Iteration 1031, loss = 2112.06948268\n",
      "Iteration 1032, loss = 2111.94329159\n",
      "Iteration 1033, loss = 2111.82786797\n",
      "Iteration 1034, loss = 2111.69489569\n",
      "Iteration 1035, loss = 2111.58801953\n",
      "Iteration 1036, loss = 2111.45021657\n",
      "Iteration 1037, loss = 2111.32624230\n",
      "Iteration 1038, loss = 2111.21003658\n",
      "Iteration 1039, loss = 2111.08165409\n",
      "Iteration 1040, loss = 2110.96634835\n",
      "Iteration 1041, loss = 2110.84203468\n",
      "Iteration 1042, loss = 2110.71921153\n",
      "Iteration 1043, loss = 2110.59691221\n",
      "Iteration 1044, loss = 2110.47302295\n",
      "Iteration 1045, loss = 2110.35215753\n",
      "Iteration 1046, loss = 2110.22688634\n",
      "Iteration 1047, loss = 2110.11183887\n",
      "Iteration 1048, loss = 2109.99262353\n",
      "Iteration 1049, loss = 2109.86514379\n",
      "Iteration 1050, loss = 2109.74400462\n",
      "Iteration 1051, loss = 2109.63675759\n",
      "Iteration 1052, loss = 2109.49909898\n",
      "Iteration 1053, loss = 2109.37725731\n",
      "Iteration 1054, loss = 2109.25486862\n",
      "Iteration 1055, loss = 2109.13349907\n",
      "Iteration 1056, loss = 2109.01800992\n",
      "Iteration 1057, loss = 2108.89974904\n",
      "Iteration 1058, loss = 2108.78244466\n",
      "Iteration 1059, loss = 2108.64700711\n",
      "Iteration 1060, loss = 2108.53080646\n",
      "Iteration 1061, loss = 2108.40270055\n",
      "Iteration 1062, loss = 2108.28328695\n",
      "Iteration 1063, loss = 2108.16062413\n",
      "Iteration 1064, loss = 2108.03957023\n",
      "Iteration 1065, loss = 2107.92620629\n",
      "Iteration 1066, loss = 2107.79416422\n",
      "Iteration 1067, loss = 2107.67908221\n",
      "Iteration 1068, loss = 2107.55949721\n",
      "Iteration 1069, loss = 2107.44886676\n",
      "Iteration 1070, loss = 2107.31864771\n",
      "Iteration 1071, loss = 2107.20575374\n",
      "Iteration 1072, loss = 2107.06589872\n",
      "Iteration 1073, loss = 2106.94968782\n",
      "Iteration 1074, loss = 2106.82542968\n",
      "Iteration 1075, loss = 2106.70978712\n",
      "Iteration 1076, loss = 2106.58645083\n",
      "Iteration 1077, loss = 2106.46147114\n",
      "Iteration 1078, loss = 2106.33913006\n",
      "Iteration 1079, loss = 2106.22837225\n",
      "Iteration 1080, loss = 2106.10486412\n",
      "Iteration 1081, loss = 2105.97832955\n",
      "Iteration 1082, loss = 2105.86224302\n",
      "Iteration 1083, loss = 2105.73916720\n",
      "Iteration 1084, loss = 2105.61480844\n",
      "Iteration 1085, loss = 2105.50062318\n",
      "Iteration 1086, loss = 2105.38737693\n",
      "Iteration 1087, loss = 2105.26743135\n",
      "Iteration 1088, loss = 2105.13113655\n",
      "Iteration 1089, loss = 2105.01723155\n",
      "Iteration 1090, loss = 2104.89119365\n",
      "Iteration 1091, loss = 2104.76723519\n",
      "Iteration 1092, loss = 2104.65872168\n",
      "Iteration 1093, loss = 2104.52819497\n",
      "Iteration 1094, loss = 2104.42309325\n",
      "Iteration 1095, loss = 2104.28973982\n",
      "Iteration 1096, loss = 2104.17581051\n",
      "Iteration 1097, loss = 2104.04223724\n",
      "Iteration 1098, loss = 2103.92796035\n",
      "Iteration 1099, loss = 2103.80755955\n",
      "Iteration 1100, loss = 2103.68025550\n",
      "Iteration 1101, loss = 2103.56599977\n",
      "Iteration 1102, loss = 2103.44091943\n",
      "Iteration 1103, loss = 2103.32240783\n",
      "Iteration 1104, loss = 2103.19951968\n",
      "Iteration 1105, loss = 2103.08031426\n",
      "Iteration 1106, loss = 2102.95721337\n",
      "Iteration 1107, loss = 2102.83682285\n",
      "Iteration 1108, loss = 2102.71597566\n",
      "Iteration 1109, loss = 2102.60679259\n",
      "Iteration 1110, loss = 2102.47918259\n",
      "Iteration 1111, loss = 2102.36010065\n",
      "Iteration 1112, loss = 2102.23607620\n",
      "Iteration 1113, loss = 2102.11534218\n",
      "Iteration 1114, loss = 2101.99089510\n",
      "Iteration 1115, loss = 2101.87949543\n",
      "Iteration 1116, loss = 2101.74816787\n",
      "Iteration 1117, loss = 2101.63203111\n",
      "Iteration 1118, loss = 2101.51004487\n",
      "Iteration 1119, loss = 2101.41243945\n",
      "Iteration 1120, loss = 2101.26579969\n",
      "Iteration 1121, loss = 2101.14906927\n",
      "Iteration 1122, loss = 2101.02658652\n",
      "Iteration 1123, loss = 2100.90651028\n",
      "Iteration 1124, loss = 2100.78253365\n",
      "Iteration 1125, loss = 2100.66535629\n",
      "Iteration 1126, loss = 2100.54243339\n",
      "Iteration 1127, loss = 2100.41977617\n",
      "Iteration 1128, loss = 2100.31132002\n",
      "Iteration 1129, loss = 2100.17628775\n",
      "Iteration 1130, loss = 2100.05773424\n",
      "Iteration 1131, loss = 2099.93944125\n",
      "Iteration 1132, loss = 2099.81948340\n",
      "Iteration 1133, loss = 2099.70385953\n",
      "Iteration 1134, loss = 2099.59077170\n",
      "Iteration 1135, loss = 2099.45734758\n",
      "Iteration 1136, loss = 2099.33179331\n",
      "Iteration 1137, loss = 2099.21429686\n",
      "Iteration 1138, loss = 2099.10258484\n",
      "Iteration 1139, loss = 2098.97769977\n",
      "Iteration 1140, loss = 2098.85183869\n",
      "Iteration 1141, loss = 2098.73253190\n",
      "Iteration 1142, loss = 2098.61198621\n",
      "Iteration 1143, loss = 2098.50495743\n",
      "Iteration 1144, loss = 2098.36692990\n",
      "Iteration 1145, loss = 2098.25255363\n",
      "Iteration 1146, loss = 2098.13016803\n",
      "Iteration 1147, loss = 2098.00592582\n",
      "Iteration 1148, loss = 2097.90246021\n",
      "Iteration 1149, loss = 2097.77186290\n",
      "Iteration 1150, loss = 2097.64615794\n",
      "Iteration 1151, loss = 2097.52790469\n",
      "Iteration 1152, loss = 2097.40654849\n",
      "Iteration 1153, loss = 2097.28815293\n",
      "Iteration 1154, loss = 2097.16696910\n",
      "Iteration 1155, loss = 2097.04399657\n",
      "Iteration 1156, loss = 2096.93420144\n",
      "Iteration 1157, loss = 2096.80602804\n",
      "Iteration 1158, loss = 2096.69977679\n",
      "Iteration 1159, loss = 2096.55769751\n",
      "Iteration 1160, loss = 2096.44119848\n",
      "Iteration 1161, loss = 2096.32513836\n",
      "Iteration 1162, loss = 2096.20488833\n",
      "Iteration 1163, loss = 2096.08131442\n",
      "Iteration 1164, loss = 2095.95881087\n",
      "Iteration 1165, loss = 2095.84399268\n",
      "Iteration 1166, loss = 2095.71478837\n",
      "Iteration 1167, loss = 2095.59711556\n",
      "Iteration 1168, loss = 2095.47734353\n",
      "Iteration 1169, loss = 2095.35888751\n",
      "Iteration 1170, loss = 2095.24692389\n",
      "Iteration 1171, loss = 2095.12009155\n",
      "Iteration 1172, loss = 2094.99769480\n",
      "Iteration 1173, loss = 2094.87647128\n",
      "Iteration 1174, loss = 2094.75582289\n",
      "Iteration 1175, loss = 2094.63457213\n",
      "Iteration 1176, loss = 2094.51355631\n",
      "Iteration 1177, loss = 2094.40579144\n",
      "Iteration 1178, loss = 2094.27346465\n",
      "Iteration 1179, loss = 2094.15071506\n",
      "Iteration 1180, loss = 2094.03336319\n",
      "Iteration 1181, loss = 2093.91405895\n",
      "Iteration 1182, loss = 2093.79389300\n",
      "Iteration 1183, loss = 2093.66850188\n",
      "Iteration 1184, loss = 2093.54969913\n",
      "Iteration 1185, loss = 2093.44391221\n",
      "Iteration 1186, loss = 2093.34356180\n",
      "Iteration 1187, loss = 2093.18878124\n",
      "Iteration 1188, loss = 2093.06918597\n",
      "Iteration 1189, loss = 2092.94716394\n",
      "Iteration 1190, loss = 2092.83386140\n",
      "Iteration 1191, loss = 2092.71474196\n",
      "Iteration 1192, loss = 2092.58612499\n",
      "Iteration 1193, loss = 2092.47696571\n",
      "Iteration 1194, loss = 2092.36596908\n",
      "Iteration 1195, loss = 2092.23375719\n",
      "Iteration 1196, loss = 2092.10383393\n",
      "Iteration 1197, loss = 2091.98958633\n",
      "Iteration 1198, loss = 2091.87471077\n",
      "Iteration 1199, loss = 2091.74301253\n",
      "Iteration 1200, loss = 2091.62873461\n",
      "Iteration 1201, loss = 2091.50321419\n",
      "Iteration 1202, loss = 2091.38508010\n",
      "Iteration 1203, loss = 2091.26479773\n",
      "Iteration 1204, loss = 2091.14955668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1205, loss = 2091.02366153\n",
      "Iteration 1206, loss = 2090.90951614\n",
      "Iteration 1207, loss = 2090.78675902\n",
      "Iteration 1208, loss = 2090.66244803\n",
      "Iteration 1209, loss = 2090.56116233\n",
      "Iteration 1210, loss = 2090.42922220\n",
      "Iteration 1211, loss = 2090.30389982\n",
      "Iteration 1212, loss = 2090.18953378\n",
      "Iteration 1213, loss = 2090.07045701\n",
      "Iteration 1214, loss = 2089.94334445\n",
      "Iteration 1215, loss = 2089.82356893\n",
      "Iteration 1216, loss = 2089.70305012\n",
      "Iteration 1217, loss = 2089.58414438\n",
      "Iteration 1218, loss = 2089.46126169\n",
      "Iteration 1219, loss = 2089.33891571\n",
      "Iteration 1220, loss = 2089.22634991\n",
      "Iteration 1221, loss = 2089.09877650\n",
      "Iteration 1222, loss = 2088.97862625\n",
      "Iteration 1223, loss = 2088.86898298\n",
      "Iteration 1224, loss = 2088.73978135\n",
      "Iteration 1225, loss = 2088.63028348\n",
      "Iteration 1226, loss = 2088.51064559\n",
      "Iteration 1227, loss = 2088.38195220\n",
      "Iteration 1228, loss = 2088.26119287\n",
      "Iteration 1229, loss = 2088.17419951\n",
      "Iteration 1230, loss = 2088.01900084\n",
      "Iteration 1231, loss = 2087.91908527\n",
      "Iteration 1232, loss = 2087.79832209\n",
      "Iteration 1233, loss = 2087.65818995\n",
      "Iteration 1234, loss = 2087.55210615\n",
      "Iteration 1235, loss = 2087.42137672\n",
      "Iteration 1236, loss = 2087.30253937\n",
      "Iteration 1237, loss = 2087.18431648\n",
      "Iteration 1238, loss = 2087.07348436\n",
      "Iteration 1239, loss = 2086.94118674\n",
      "Iteration 1240, loss = 2086.82115175\n",
      "Iteration 1241, loss = 2086.70887877\n",
      "Iteration 1242, loss = 2086.58458762\n",
      "Iteration 1243, loss = 2086.46496787\n",
      "Iteration 1244, loss = 2086.34410854\n",
      "Iteration 1245, loss = 2086.23134718\n",
      "Iteration 1246, loss = 2086.11273367\n",
      "Iteration 1247, loss = 2085.98693156\n",
      "Iteration 1248, loss = 2085.86441400\n",
      "Iteration 1249, loss = 2085.74516774\n",
      "Iteration 1250, loss = 2085.62761394\n",
      "Iteration 1251, loss = 2085.50780291\n",
      "Iteration 1252, loss = 2085.38733538\n",
      "Iteration 1253, loss = 2085.27042936\n",
      "Iteration 1254, loss = 2085.14835762\n",
      "Iteration 1255, loss = 2085.03140601\n",
      "Iteration 1256, loss = 2084.91944180\n",
      "Iteration 1257, loss = 2084.78729402\n",
      "Iteration 1258, loss = 2084.66858027\n",
      "Iteration 1259, loss = 2084.55187711\n",
      "Iteration 1260, loss = 2084.43037034\n",
      "Iteration 1261, loss = 2084.31303424\n",
      "Iteration 1262, loss = 2084.20006453\n",
      "Iteration 1263, loss = 2084.06509260\n",
      "Iteration 1264, loss = 2083.94969108\n",
      "Iteration 1265, loss = 2083.83321563\n",
      "Iteration 1266, loss = 2083.72073591\n",
      "Iteration 1267, loss = 2083.62532409\n",
      "Iteration 1268, loss = 2083.47014316\n",
      "Iteration 1269, loss = 2083.35207544\n",
      "Iteration 1270, loss = 2083.23324367\n",
      "Iteration 1271, loss = 2083.11948575\n",
      "Iteration 1272, loss = 2083.00750853\n",
      "Iteration 1273, loss = 2082.87263590\n",
      "Iteration 1274, loss = 2082.75823941\n",
      "Iteration 1275, loss = 2082.64361168\n",
      "Iteration 1276, loss = 2082.52647092\n",
      "Iteration 1277, loss = 2082.39582447\n",
      "Iteration 1278, loss = 2082.27651976\n",
      "Iteration 1279, loss = 2082.16362602\n",
      "Iteration 1280, loss = 2082.04440362\n",
      "Iteration 1281, loss = 2081.92185403\n",
      "Iteration 1282, loss = 2081.79854373\n",
      "Iteration 1283, loss = 2081.67921364\n",
      "Iteration 1284, loss = 2081.56454389\n",
      "Iteration 1285, loss = 2081.45643321\n",
      "Iteration 1286, loss = 2081.33191042\n",
      "Iteration 1287, loss = 2081.20763608\n",
      "Iteration 1288, loss = 2081.08610998\n",
      "Iteration 1289, loss = 2080.96781391\n",
      "Iteration 1290, loss = 2080.84791580\n",
      "Iteration 1291, loss = 2080.73033306\n",
      "Iteration 1292, loss = 2080.61000796\n",
      "Iteration 1293, loss = 2080.48911490\n",
      "Iteration 1294, loss = 2080.36639891\n",
      "Iteration 1295, loss = 2080.26503468\n",
      "Iteration 1296, loss = 2080.13544794\n",
      "Iteration 1297, loss = 2080.01316162\n",
      "Iteration 1298, loss = 2079.89813145\n",
      "Iteration 1299, loss = 2079.77275861\n",
      "Iteration 1300, loss = 2079.65733077\n",
      "Iteration 1301, loss = 2079.53852535\n",
      "Iteration 1302, loss = 2079.42783919\n",
      "Iteration 1303, loss = 2079.30537366\n",
      "Iteration 1304, loss = 2079.18333384\n",
      "Iteration 1305, loss = 2079.06058557\n",
      "Iteration 1306, loss = 2078.94892992\n",
      "Iteration 1307, loss = 2078.82078420\n",
      "Iteration 1308, loss = 2078.70504112\n",
      "Iteration 1309, loss = 2078.59280566\n",
      "Iteration 1310, loss = 2078.46659959\n",
      "Iteration 1311, loss = 2078.34546021\n",
      "Iteration 1312, loss = 2078.23053927\n",
      "Iteration 1313, loss = 2078.10833282\n",
      "Iteration 1314, loss = 2078.01209074\n",
      "Iteration 1315, loss = 2077.87586417\n",
      "Iteration 1316, loss = 2077.74962537\n",
      "Iteration 1317, loss = 2077.63103131\n",
      "Iteration 1318, loss = 2077.52177161\n",
      "Iteration 1319, loss = 2077.42438900\n",
      "Iteration 1320, loss = 2077.27735269\n",
      "Iteration 1321, loss = 2077.16439324\n",
      "Iteration 1322, loss = 2077.04587432\n",
      "Iteration 1323, loss = 2076.92376900\n",
      "Iteration 1324, loss = 2076.80939534\n",
      "Iteration 1325, loss = 2076.70607726\n",
      "Iteration 1326, loss = 2076.56728080\n",
      "Iteration 1327, loss = 2076.44637984\n",
      "Iteration 1328, loss = 2076.33152146\n",
      "Iteration 1329, loss = 2076.21681773\n",
      "Iteration 1330, loss = 2076.09256236\n",
      "Iteration 1331, loss = 2075.98273552\n",
      "Iteration 1332, loss = 2075.85387383\n",
      "Iteration 1333, loss = 2075.73407971\n",
      "Iteration 1334, loss = 2075.61513764\n",
      "Iteration 1335, loss = 2075.50297274\n",
      "Iteration 1336, loss = 2075.38411791\n",
      "Iteration 1337, loss = 2075.26400395\n",
      "Iteration 1338, loss = 2075.14781614\n",
      "Iteration 1339, loss = 2075.03285575\n",
      "Iteration 1340, loss = 2074.91503839\n",
      "Iteration 1341, loss = 2074.78830921\n",
      "Iteration 1342, loss = 2074.66842675\n",
      "Iteration 1343, loss = 2074.55080136\n",
      "Iteration 1344, loss = 2074.43205772\n",
      "Iteration 1345, loss = 2074.31279028\n",
      "Iteration 1346, loss = 2074.19483894\n",
      "Iteration 1347, loss = 2074.07643486\n",
      "Iteration 1348, loss = 2073.96052455\n",
      "Iteration 1349, loss = 2073.84377460\n",
      "Iteration 1350, loss = 2073.73644517\n",
      "Iteration 1351, loss = 2073.59993918\n",
      "Iteration 1352, loss = 2073.50127659\n",
      "Iteration 1353, loss = 2073.37359989\n",
      "Iteration 1354, loss = 2073.24817460\n",
      "Iteration 1355, loss = 2073.13107440\n",
      "Iteration 1356, loss = 2073.00958964\n",
      "Iteration 1357, loss = 2072.89497178\n",
      "Iteration 1358, loss = 2072.77280651\n",
      "Iteration 1359, loss = 2072.65995282\n",
      "Iteration 1360, loss = 2072.54180486\n",
      "Iteration 1361, loss = 2072.44221912\n",
      "Iteration 1362, loss = 2072.30464643\n",
      "Iteration 1363, loss = 2072.19307168\n",
      "Iteration 1364, loss = 2072.06559464\n",
      "Iteration 1365, loss = 2071.94975185\n",
      "Iteration 1366, loss = 2071.83292227\n",
      "Iteration 1367, loss = 2071.71428466\n",
      "Iteration 1368, loss = 2071.59867825\n",
      "Iteration 1369, loss = 2071.47888644\n",
      "Iteration 1370, loss = 2071.35549476\n",
      "Iteration 1371, loss = 2071.25401261\n",
      "Iteration 1372, loss = 2071.12970315\n",
      "Iteration 1373, loss = 2071.00485029\n",
      "Iteration 1374, loss = 2070.89391307\n",
      "Iteration 1375, loss = 2070.77025063\n",
      "Iteration 1376, loss = 2070.64933265\n",
      "Iteration 1377, loss = 2070.53633898\n",
      "Iteration 1378, loss = 2070.43553648\n",
      "Iteration 1379, loss = 2070.31266618\n",
      "Iteration 1380, loss = 2070.18356424\n",
      "Iteration 1381, loss = 2070.06028861\n",
      "Iteration 1382, loss = 2069.94392287\n",
      "Iteration 1383, loss = 2069.82663768\n",
      "Iteration 1384, loss = 2069.71335358\n",
      "Iteration 1385, loss = 2069.59758175\n",
      "Iteration 1386, loss = 2069.47851106\n",
      "Iteration 1387, loss = 2069.36884947\n",
      "Iteration 1388, loss = 2069.24372490\n",
      "Iteration 1389, loss = 2069.12251134\n",
      "Iteration 1390, loss = 2068.99964577\n",
      "Iteration 1391, loss = 2068.88229681\n",
      "Iteration 1392, loss = 2068.77074742\n",
      "Iteration 1393, loss = 2068.65090072\n",
      "Iteration 1394, loss = 2068.53308286\n",
      "Iteration 1395, loss = 2068.41619586\n",
      "Iteration 1396, loss = 2068.30473912\n",
      "Iteration 1397, loss = 2068.18566807\n",
      "Iteration 1398, loss = 2068.06645726\n",
      "Iteration 1399, loss = 2067.97760548\n",
      "Iteration 1400, loss = 2067.82765194\n",
      "Iteration 1401, loss = 2067.71010273\n",
      "Iteration 1402, loss = 2067.61467569\n",
      "Iteration 1403, loss = 2067.47512504\n",
      "Iteration 1404, loss = 2067.37209956\n",
      "Iteration 1405, loss = 2067.25602502\n",
      "Iteration 1406, loss = 2067.12212227\n",
      "Iteration 1407, loss = 2067.00933442\n",
      "Iteration 1408, loss = 2066.89171357\n",
      "Iteration 1409, loss = 2066.77045280\n",
      "Iteration 1410, loss = 2066.65095057\n",
      "Iteration 1411, loss = 2066.53511472\n",
      "Iteration 1412, loss = 2066.42134841\n",
      "Iteration 1413, loss = 2066.30767586\n",
      "Iteration 1414, loss = 2066.17999338\n",
      "Iteration 1415, loss = 2066.06742772\n",
      "Iteration 1416, loss = 2065.94865173\n",
      "Iteration 1417, loss = 2065.84024818\n",
      "Iteration 1418, loss = 2065.72544273\n",
      "Iteration 1419, loss = 2065.60762150\n",
      "Iteration 1420, loss = 2065.49833651\n",
      "Iteration 1421, loss = 2065.36672725\n",
      "Iteration 1422, loss = 2065.24872672\n",
      "Iteration 1423, loss = 2065.12936486\n",
      "Iteration 1424, loss = 2065.02248962\n",
      "Iteration 1425, loss = 2064.90059331\n",
      "Iteration 1426, loss = 2064.77570362\n",
      "Iteration 1427, loss = 2064.66412231\n",
      "Iteration 1428, loss = 2064.54393994\n",
      "Iteration 1429, loss = 2064.42666867\n",
      "Iteration 1430, loss = 2064.30711595\n",
      "Iteration 1431, loss = 2064.19206925\n",
      "Iteration 1432, loss = 2064.07759266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1433, loss = 2063.95877075\n",
      "Iteration 1434, loss = 2063.84310090\n",
      "Iteration 1435, loss = 2063.73392274\n",
      "Iteration 1436, loss = 2063.60619124\n",
      "Iteration 1437, loss = 2063.48803363\n",
      "Iteration 1438, loss = 2063.38090734\n",
      "Iteration 1439, loss = 2063.25609659\n",
      "Iteration 1440, loss = 2063.14034834\n",
      "Iteration 1441, loss = 2063.01968338\n",
      "Iteration 1442, loss = 2062.90307275\n",
      "Iteration 1443, loss = 2062.79244634\n",
      "Iteration 1444, loss = 2062.67258607\n",
      "Iteration 1445, loss = 2062.55420301\n",
      "Iteration 1446, loss = 2062.44347163\n",
      "Iteration 1447, loss = 2062.32555808\n",
      "Iteration 1448, loss = 2062.20756229\n",
      "Iteration 1449, loss = 2062.08535797\n",
      "Iteration 1450, loss = 2061.97189251\n",
      "Iteration 1451, loss = 2061.85848752\n",
      "Iteration 1452, loss = 2061.73677365\n",
      "Iteration 1453, loss = 2061.62076676\n",
      "Iteration 1454, loss = 2061.51051760\n",
      "Iteration 1455, loss = 2061.40952273\n",
      "Iteration 1456, loss = 2061.27164535\n",
      "Iteration 1457, loss = 2061.15934280\n",
      "Iteration 1458, loss = 2061.06548622\n",
      "Iteration 1459, loss = 2060.92086031\n",
      "Iteration 1460, loss = 2060.80535441\n",
      "Iteration 1461, loss = 2060.68637190\n",
      "Iteration 1462, loss = 2060.58411992\n",
      "Iteration 1463, loss = 2060.45632921\n",
      "Iteration 1464, loss = 2060.33990776\n",
      "Iteration 1465, loss = 2060.22105305\n",
      "Iteration 1466, loss = 2060.10421506\n",
      "Iteration 1467, loss = 2059.99294145\n",
      "Iteration 1468, loss = 2059.88248026\n",
      "Iteration 1469, loss = 2059.77517416\n",
      "Iteration 1470, loss = 2059.64367623\n",
      "Iteration 1471, loss = 2059.53403326\n",
      "Iteration 1472, loss = 2059.41183780\n",
      "Iteration 1473, loss = 2059.29349386\n",
      "Iteration 1474, loss = 2059.17354050\n",
      "Iteration 1475, loss = 2059.05719235\n",
      "Iteration 1476, loss = 2058.95558564\n",
      "Iteration 1477, loss = 2058.82980635\n",
      "Iteration 1478, loss = 2058.70934004\n",
      "Iteration 1479, loss = 2058.59397355\n",
      "Iteration 1480, loss = 2058.48540447\n",
      "Iteration 1481, loss = 2058.36398448\n",
      "Iteration 1482, loss = 2058.25592874\n",
      "Iteration 1483, loss = 2058.13651769\n",
      "Iteration 1484, loss = 2058.03356370\n",
      "Iteration 1485, loss = 2057.90269461\n",
      "Iteration 1486, loss = 2057.78087050\n",
      "Iteration 1487, loss = 2057.66228522\n",
      "Iteration 1488, loss = 2057.55106465\n",
      "Iteration 1489, loss = 2057.43832045\n",
      "Iteration 1490, loss = 2057.32022865\n",
      "Iteration 1491, loss = 2057.21644731\n",
      "Iteration 1492, loss = 2057.08156480\n",
      "Iteration 1493, loss = 2056.96773992\n",
      "Iteration 1494, loss = 2056.85782554\n",
      "Iteration 1495, loss = 2056.74002293\n",
      "Iteration 1496, loss = 2056.62351910\n",
      "Iteration 1497, loss = 2056.50558278\n",
      "Iteration 1498, loss = 2056.39185087\n",
      "Iteration 1499, loss = 2056.27905821\n",
      "Iteration 1500, loss = 2056.18260312\n",
      "Iteration 1501, loss = 2056.05394821\n",
      "Iteration 1502, loss = 2055.92915909\n",
      "Iteration 1503, loss = 2055.81026096\n",
      "Iteration 1504, loss = 2055.69806766\n",
      "Iteration 1505, loss = 2055.57896222\n",
      "Iteration 1506, loss = 2055.47151970\n",
      "Iteration 1507, loss = 2055.36068419\n",
      "Iteration 1508, loss = 2055.24755644\n",
      "Iteration 1509, loss = 2055.11513594\n",
      "Iteration 1510, loss = 2055.03930867\n",
      "Iteration 1511, loss = 2054.89601717\n",
      "Iteration 1512, loss = 2054.76793423\n",
      "Iteration 1513, loss = 2054.66057940\n",
      "Iteration 1514, loss = 2054.53521423\n",
      "Iteration 1515, loss = 2054.43950616\n",
      "Iteration 1516, loss = 2054.30413672\n",
      "Iteration 1517, loss = 2054.19252889\n",
      "Iteration 1518, loss = 2054.07897224\n",
      "Iteration 1519, loss = 2053.96451294\n",
      "Iteration 1520, loss = 2053.85875371\n",
      "Iteration 1521, loss = 2053.73752966\n",
      "Iteration 1522, loss = 2053.61672258\n",
      "Iteration 1523, loss = 2053.49572063\n",
      "Iteration 1524, loss = 2053.38620465\n",
      "Iteration 1525, loss = 2053.26643237\n",
      "Iteration 1526, loss = 2053.15259481\n",
      "Iteration 1527, loss = 2053.03752119\n",
      "Iteration 1528, loss = 2052.92018493\n",
      "Iteration 1529, loss = 2052.80890689\n",
      "Iteration 1530, loss = 2052.69930414\n",
      "Iteration 1531, loss = 2052.57475100\n",
      "Iteration 1532, loss = 2052.47573215\n",
      "Iteration 1533, loss = 2052.34469082\n",
      "Iteration 1534, loss = 2052.22932887\n",
      "Iteration 1535, loss = 2052.11635274\n",
      "Iteration 1536, loss = 2052.00307135\n",
      "Iteration 1537, loss = 2051.88279319\n",
      "Iteration 1538, loss = 2051.76739918\n",
      "Iteration 1539, loss = 2051.65145876\n",
      "Iteration 1540, loss = 2051.53770755\n",
      "Iteration 1541, loss = 2051.42480345\n",
      "Iteration 1542, loss = 2051.30813252\n",
      "Iteration 1543, loss = 2051.20542184\n",
      "Iteration 1544, loss = 2051.09180458\n",
      "Iteration 1545, loss = 2050.96915183\n",
      "Iteration 1546, loss = 2050.84506315\n",
      "Iteration 1547, loss = 2050.73195489\n",
      "Iteration 1548, loss = 2050.63015578\n",
      "Iteration 1549, loss = 2050.50344153\n",
      "Iteration 1550, loss = 2050.38594058\n",
      "Iteration 1551, loss = 2050.28128533\n",
      "Iteration 1552, loss = 2050.15816139\n",
      "Iteration 1553, loss = 2050.04501956\n",
      "Iteration 1554, loss = 2049.93673995\n",
      "Iteration 1555, loss = 2049.81468929\n",
      "Iteration 1556, loss = 2049.69461558\n",
      "Iteration 1557, loss = 2049.57966318\n",
      "Iteration 1558, loss = 2049.47514417\n",
      "Iteration 1559, loss = 2049.35395460\n",
      "Iteration 1560, loss = 2049.24914190\n",
      "Iteration 1561, loss = 2049.12125636\n",
      "Iteration 1562, loss = 2049.00942042\n",
      "Iteration 1563, loss = 2048.89819346\n",
      "Iteration 1564, loss = 2048.78002088\n",
      "Iteration 1565, loss = 2048.66375168\n",
      "Iteration 1566, loss = 2048.55428791\n",
      "Iteration 1567, loss = 2048.43478180\n",
      "Iteration 1568, loss = 2048.31793914\n",
      "Iteration 1569, loss = 2048.20800452\n",
      "Iteration 1570, loss = 2048.09856320\n",
      "Iteration 1571, loss = 2047.98167701\n",
      "Iteration 1572, loss = 2047.87587287\n",
      "Iteration 1573, loss = 2047.74822401\n",
      "Iteration 1574, loss = 2047.63473481\n",
      "Iteration 1575, loss = 2047.51879289\n",
      "Iteration 1576, loss = 2047.39982385\n",
      "Iteration 1577, loss = 2047.31565433\n",
      "Iteration 1578, loss = 2047.17289479\n",
      "Iteration 1579, loss = 2047.06135205\n",
      "Iteration 1580, loss = 2046.94292040\n",
      "Iteration 1581, loss = 2046.83135798\n",
      "Iteration 1582, loss = 2046.71478504\n",
      "Iteration 1583, loss = 2046.60103785\n",
      "Iteration 1584, loss = 2046.48907261\n",
      "Iteration 1585, loss = 2046.38194776\n",
      "Iteration 1586, loss = 2046.25754678\n",
      "Iteration 1587, loss = 2046.15216948\n",
      "Iteration 1588, loss = 2046.03057840\n",
      "Iteration 1589, loss = 2045.92623724\n",
      "Iteration 1590, loss = 2045.81162849\n",
      "Iteration 1591, loss = 2045.69543284\n",
      "Iteration 1592, loss = 2045.57504435\n",
      "Iteration 1593, loss = 2045.46559356\n",
      "Iteration 1594, loss = 2045.34455261\n",
      "Iteration 1595, loss = 2045.23306530\n",
      "Iteration 1596, loss = 2045.11450475\n",
      "Iteration 1597, loss = 2045.00314015\n",
      "Iteration 1598, loss = 2044.89382790\n",
      "Iteration 1599, loss = 2044.77655576\n",
      "Iteration 1600, loss = 2044.66517472\n",
      "Iteration 1601, loss = 2044.54805922\n",
      "Iteration 1602, loss = 2044.43630137\n",
      "Iteration 1603, loss = 2044.31710727\n",
      "Iteration 1604, loss = 2044.20401891\n",
      "Iteration 1605, loss = 2044.08832076\n",
      "Iteration 1606, loss = 2043.97531728\n",
      "Iteration 1607, loss = 2043.86553220\n",
      "Iteration 1608, loss = 2043.74899267\n",
      "Iteration 1609, loss = 2043.63940693\n",
      "Iteration 1610, loss = 2043.52364550\n",
      "Iteration 1611, loss = 2043.40691396\n",
      "Iteration 1612, loss = 2043.29179644\n",
      "Iteration 1613, loss = 2043.18638891\n",
      "Iteration 1614, loss = 2043.06421258\n",
      "Iteration 1615, loss = 2042.94725900\n",
      "Iteration 1616, loss = 2042.83676791\n",
      "Iteration 1617, loss = 2042.72148618\n",
      "Iteration 1618, loss = 2042.61804337\n",
      "Iteration 1619, loss = 2042.49982008\n",
      "Iteration 1620, loss = 2042.38651277\n",
      "Iteration 1621, loss = 2042.26582223\n",
      "Iteration 1622, loss = 2042.15209818\n",
      "Iteration 1623, loss = 2042.04607184\n",
      "Iteration 1624, loss = 2041.92745131\n",
      "Iteration 1625, loss = 2041.81446336\n",
      "Iteration 1626, loss = 2041.70255125\n",
      "Iteration 1627, loss = 2041.59854799\n",
      "Iteration 1628, loss = 2041.47095791\n",
      "Iteration 1629, loss = 2041.37045106\n",
      "Iteration 1630, loss = 2041.24748159\n",
      "Iteration 1631, loss = 2041.14008599\n",
      "Iteration 1632, loss = 2041.01919512\n",
      "Iteration 1633, loss = 2040.90910186\n",
      "Iteration 1634, loss = 2040.79511709\n",
      "Iteration 1635, loss = 2040.68452752\n",
      "Iteration 1636, loss = 2040.55738693\n",
      "Iteration 1637, loss = 2040.45686757\n",
      "Iteration 1638, loss = 2040.35133773\n",
      "Iteration 1639, loss = 2040.22692996\n",
      "Iteration 1640, loss = 2040.10181710\n",
      "Iteration 1641, loss = 2039.99105247\n",
      "Iteration 1642, loss = 2039.88039764\n",
      "Iteration 1643, loss = 2039.77177526\n",
      "Iteration 1644, loss = 2039.65887430\n",
      "Iteration 1645, loss = 2039.53953692\n",
      "Iteration 1646, loss = 2039.42913823\n",
      "Iteration 1647, loss = 2039.31808971\n",
      "Iteration 1648, loss = 2039.20272337\n",
      "Iteration 1649, loss = 2039.08670030\n",
      "Iteration 1650, loss = 2038.97863766\n",
      "Iteration 1651, loss = 2038.86192622\n",
      "Iteration 1652, loss = 2038.75439494\n",
      "Iteration 1653, loss = 2038.63452577\n",
      "Iteration 1654, loss = 2038.53357114\n",
      "Iteration 1655, loss = 2038.40724623\n",
      "Iteration 1656, loss = 2038.29361034\n",
      "Iteration 1657, loss = 2038.18105210\n",
      "Iteration 1658, loss = 2038.07025129\n",
      "Iteration 1659, loss = 2037.95690186\n",
      "Iteration 1660, loss = 2037.86647311\n",
      "Iteration 1661, loss = 2037.72609806\n",
      "Iteration 1662, loss = 2037.61770888\n",
      "Iteration 1663, loss = 2037.50568348\n",
      "Iteration 1664, loss = 2037.38945381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1665, loss = 2037.27410018\n",
      "Iteration 1666, loss = 2037.16264771\n",
      "Iteration 1667, loss = 2037.06195925\n",
      "Iteration 1668, loss = 2036.93746540\n",
      "Iteration 1669, loss = 2036.83464591\n",
      "Iteration 1670, loss = 2036.71293202\n",
      "Iteration 1671, loss = 2036.59844801\n",
      "Iteration 1672, loss = 2036.48451236\n",
      "Iteration 1673, loss = 2036.37329845\n",
      "Iteration 1674, loss = 2036.27721643\n",
      "Iteration 1675, loss = 2036.14814786\n",
      "Iteration 1676, loss = 2036.03428164\n",
      "Iteration 1677, loss = 2035.92831243\n",
      "Iteration 1678, loss = 2035.81016476\n",
      "Iteration 1679, loss = 2035.70132978\n",
      "Iteration 1680, loss = 2035.58667686\n",
      "Iteration 1681, loss = 2035.46919575\n",
      "Iteration 1682, loss = 2035.36147615\n",
      "Iteration 1683, loss = 2035.25613052\n",
      "Iteration 1684, loss = 2035.13367843\n",
      "Iteration 1685, loss = 2035.02433584\n",
      "Iteration 1686, loss = 2034.91931713\n",
      "Iteration 1687, loss = 2034.80176786\n",
      "Iteration 1688, loss = 2034.68282689\n",
      "Iteration 1689, loss = 2034.57497559\n",
      "Iteration 1690, loss = 2034.45189494\n",
      "Iteration 1691, loss = 2034.34574245\n",
      "Iteration 1692, loss = 2034.23535222\n",
      "Iteration 1693, loss = 2034.12354548\n",
      "Iteration 1694, loss = 2034.01171906\n",
      "Iteration 1695, loss = 2033.89296458\n",
      "Iteration 1696, loss = 2033.77974957\n",
      "Iteration 1697, loss = 2033.66544903\n",
      "Iteration 1698, loss = 2033.56680935\n",
      "Iteration 1699, loss = 2033.44839508\n",
      "Iteration 1700, loss = 2033.33166312\n",
      "Iteration 1701, loss = 2033.22850736\n",
      "Iteration 1702, loss = 2033.10635329\n",
      "Iteration 1703, loss = 2032.99293717\n",
      "Iteration 1704, loss = 2032.88270568\n",
      "Iteration 1705, loss = 2032.78080900\n",
      "Iteration 1706, loss = 2032.66050104\n",
      "Iteration 1707, loss = 2032.54010818\n",
      "Iteration 1708, loss = 2032.43125550\n",
      "Iteration 1709, loss = 2032.31958797\n",
      "Iteration 1710, loss = 2032.20536255\n",
      "Iteration 1711, loss = 2032.09295539\n",
      "Iteration 1712, loss = 2031.97929583\n",
      "Iteration 1713, loss = 2031.87275844\n",
      "Iteration 1714, loss = 2031.76006851\n",
      "Iteration 1715, loss = 2031.64503737\n",
      "Iteration 1716, loss = 2031.54812400\n",
      "Iteration 1717, loss = 2031.42213050\n",
      "Iteration 1718, loss = 2031.31388733\n",
      "Iteration 1719, loss = 2031.20507558\n",
      "Iteration 1720, loss = 2031.08659736\n",
      "Iteration 1721, loss = 2030.97898914\n",
      "Iteration 1722, loss = 2030.85579049\n",
      "Iteration 1723, loss = 2030.74386986\n",
      "Iteration 1724, loss = 2030.64131627\n",
      "Iteration 1725, loss = 2030.53216117\n",
      "Iteration 1726, loss = 2030.42319192\n",
      "Iteration 1727, loss = 2030.29595719\n",
      "Iteration 1728, loss = 2030.19426425\n",
      "Iteration 1729, loss = 2030.07685942\n",
      "Iteration 1730, loss = 2029.96302201\n",
      "Iteration 1731, loss = 2029.86288416\n",
      "Iteration 1732, loss = 2029.73781041\n",
      "Iteration 1733, loss = 2029.63174949\n",
      "Iteration 1734, loss = 2029.51976660\n",
      "Iteration 1735, loss = 2029.40603154\n",
      "Iteration 1736, loss = 2029.30875708\n",
      "Iteration 1737, loss = 2029.17825562\n",
      "Iteration 1738, loss = 2029.07472600\n",
      "Iteration 1739, loss = 2028.95772337\n",
      "Iteration 1740, loss = 2028.84715950\n",
      "Iteration 1741, loss = 2028.73201432\n",
      "Iteration 1742, loss = 2028.62833722\n",
      "Iteration 1743, loss = 2028.51834796\n",
      "Iteration 1744, loss = 2028.40323112\n",
      "Iteration 1745, loss = 2028.28979437\n",
      "Iteration 1746, loss = 2028.17734791\n",
      "Iteration 1747, loss = 2028.06210905\n",
      "Iteration 1748, loss = 2027.94881735\n",
      "Iteration 1749, loss = 2027.85216034\n",
      "Iteration 1750, loss = 2027.72973122\n",
      "Iteration 1751, loss = 2027.61309904\n",
      "Iteration 1752, loss = 2027.50266937\n",
      "Iteration 1753, loss = 2027.41034250\n",
      "Iteration 1754, loss = 2027.28391856\n",
      "Iteration 1755, loss = 2027.16550447\n",
      "Iteration 1756, loss = 2027.05657530\n",
      "Iteration 1757, loss = 2026.94205077\n",
      "Iteration 1758, loss = 2026.83883066\n",
      "Iteration 1759, loss = 2026.72181398\n",
      "Iteration 1760, loss = 2026.62613436\n",
      "Iteration 1761, loss = 2026.49613962\n",
      "Iteration 1762, loss = 2026.41107211\n",
      "Iteration 1763, loss = 2026.28423007\n",
      "Iteration 1764, loss = 2026.16970616\n",
      "Iteration 1765, loss = 2026.05602352\n",
      "Iteration 1766, loss = 2025.94501006\n",
      "Iteration 1767, loss = 2025.82967400\n",
      "Iteration 1768, loss = 2025.72241556\n",
      "Iteration 1769, loss = 2025.61251961\n",
      "Iteration 1770, loss = 2025.49780532\n",
      "Iteration 1771, loss = 2025.38680640\n",
      "Iteration 1772, loss = 2025.28340814\n",
      "Iteration 1773, loss = 2025.16163513\n",
      "Iteration 1774, loss = 2025.04963293\n",
      "Iteration 1775, loss = 2024.94774499\n",
      "Iteration 1776, loss = 2024.82826246\n",
      "Iteration 1777, loss = 2024.71904635\n",
      "Iteration 1778, loss = 2024.61021256\n",
      "Iteration 1779, loss = 2024.50588507\n",
      "Iteration 1780, loss = 2024.38370846\n",
      "Iteration 1781, loss = 2024.28600693\n",
      "Iteration 1782, loss = 2024.16107036\n",
      "Iteration 1783, loss = 2024.04889351\n",
      "Iteration 1784, loss = 2023.94635372\n",
      "Iteration 1785, loss = 2023.83114123\n",
      "Iteration 1786, loss = 2023.72405302\n",
      "Iteration 1787, loss = 2023.60413254\n",
      "Iteration 1788, loss = 2023.50289929\n",
      "Iteration 1789, loss = 2023.39681068\n",
      "Iteration 1790, loss = 2023.26807979\n",
      "Iteration 1791, loss = 2023.17751891\n",
      "Iteration 1792, loss = 2023.05796053\n",
      "Iteration 1793, loss = 2022.93680290\n",
      "Iteration 1794, loss = 2022.83584720\n",
      "Iteration 1795, loss = 2022.71629313\n",
      "Iteration 1796, loss = 2022.61852217\n",
      "Iteration 1797, loss = 2022.50615872\n",
      "Iteration 1798, loss = 2022.38331158\n",
      "Iteration 1799, loss = 2022.27107029\n",
      "Iteration 1800, loss = 2022.16103391\n",
      "Iteration 1801, loss = 2022.05864251\n",
      "Iteration 1802, loss = 2021.94712539\n",
      "Iteration 1803, loss = 2021.83630750\n",
      "Iteration 1804, loss = 2021.72307139\n",
      "Iteration 1805, loss = 2021.61456157\n",
      "Iteration 1806, loss = 2021.50792285\n",
      "Iteration 1807, loss = 2021.39214569\n",
      "Iteration 1808, loss = 2021.28871117\n",
      "Iteration 1809, loss = 2021.18027679\n",
      "Iteration 1810, loss = 2021.06443733\n",
      "Iteration 1811, loss = 2020.95551139\n",
      "Iteration 1812, loss = 2020.83831133\n",
      "Iteration 1813, loss = 2020.73834308\n",
      "Iteration 1814, loss = 2020.60876889\n",
      "Iteration 1815, loss = 2020.50574263\n",
      "Iteration 1816, loss = 2020.39445701\n",
      "Iteration 1817, loss = 2020.28330490\n",
      "Iteration 1818, loss = 2020.17064815\n",
      "Iteration 1819, loss = 2020.06234921\n",
      "Iteration 1820, loss = 2019.95527925\n",
      "Iteration 1821, loss = 2019.83778132\n",
      "Iteration 1822, loss = 2019.74628646\n",
      "Iteration 1823, loss = 2019.61778797\n",
      "Iteration 1824, loss = 2019.52058735\n",
      "Iteration 1825, loss = 2019.39996143\n",
      "Iteration 1826, loss = 2019.28311810\n",
      "Iteration 1827, loss = 2019.17919812\n",
      "Iteration 1828, loss = 2019.06532936\n",
      "Iteration 1829, loss = 2018.95250413\n",
      "Iteration 1830, loss = 2018.84217776\n",
      "Iteration 1831, loss = 2018.73363036\n",
      "Iteration 1832, loss = 2018.63125533\n",
      "Iteration 1833, loss = 2018.51122903\n",
      "Iteration 1834, loss = 2018.40393579\n",
      "Iteration 1835, loss = 2018.29019973\n",
      "Iteration 1836, loss = 2018.18261837\n",
      "Iteration 1837, loss = 2018.07030001\n",
      "Iteration 1838, loss = 2017.96181822\n",
      "Iteration 1839, loss = 2017.86040013\n",
      "Iteration 1840, loss = 2017.74333849\n",
      "Iteration 1841, loss = 2017.63488491\n",
      "Iteration 1842, loss = 2017.55899536\n",
      "Iteration 1843, loss = 2017.40730461\n",
      "Iteration 1844, loss = 2017.29781055\n",
      "Iteration 1845, loss = 2017.19227618\n",
      "Iteration 1846, loss = 2017.07860695\n",
      "Iteration 1847, loss = 2016.97430244\n",
      "Iteration 1848, loss = 2016.85737647\n",
      "Iteration 1849, loss = 2016.74901218\n",
      "Iteration 1850, loss = 2016.63615041\n",
      "Iteration 1851, loss = 2016.53198274\n",
      "Iteration 1852, loss = 2016.44359771\n",
      "Iteration 1853, loss = 2016.31023448\n",
      "Iteration 1854, loss = 2016.20664119\n",
      "Iteration 1855, loss = 2016.09662129\n",
      "Iteration 1856, loss = 2015.97748843\n",
      "Iteration 1857, loss = 2015.87693566\n",
      "Iteration 1858, loss = 2015.76183262\n",
      "Iteration 1859, loss = 2015.64597785\n",
      "Iteration 1860, loss = 2015.54588354\n",
      "Iteration 1861, loss = 2015.44601579\n",
      "Iteration 1862, loss = 2015.31763944\n",
      "Iteration 1863, loss = 2015.21000563\n",
      "Iteration 1864, loss = 2015.09702095\n",
      "Iteration 1865, loss = 2014.98853922\n",
      "Iteration 1866, loss = 2014.88171904\n",
      "Iteration 1867, loss = 2014.77986526\n",
      "Iteration 1868, loss = 2014.66849286\n",
      "Iteration 1869, loss = 2014.55011785\n",
      "Iteration 1870, loss = 2014.44278530\n",
      "Iteration 1871, loss = 2014.32593331\n",
      "Iteration 1872, loss = 2014.24216455\n",
      "Iteration 1873, loss = 2014.10826832\n",
      "Iteration 1874, loss = 2014.00454052\n",
      "Iteration 1875, loss = 2013.89287611\n",
      "Iteration 1876, loss = 2013.78349433\n",
      "Iteration 1877, loss = 2013.66732544\n",
      "Iteration 1878, loss = 2013.57365866\n",
      "Iteration 1879, loss = 2013.45356217\n",
      "Iteration 1880, loss = 2013.34910187\n",
      "Iteration 1881, loss = 2013.23071433\n",
      "Iteration 1882, loss = 2013.12316893\n",
      "Iteration 1883, loss = 2013.01739289\n",
      "Iteration 1884, loss = 2012.90619402\n",
      "Iteration 1885, loss = 2012.79215950\n",
      "Iteration 1886, loss = 2012.68249924\n",
      "Iteration 1887, loss = 2012.57640433\n",
      "Iteration 1888, loss = 2012.45966563\n",
      "Iteration 1889, loss = 2012.36375188\n",
      "Iteration 1890, loss = 2012.24710117\n",
      "Iteration 1891, loss = 2012.14191840\n",
      "Iteration 1892, loss = 2012.02649240\n",
      "Iteration 1893, loss = 2011.91536374\n",
      "Iteration 1894, loss = 2011.81473663\n",
      "Iteration 1895, loss = 2011.69235612\n",
      "Iteration 1896, loss = 2011.59275700\n",
      "Iteration 1897, loss = 2011.50996150\n",
      "Iteration 1898, loss = 2011.38018980\n",
      "Iteration 1899, loss = 2011.26371050\n",
      "Iteration 1900, loss = 2011.16067540\n",
      "Iteration 1901, loss = 2011.04068287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1902, loss = 2010.93070080\n",
      "Iteration 1903, loss = 2010.82265803\n",
      "Iteration 1904, loss = 2010.71485949\n",
      "Iteration 1905, loss = 2010.60601144\n",
      "Iteration 1906, loss = 2010.49784561\n",
      "Iteration 1907, loss = 2010.38741118\n",
      "Iteration 1908, loss = 2010.29355851\n",
      "Iteration 1909, loss = 2010.17000857\n",
      "Iteration 1910, loss = 2010.05905528\n",
      "Iteration 1911, loss = 2009.94274151\n",
      "Iteration 1912, loss = 2009.83589814\n",
      "Iteration 1913, loss = 2009.73570264\n",
      "Iteration 1914, loss = 2009.62266751\n",
      "Iteration 1915, loss = 2009.52703762\n",
      "Iteration 1916, loss = 2009.40428010\n",
      "Iteration 1917, loss = 2009.29221912\n",
      "Iteration 1918, loss = 2009.18573474\n",
      "Iteration 1919, loss = 2009.08085195\n",
      "Iteration 1920, loss = 2008.96862657\n",
      "Iteration 1921, loss = 2008.86993073\n",
      "Iteration 1922, loss = 2008.74869975\n",
      "Iteration 1923, loss = 2008.64288812\n",
      "Iteration 1924, loss = 2008.52705812\n",
      "Iteration 1925, loss = 2008.41675338\n",
      "Iteration 1926, loss = 2008.31949419\n",
      "Iteration 1927, loss = 2008.20208939\n",
      "Iteration 1928, loss = 2008.10077592\n",
      "Iteration 1929, loss = 2007.98085075\n",
      "Iteration 1930, loss = 2007.87346408\n",
      "Iteration 1931, loss = 2007.76336628\n",
      "Iteration 1932, loss = 2007.65914337\n",
      "Iteration 1933, loss = 2007.54687454\n",
      "Iteration 1934, loss = 2007.44788732\n",
      "Iteration 1935, loss = 2007.33461575\n",
      "Iteration 1936, loss = 2007.22220503\n",
      "Iteration 1937, loss = 2007.11808408\n",
      "Iteration 1938, loss = 2007.01720763\n",
      "Iteration 1939, loss = 2006.89071277\n",
      "Iteration 1940, loss = 2006.78786383\n",
      "Iteration 1941, loss = 2006.67572319\n",
      "Iteration 1942, loss = 2006.56994510\n",
      "Iteration 1943, loss = 2006.45648992\n",
      "Iteration 1944, loss = 2006.34981106\n",
      "Iteration 1945, loss = 2006.24160321\n",
      "Iteration 1946, loss = 2006.12998716\n",
      "Iteration 1947, loss = 2006.02979594\n",
      "Iteration 1948, loss = 2005.91466580\n",
      "Iteration 1949, loss = 2005.79875004\n",
      "Iteration 1950, loss = 2005.69620537\n",
      "Iteration 1951, loss = 2005.58715814\n",
      "Iteration 1952, loss = 2005.49061922\n",
      "Iteration 1953, loss = 2005.37679592\n",
      "Iteration 1954, loss = 2005.26755070\n",
      "Iteration 1955, loss = 2005.15447448\n",
      "Iteration 1956, loss = 2005.05007987\n",
      "Iteration 1957, loss = 2004.93794311\n",
      "Iteration 1958, loss = 2004.83072062\n",
      "Iteration 1959, loss = 2004.73409810\n",
      "Iteration 1960, loss = 2004.61304143\n",
      "Iteration 1961, loss = 2004.51007688\n",
      "Iteration 1962, loss = 2004.39288002\n",
      "Iteration 1963, loss = 2004.28441730\n",
      "Iteration 1964, loss = 2004.17694001\n",
      "Iteration 1965, loss = 2004.08069789\n",
      "Iteration 1966, loss = 2003.95809496\n",
      "Iteration 1967, loss = 2003.85507562\n",
      "Iteration 1968, loss = 2003.74274918\n",
      "Iteration 1969, loss = 2003.66090277\n",
      "Iteration 1970, loss = 2003.52006406\n",
      "Iteration 1971, loss = 2003.41902673\n",
      "Iteration 1972, loss = 2003.31006997\n",
      "Iteration 1973, loss = 2003.22354228\n",
      "Iteration 1974, loss = 2003.10429259\n",
      "Iteration 1975, loss = 2002.99179697\n",
      "Iteration 1976, loss = 2002.87507177\n",
      "Iteration 1977, loss = 2002.76567439\n",
      "Iteration 1978, loss = 2002.65689241\n",
      "Iteration 1979, loss = 2002.54686702\n",
      "Iteration 1980, loss = 2002.45093313\n",
      "Iteration 1981, loss = 2002.33355844\n",
      "Iteration 1982, loss = 2002.22203527\n",
      "Iteration 1983, loss = 2002.11523513\n",
      "Iteration 1984, loss = 2002.02424107\n",
      "Iteration 1985, loss = 2001.90445456\n",
      "Iteration 1986, loss = 2001.79080118\n",
      "Iteration 1987, loss = 2001.70698804\n",
      "Iteration 1988, loss = 2001.56994657\n",
      "Iteration 1989, loss = 2001.46955803\n",
      "Iteration 1990, loss = 2001.36351159\n",
      "Iteration 1991, loss = 2001.25264653\n",
      "Iteration 1992, loss = 2001.16442786\n",
      "Iteration 1993, loss = 2001.03948148\n",
      "Iteration 1994, loss = 2000.92882982\n",
      "Iteration 1995, loss = 2000.82726345\n",
      "Iteration 1996, loss = 2000.72463270\n",
      "Iteration 1997, loss = 2000.60192121\n",
      "Iteration 1998, loss = 2000.50129345\n",
      "Iteration 1999, loss = 2000.40025295\n",
      "Iteration 2000, loss = 2000.29892464\n",
      "Iteration 2001, loss = 2000.18163442\n",
      "Iteration 2002, loss = 2000.07337648\n",
      "Iteration 2003, loss = 1999.95533519\n",
      "Iteration 2004, loss = 1999.84721633\n",
      "Iteration 2005, loss = 1999.74619198\n",
      "Iteration 2006, loss = 1999.63406946\n",
      "Iteration 2007, loss = 1999.52515292\n",
      "Iteration 2008, loss = 1999.41623325\n",
      "Iteration 2009, loss = 1999.31026908\n",
      "Iteration 2010, loss = 1999.20365097\n",
      "Iteration 2011, loss = 1999.09883266\n",
      "Iteration 2012, loss = 1998.98730055\n",
      "Iteration 2013, loss = 1998.87743280\n",
      "Iteration 2014, loss = 1998.79059572\n",
      "Iteration 2015, loss = 1998.68095284\n",
      "Iteration 2016, loss = 1998.55747766\n",
      "Iteration 2017, loss = 1998.46315086\n",
      "Iteration 2018, loss = 1998.34416040\n",
      "Iteration 2019, loss = 1998.23272746\n",
      "Iteration 2020, loss = 1998.12865300\n",
      "Iteration 2021, loss = 1998.01554516\n",
      "Iteration 2022, loss = 1997.91275040\n",
      "Iteration 2023, loss = 1997.79860420\n",
      "Iteration 2024, loss = 1997.69793967\n",
      "Iteration 2025, loss = 1997.59547846\n",
      "Iteration 2026, loss = 1997.48834626\n",
      "Iteration 2027, loss = 1997.36697646\n",
      "Iteration 2028, loss = 1997.26926449\n",
      "Iteration 2029, loss = 1997.16079914\n",
      "Iteration 2030, loss = 1997.04655784\n",
      "Iteration 2031, loss = 1996.93791900\n",
      "Iteration 2032, loss = 1996.83108001\n",
      "Iteration 2033, loss = 1996.72697974\n",
      "Iteration 2034, loss = 1996.61994206\n",
      "Iteration 2035, loss = 1996.50735670\n",
      "Iteration 2036, loss = 1996.40761352\n",
      "Iteration 2037, loss = 1996.31045086\n",
      "Iteration 2038, loss = 1996.19659741\n",
      "Iteration 2039, loss = 1996.08232803\n",
      "Iteration 2040, loss = 1996.00496842\n",
      "Iteration 2041, loss = 1995.86168412\n",
      "Iteration 2042, loss = 1995.76394938\n",
      "Iteration 2043, loss = 1995.65484399\n",
      "Iteration 2044, loss = 1995.55175285\n",
      "Iteration 2045, loss = 1995.44539098\n",
      "Iteration 2046, loss = 1995.33240844\n",
      "Iteration 2047, loss = 1995.22449900\n",
      "Iteration 2048, loss = 1995.12025570\n",
      "Iteration 2049, loss = 1995.01000562\n",
      "Iteration 2050, loss = 1994.90615853\n",
      "Iteration 2051, loss = 1994.79213194\n",
      "Iteration 2052, loss = 1994.68243929\n",
      "Iteration 2053, loss = 1994.58836893\n",
      "Iteration 2054, loss = 1994.48351186\n",
      "Iteration 2055, loss = 1994.36140237\n",
      "Iteration 2056, loss = 1994.25116859\n",
      "Iteration 2057, loss = 1994.14879611\n",
      "Iteration 2058, loss = 1994.04583251\n",
      "Iteration 2059, loss = 1993.93323686\n",
      "Iteration 2060, loss = 1993.82787029\n",
      "Iteration 2061, loss = 1993.73269578\n",
      "Iteration 2062, loss = 1993.61264539\n",
      "Iteration 2063, loss = 1993.50516172\n",
      "Iteration 2064, loss = 1993.41521872\n",
      "Iteration 2065, loss = 1993.29667825\n",
      "Iteration 2066, loss = 1993.18712516\n",
      "Iteration 2067, loss = 1993.07584037\n",
      "Iteration 2068, loss = 1992.98345276\n",
      "Iteration 2069, loss = 1992.86481665\n",
      "Iteration 2070, loss = 1992.75864763\n",
      "Iteration 2071, loss = 1992.65029922\n",
      "Iteration 2072, loss = 1992.54667146\n",
      "Iteration 2073, loss = 1992.43791652\n",
      "Iteration 2074, loss = 1992.33309695\n",
      "Iteration 2075, loss = 1992.22261257\n",
      "Iteration 2076, loss = 1992.11172984\n",
      "Iteration 2077, loss = 1992.00487411\n",
      "Iteration 2078, loss = 1991.90322579\n",
      "Iteration 2079, loss = 1991.79909366\n",
      "Iteration 2080, loss = 1991.69822888\n",
      "Iteration 2081, loss = 1991.57975053\n",
      "Iteration 2082, loss = 1991.47339861\n",
      "Iteration 2083, loss = 1991.39245260\n",
      "Iteration 2084, loss = 1991.25985014\n",
      "Iteration 2085, loss = 1991.16251412\n",
      "Iteration 2086, loss = 1991.04779159\n",
      "Iteration 2087, loss = 1990.94382183\n",
      "Iteration 2088, loss = 1990.83253884\n",
      "Iteration 2089, loss = 1990.74211261\n",
      "Iteration 2090, loss = 1990.62216720\n",
      "Iteration 2091, loss = 1990.52254642\n",
      "Iteration 2092, loss = 1990.40557185\n",
      "Iteration 2093, loss = 1990.29839951\n",
      "Iteration 2094, loss = 1990.19108655\n",
      "Iteration 2095, loss = 1990.09034538\n",
      "Iteration 2096, loss = 1989.98300862\n",
      "Iteration 2097, loss = 1989.87309018\n",
      "Iteration 2098, loss = 1989.76797913\n",
      "Iteration 2099, loss = 1989.66304331\n",
      "Iteration 2100, loss = 1989.56106057\n",
      "Iteration 2101, loss = 1989.45120526\n",
      "Iteration 2102, loss = 1989.33871670\n",
      "Iteration 2103, loss = 1989.22881437\n",
      "Iteration 2104, loss = 1989.12361947\n",
      "Iteration 2105, loss = 1989.01801622\n",
      "Iteration 2106, loss = 1988.91834383\n",
      "Iteration 2107, loss = 1988.80450219\n",
      "Iteration 2108, loss = 1988.69790986\n",
      "Iteration 2109, loss = 1988.59229777\n",
      "Iteration 2110, loss = 1988.48570406\n",
      "Iteration 2111, loss = 1988.38360931\n",
      "Iteration 2112, loss = 1988.27776521\n",
      "Iteration 2113, loss = 1988.18457818\n",
      "Iteration 2114, loss = 1988.06279024\n",
      "Iteration 2115, loss = 1987.95657519\n",
      "Iteration 2116, loss = 1987.84440468\n",
      "Iteration 2117, loss = 1987.77636456\n",
      "Iteration 2118, loss = 1987.63640749\n",
      "Iteration 2119, loss = 1987.53000934\n",
      "Iteration 2120, loss = 1987.42931997\n",
      "Iteration 2121, loss = 1987.31850093\n",
      "Iteration 2122, loss = 1987.21669174\n",
      "Iteration 2123, loss = 1987.12453628\n",
      "Iteration 2124, loss = 1987.01137743\n",
      "Iteration 2125, loss = 1986.89615517\n",
      "Iteration 2126, loss = 1986.78600613\n",
      "Iteration 2127, loss = 1986.69936756\n",
      "Iteration 2128, loss = 1986.57814802\n",
      "Iteration 2129, loss = 1986.46220040\n",
      "Iteration 2130, loss = 1986.37515837\n",
      "Iteration 2131, loss = 1986.24891081\n",
      "Iteration 2132, loss = 1986.14861632\n",
      "Iteration 2133, loss = 1986.03893434\n",
      "Iteration 2134, loss = 1985.93106838\n",
      "Iteration 2135, loss = 1985.82632100\n",
      "Iteration 2136, loss = 1985.73303072\n",
      "Iteration 2137, loss = 1985.62319885\n",
      "Iteration 2138, loss = 1985.51215295\n",
      "Iteration 2139, loss = 1985.40511495\n",
      "Iteration 2140, loss = 1985.31668032\n",
      "Iteration 2141, loss = 1985.19052710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2142, loss = 1985.09545626\n",
      "Iteration 2143, loss = 1984.97784189\n",
      "Iteration 2144, loss = 1984.90028867\n",
      "Iteration 2145, loss = 1984.76324002\n",
      "Iteration 2146, loss = 1984.65878401\n",
      "Iteration 2147, loss = 1984.57521714\n",
      "Iteration 2148, loss = 1984.45309769\n",
      "Iteration 2149, loss = 1984.36094030\n",
      "Iteration 2150, loss = 1984.23950774\n",
      "Iteration 2151, loss = 1984.13187309\n",
      "Iteration 2152, loss = 1984.02867513\n",
      "Iteration 2153, loss = 1983.92467114\n",
      "Iteration 2154, loss = 1983.82336501\n",
      "Iteration 2155, loss = 1983.70863335\n",
      "Iteration 2156, loss = 1983.60335765\n",
      "Iteration 2157, loss = 1983.49831526\n",
      "Iteration 2158, loss = 1983.40943959\n",
      "Iteration 2159, loss = 1983.28857564\n",
      "Iteration 2160, loss = 1983.18323297\n",
      "Iteration 2161, loss = 1983.06882356\n",
      "Iteration 2162, loss = 1982.97232737\n",
      "Iteration 2163, loss = 1982.86727920\n",
      "Iteration 2164, loss = 1982.76716807\n",
      "Iteration 2165, loss = 1982.65379614\n",
      "Iteration 2166, loss = 1982.56117744\n",
      "Iteration 2167, loss = 1982.44156786\n",
      "Iteration 2168, loss = 1982.32686426\n",
      "Iteration 2169, loss = 1982.23061216\n",
      "Iteration 2170, loss = 1982.12700638\n",
      "Iteration 2171, loss = 1982.01041282\n",
      "Iteration 2172, loss = 1981.92745021\n",
      "Iteration 2173, loss = 1981.81111321\n",
      "Iteration 2174, loss = 1981.70456890\n",
      "Iteration 2175, loss = 1981.58842593\n",
      "Iteration 2176, loss = 1981.48575096\n",
      "Iteration 2177, loss = 1981.38539468\n",
      "Iteration 2178, loss = 1981.27011466\n",
      "Iteration 2179, loss = 1981.17009038\n",
      "Iteration 2180, loss = 1981.07372126\n",
      "Iteration 2181, loss = 1980.95606190\n",
      "Iteration 2182, loss = 1980.86642423\n",
      "Iteration 2183, loss = 1980.74253606\n",
      "Iteration 2184, loss = 1980.66801022\n",
      "Iteration 2185, loss = 1980.53003264\n",
      "Iteration 2186, loss = 1980.43208983\n",
      "Iteration 2187, loss = 1980.33652915\n",
      "Iteration 2188, loss = 1980.21826703\n",
      "Iteration 2189, loss = 1980.11281382\n",
      "Iteration 2190, loss = 1980.00879383\n",
      "Iteration 2191, loss = 1979.91195677\n",
      "Iteration 2192, loss = 1979.82501379\n",
      "Iteration 2193, loss = 1979.69586851\n",
      "Iteration 2194, loss = 1979.59412750\n",
      "Iteration 2195, loss = 1979.47709621\n",
      "Iteration 2196, loss = 1979.37696728\n",
      "Iteration 2197, loss = 1979.27401755\n",
      "Iteration 2198, loss = 1979.17008587\n",
      "Iteration 2199, loss = 1979.06042776\n",
      "Iteration 2200, loss = 1978.95375515\n",
      "Iteration 2201, loss = 1978.84449778\n",
      "Iteration 2202, loss = 1978.74760375\n",
      "Iteration 2203, loss = 1978.64955507\n",
      "Iteration 2204, loss = 1978.54813718\n",
      "Iteration 2205, loss = 1978.42878253\n",
      "Iteration 2206, loss = 1978.32576400\n",
      "Iteration 2207, loss = 1978.22024603\n",
      "Iteration 2208, loss = 1978.10646192\n",
      "Iteration 2209, loss = 1978.00885178\n",
      "Iteration 2210, loss = 1977.90850133\n",
      "Iteration 2211, loss = 1977.79508695\n",
      "Iteration 2212, loss = 1977.68919920\n",
      "Iteration 2213, loss = 1977.58961086\n",
      "Iteration 2214, loss = 1977.48443597\n",
      "Iteration 2215, loss = 1977.38124440\n",
      "Iteration 2216, loss = 1977.26740690\n",
      "Iteration 2217, loss = 1977.16683125\n",
      "Iteration 2218, loss = 1977.05628040\n",
      "Iteration 2219, loss = 1976.96366288\n",
      "Iteration 2220, loss = 1976.84259431\n",
      "Iteration 2221, loss = 1976.74322263\n",
      "Iteration 2222, loss = 1976.67200763\n",
      "Iteration 2223, loss = 1976.53593827\n",
      "Iteration 2224, loss = 1976.43615606\n",
      "Iteration 2225, loss = 1976.32714105\n",
      "Iteration 2226, loss = 1976.22196778\n",
      "Iteration 2227, loss = 1976.11447900\n",
      "Iteration 2228, loss = 1976.01291253\n",
      "Iteration 2229, loss = 1975.90768218\n",
      "Iteration 2230, loss = 1975.81495195\n",
      "Iteration 2231, loss = 1975.70159064\n",
      "Iteration 2232, loss = 1975.60582789\n",
      "Iteration 2233, loss = 1975.51268540\n",
      "Iteration 2234, loss = 1975.37766201\n",
      "Iteration 2235, loss = 1975.28332121\n",
      "Iteration 2236, loss = 1975.16465438\n",
      "Iteration 2237, loss = 1975.06568167\n",
      "Iteration 2238, loss = 1974.96008428\n",
      "Iteration 2239, loss = 1974.85286018\n",
      "Iteration 2240, loss = 1974.76389075\n",
      "Iteration 2241, loss = 1974.63966017\n",
      "Iteration 2242, loss = 1974.55131293\n",
      "Iteration 2243, loss = 1974.42976622\n",
      "Iteration 2244, loss = 1974.33253712\n",
      "Iteration 2245, loss = 1974.22612375\n",
      "Iteration 2246, loss = 1974.12980554\n",
      "Iteration 2247, loss = 1974.03115634\n",
      "Iteration 2248, loss = 1973.91536564\n",
      "Iteration 2249, loss = 1973.80880652\n",
      "Iteration 2250, loss = 1973.70613344\n",
      "Iteration 2251, loss = 1973.59790117\n",
      "Iteration 2252, loss = 1973.49712390\n",
      "Iteration 2253, loss = 1973.38400302\n",
      "Iteration 2254, loss = 1973.28663751\n",
      "Iteration 2255, loss = 1973.17584037\n",
      "Iteration 2256, loss = 1973.07947526\n",
      "Iteration 2257, loss = 1972.96553715\n",
      "Iteration 2258, loss = 1972.85750169\n",
      "Iteration 2259, loss = 1972.75821890\n",
      "Iteration 2260, loss = 1972.65015457\n",
      "Iteration 2261, loss = 1972.56778136\n",
      "Iteration 2262, loss = 1972.44125419\n",
      "Iteration 2263, loss = 1972.34489274\n",
      "Iteration 2264, loss = 1972.24080699\n",
      "Iteration 2265, loss = 1972.13736491\n",
      "Iteration 2266, loss = 1972.03093759\n",
      "Iteration 2267, loss = 1971.91918700\n",
      "Iteration 2268, loss = 1971.82095628\n",
      "Iteration 2269, loss = 1971.71463059\n",
      "Iteration 2270, loss = 1971.60711753\n",
      "Iteration 2271, loss = 1971.50310179\n",
      "Iteration 2272, loss = 1971.41004614\n",
      "Iteration 2273, loss = 1971.29183384\n",
      "Iteration 2274, loss = 1971.19258650\n",
      "Iteration 2275, loss = 1971.09079551\n",
      "Iteration 2276, loss = 1970.98776817\n",
      "Iteration 2277, loss = 1970.90051814\n",
      "Iteration 2278, loss = 1970.77395346\n",
      "Iteration 2279, loss = 1970.70172434\n",
      "Iteration 2280, loss = 1970.57517676\n",
      "Iteration 2281, loss = 1970.45504130\n",
      "Iteration 2282, loss = 1970.37019484\n",
      "Iteration 2283, loss = 1970.24512376\n",
      "Iteration 2284, loss = 1970.14209807\n",
      "Iteration 2285, loss = 1970.06287704\n",
      "Iteration 2286, loss = 1969.93599306\n",
      "Iteration 2287, loss = 1969.84807249\n",
      "Iteration 2288, loss = 1969.73594166\n",
      "Iteration 2289, loss = 1969.62713266\n",
      "Iteration 2290, loss = 1969.54256372\n",
      "Iteration 2291, loss = 1969.44796686\n",
      "Iteration 2292, loss = 1969.30548474\n",
      "Iteration 2293, loss = 1969.21076575\n",
      "Iteration 2294, loss = 1969.11705177\n",
      "Iteration 2295, loss = 1969.00919540\n",
      "Iteration 2296, loss = 1968.90644472\n",
      "Iteration 2297, loss = 1968.79825515\n",
      "Iteration 2298, loss = 1968.68349936\n",
      "Iteration 2299, loss = 1968.60648058\n",
      "Iteration 2300, loss = 1968.50269673\n",
      "Iteration 2301, loss = 1968.38520085\n",
      "Iteration 2302, loss = 1968.28933036\n",
      "Iteration 2303, loss = 1968.18772539\n",
      "Iteration 2304, loss = 1968.06786539\n",
      "Iteration 2305, loss = 1967.97061707\n",
      "Iteration 2306, loss = 1967.86133408\n",
      "Iteration 2307, loss = 1967.74943973\n",
      "Iteration 2308, loss = 1967.65368719\n",
      "Iteration 2309, loss = 1967.54230825\n",
      "Iteration 2310, loss = 1967.43568569\n",
      "Iteration 2311, loss = 1967.33697211\n",
      "Iteration 2312, loss = 1967.22607108\n",
      "Iteration 2313, loss = 1967.13180927\n",
      "Iteration 2314, loss = 1967.02670896\n",
      "Iteration 2315, loss = 1966.91927579\n",
      "Iteration 2316, loss = 1966.82509657\n",
      "Iteration 2317, loss = 1966.70852324\n",
      "Iteration 2318, loss = 1966.61399896\n",
      "Iteration 2319, loss = 1966.50161511\n",
      "Iteration 2320, loss = 1966.39027064\n",
      "Iteration 2321, loss = 1966.30920960\n",
      "Iteration 2322, loss = 1966.18622105\n",
      "Iteration 2323, loss = 1966.08785174\n",
      "Iteration 2324, loss = 1966.00909083\n",
      "Iteration 2325, loss = 1965.87896601\n",
      "Iteration 2326, loss = 1965.77715133\n",
      "Iteration 2327, loss = 1965.66815799\n",
      "Iteration 2328, loss = 1965.57102473\n",
      "Iteration 2329, loss = 1965.46372974\n",
      "Iteration 2330, loss = 1965.35833015\n",
      "Iteration 2331, loss = 1965.25626595\n",
      "Iteration 2332, loss = 1965.15561605\n",
      "Iteration 2333, loss = 1965.05602257\n",
      "Iteration 2334, loss = 1964.94624593\n",
      "Iteration 2335, loss = 1964.84663424\n",
      "Iteration 2336, loss = 1964.75497326\n",
      "Iteration 2337, loss = 1964.64514462\n",
      "Iteration 2338, loss = 1964.53670510\n",
      "Iteration 2339, loss = 1964.44527523\n",
      "Iteration 2340, loss = 1964.33516682\n",
      "Iteration 2341, loss = 1964.22223979\n",
      "Iteration 2342, loss = 1964.11774423\n",
      "Iteration 2343, loss = 1964.02740522\n",
      "Iteration 2344, loss = 1963.92043079\n",
      "Iteration 2345, loss = 1963.79982835\n",
      "Iteration 2346, loss = 1963.70506667\n",
      "Iteration 2347, loss = 1963.59299173\n",
      "Iteration 2348, loss = 1963.52732122\n",
      "Iteration 2349, loss = 1963.40516714\n",
      "Iteration 2350, loss = 1963.29273322\n",
      "Iteration 2351, loss = 1963.19688590\n",
      "Iteration 2352, loss = 1963.08323360\n",
      "Iteration 2353, loss = 1963.01270317\n",
      "Iteration 2354, loss = 1962.87942190\n",
      "Iteration 2355, loss = 1962.78820046\n",
      "Iteration 2356, loss = 1962.71433192\n",
      "Iteration 2357, loss = 1962.56185596\n",
      "Iteration 2358, loss = 1962.46415065\n",
      "Iteration 2359, loss = 1962.37636095\n",
      "Iteration 2360, loss = 1962.25374856\n",
      "Iteration 2361, loss = 1962.15272119\n",
      "Iteration 2362, loss = 1962.04847939\n",
      "Iteration 2363, loss = 1961.93935748\n",
      "Iteration 2364, loss = 1961.84533700\n",
      "Iteration 2365, loss = 1961.73330659\n",
      "Iteration 2366, loss = 1961.62985784\n",
      "Iteration 2367, loss = 1961.53784701\n",
      "Iteration 2368, loss = 1961.43859829\n",
      "Iteration 2369, loss = 1961.33976767\n",
      "Iteration 2370, loss = 1961.22955379\n",
      "Iteration 2371, loss = 1961.12680001\n",
      "Iteration 2372, loss = 1961.01373975\n",
      "Iteration 2373, loss = 1960.91606467\n",
      "Iteration 2374, loss = 1960.80651596\n",
      "Iteration 2375, loss = 1960.72431595\n",
      "Iteration 2376, loss = 1960.60334903\n",
      "Iteration 2377, loss = 1960.49736954\n",
      "Iteration 2378, loss = 1960.40012184\n",
      "Iteration 2379, loss = 1960.30995417\n",
      "Iteration 2380, loss = 1960.18993932\n",
      "Iteration 2381, loss = 1960.09991012\n",
      "Iteration 2382, loss = 1959.99144779\n",
      "Iteration 2383, loss = 1959.89153642\n",
      "Iteration 2384, loss = 1959.77811325\n",
      "Iteration 2385, loss = 1959.67533452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2386, loss = 1959.58908872\n",
      "Iteration 2387, loss = 1959.47951744\n",
      "Iteration 2388, loss = 1959.37079099\n",
      "Iteration 2389, loss = 1959.25951331\n",
      "Iteration 2390, loss = 1959.16258834\n",
      "Iteration 2391, loss = 1959.06251363\n",
      "Iteration 2392, loss = 1958.96042010\n",
      "Iteration 2393, loss = 1958.84508534\n",
      "Iteration 2394, loss = 1958.76583101\n",
      "Iteration 2395, loss = 1958.64126628\n",
      "Iteration 2396, loss = 1958.53833464\n",
      "Iteration 2397, loss = 1958.45314311\n",
      "Iteration 2398, loss = 1958.34396999\n",
      "Iteration 2399, loss = 1958.23606023\n",
      "Iteration 2400, loss = 1958.12841083\n",
      "Iteration 2401, loss = 1958.02597303\n",
      "Iteration 2402, loss = 1957.92625467\n",
      "Iteration 2403, loss = 1957.81617959\n",
      "Iteration 2404, loss = 1957.73165778\n",
      "Iteration 2405, loss = 1957.62203125\n",
      "Iteration 2406, loss = 1957.51694437\n",
      "Iteration 2407, loss = 1957.41502321\n",
      "Iteration 2408, loss = 1957.30406824\n",
      "Iteration 2409, loss = 1957.20359536\n",
      "Iteration 2410, loss = 1957.12295729\n",
      "Iteration 2411, loss = 1957.00546147\n",
      "Iteration 2412, loss = 1956.89545114\n",
      "Iteration 2413, loss = 1956.78768454\n",
      "Iteration 2414, loss = 1956.68895835\n",
      "Iteration 2415, loss = 1956.60562622\n",
      "Iteration 2416, loss = 1956.50119948\n",
      "Iteration 2417, loss = 1956.39367470\n",
      "Iteration 2418, loss = 1956.28380946\n",
      "Iteration 2419, loss = 1956.18181246\n",
      "Iteration 2420, loss = 1956.07596738\n",
      "Iteration 2421, loss = 1955.98700094\n",
      "Iteration 2422, loss = 1955.87399124\n",
      "Iteration 2423, loss = 1955.78744663\n",
      "Iteration 2424, loss = 1955.67783721\n",
      "Iteration 2425, loss = 1955.55818558\n",
      "Iteration 2426, loss = 1955.45088700\n",
      "Iteration 2427, loss = 1955.35113158\n",
      "Iteration 2428, loss = 1955.25771426\n",
      "Iteration 2429, loss = 1955.16709951\n",
      "Iteration 2430, loss = 1955.03913650\n",
      "Iteration 2431, loss = 1954.94240691\n",
      "Iteration 2432, loss = 1954.83791215\n",
      "Iteration 2433, loss = 1954.73071646\n",
      "Iteration 2434, loss = 1954.64705953\n",
      "Iteration 2435, loss = 1954.53439702\n",
      "Iteration 2436, loss = 1954.44475244\n",
      "Iteration 2437, loss = 1954.35585945\n",
      "Iteration 2438, loss = 1954.21880398\n",
      "Iteration 2439, loss = 1954.14490292\n",
      "Iteration 2440, loss = 1954.02361514\n",
      "Iteration 2441, loss = 1953.93141915\n",
      "Iteration 2442, loss = 1953.83439807\n",
      "Iteration 2443, loss = 1953.72252266\n",
      "Iteration 2444, loss = 1953.61045684\n",
      "Iteration 2445, loss = 1953.50955565\n",
      "Iteration 2446, loss = 1953.39943172\n",
      "Iteration 2447, loss = 1953.29762034\n",
      "Iteration 2448, loss = 1953.20327705\n",
      "Iteration 2449, loss = 1953.10275631\n",
      "Iteration 2450, loss = 1952.99459457\n",
      "Iteration 2451, loss = 1952.88291782\n",
      "Iteration 2452, loss = 1952.79413582\n",
      "Iteration 2453, loss = 1952.68807454\n",
      "Iteration 2454, loss = 1952.59102556\n",
      "Iteration 2455, loss = 1952.48975572\n",
      "Iteration 2456, loss = 1952.39653381\n",
      "Iteration 2457, loss = 1952.28627870\n",
      "Iteration 2458, loss = 1952.16776116\n",
      "Iteration 2459, loss = 1952.06550923\n",
      "Iteration 2460, loss = 1951.97658169\n",
      "Iteration 2461, loss = 1951.86811152\n",
      "Iteration 2462, loss = 1951.77656127\n",
      "Iteration 2463, loss = 1951.65685630\n",
      "Iteration 2464, loss = 1951.55867848\n",
      "Iteration 2465, loss = 1951.48073667\n",
      "Iteration 2466, loss = 1951.36404490\n",
      "Iteration 2467, loss = 1951.27413757\n",
      "Iteration 2468, loss = 1951.14495385\n",
      "Iteration 2469, loss = 1951.05780103\n",
      "Iteration 2470, loss = 1950.95176915\n",
      "Iteration 2471, loss = 1950.84698115\n",
      "Iteration 2472, loss = 1950.75519696\n",
      "Iteration 2473, loss = 1950.63554534\n",
      "Iteration 2474, loss = 1950.52859899\n",
      "Iteration 2475, loss = 1950.42981475\n",
      "Iteration 2476, loss = 1950.33102289\n",
      "Iteration 2477, loss = 1950.23421172\n",
      "Iteration 2478, loss = 1950.12679384\n",
      "Iteration 2479, loss = 1950.03197733\n",
      "Iteration 2480, loss = 1949.94278536\n",
      "Iteration 2481, loss = 1949.81785951\n",
      "Iteration 2482, loss = 1949.72506609\n",
      "Iteration 2483, loss = 1949.62058161\n",
      "Iteration 2484, loss = 1949.50935119\n",
      "Iteration 2485, loss = 1949.40548550\n",
      "Iteration 2486, loss = 1949.30991976\n",
      "Iteration 2487, loss = 1949.21739886\n",
      "Iteration 2488, loss = 1949.09469039\n",
      "Iteration 2489, loss = 1949.00709658\n",
      "Iteration 2490, loss = 1948.89655047\n",
      "Iteration 2491, loss = 1948.81194080\n",
      "Iteration 2492, loss = 1948.71166586\n",
      "Iteration 2493, loss = 1948.58736885\n",
      "Iteration 2494, loss = 1948.49432809\n",
      "Iteration 2495, loss = 1948.38348636\n",
      "Iteration 2496, loss = 1948.28616461\n",
      "Iteration 2497, loss = 1948.18558574\n",
      "Iteration 2498, loss = 1948.09893975\n",
      "Iteration 2499, loss = 1947.98140001\n",
      "Iteration 2500, loss = 1947.87895839\n",
      "Iteration 2501, loss = 1947.80118569\n",
      "Iteration 2502, loss = 1947.68348712\n",
      "Iteration 2503, loss = 1947.58384794\n",
      "Iteration 2504, loss = 1947.46920471\n",
      "Iteration 2505, loss = 1947.36667621\n",
      "Iteration 2506, loss = 1947.27801655\n",
      "Iteration 2507, loss = 1947.16244986\n",
      "Iteration 2508, loss = 1947.07891812\n",
      "Iteration 2509, loss = 1947.00272816\n",
      "Iteration 2510, loss = 1946.85779214\n",
      "Iteration 2511, loss = 1946.75157464\n",
      "Iteration 2512, loss = 1946.65725919\n",
      "Iteration 2513, loss = 1946.56964184\n",
      "Iteration 2514, loss = 1946.44158917\n",
      "Iteration 2515, loss = 1946.34931297\n",
      "Iteration 2516, loss = 1946.24304065\n",
      "Iteration 2517, loss = 1946.14909270\n",
      "Iteration 2518, loss = 1946.05317801\n",
      "Iteration 2519, loss = 1945.94139497\n",
      "Iteration 2520, loss = 1945.84923153\n",
      "Iteration 2521, loss = 1945.74005180\n",
      "Iteration 2522, loss = 1945.63046048\n",
      "Iteration 2523, loss = 1945.52281315\n",
      "Iteration 2524, loss = 1945.42897207\n",
      "Iteration 2525, loss = 1945.33503781\n",
      "Iteration 2526, loss = 1945.22467995\n",
      "Iteration 2527, loss = 1945.11701624\n",
      "Iteration 2528, loss = 1945.03340818\n",
      "Iteration 2529, loss = 1944.91789141\n",
      "Iteration 2530, loss = 1944.81776929\n",
      "Iteration 2531, loss = 1944.72917606\n",
      "Iteration 2532, loss = 1944.61310146\n",
      "Iteration 2533, loss = 1944.52675004\n",
      "Iteration 2534, loss = 1944.45157482\n",
      "Iteration 2535, loss = 1944.31244279\n",
      "Iteration 2536, loss = 1944.20761523\n",
      "Iteration 2537, loss = 1944.10683554\n",
      "Iteration 2538, loss = 1944.00341223\n",
      "Iteration 2539, loss = 1943.89741440\n",
      "Iteration 2540, loss = 1943.79079825\n",
      "Iteration 2541, loss = 1943.70123734\n",
      "Iteration 2542, loss = 1943.59508227\n",
      "Iteration 2543, loss = 1943.49004978\n",
      "Iteration 2544, loss = 1943.39130734\n",
      "Iteration 2545, loss = 1943.30813577\n",
      "Iteration 2546, loss = 1943.17818258\n",
      "Iteration 2547, loss = 1943.09257659\n",
      "Iteration 2548, loss = 1942.98972179\n",
      "Iteration 2549, loss = 1942.89057543\n",
      "Iteration 2550, loss = 1942.86273111\n",
      "Iteration 2551, loss = 1942.67216469\n",
      "Iteration 2552, loss = 1942.58170730\n",
      "Iteration 2553, loss = 1942.48160878\n",
      "Iteration 2554, loss = 1942.40013192\n",
      "Iteration 2555, loss = 1942.27217806\n",
      "Iteration 2556, loss = 1942.17630385\n",
      "Iteration 2557, loss = 1942.07172156\n",
      "Iteration 2558, loss = 1941.98378502\n",
      "Iteration 2559, loss = 1941.86393252\n",
      "Iteration 2560, loss = 1941.75886042\n",
      "Iteration 2561, loss = 1941.68055898\n",
      "Iteration 2562, loss = 1941.57183021\n",
      "Iteration 2563, loss = 1941.45949014\n",
      "Iteration 2564, loss = 1941.35381281\n",
      "Iteration 2565, loss = 1941.25874149\n",
      "Iteration 2566, loss = 1941.15394108\n",
      "Iteration 2567, loss = 1941.04609200\n",
      "Iteration 2568, loss = 1940.94462164\n",
      "Iteration 2569, loss = 1940.86618465\n",
      "Iteration 2570, loss = 1940.75305381\n",
      "Iteration 2571, loss = 1940.67481212\n",
      "Iteration 2572, loss = 1940.53995262\n",
      "Iteration 2573, loss = 1940.43404650\n",
      "Iteration 2574, loss = 1940.40470682\n",
      "Iteration 2575, loss = 1940.24047236\n",
      "Iteration 2576, loss = 1940.13955169\n",
      "Iteration 2577, loss = 1940.02456010\n",
      "Iteration 2578, loss = 1939.94035125\n",
      "Iteration 2579, loss = 1939.84424198\n",
      "Iteration 2580, loss = 1939.74533921\n",
      "Iteration 2581, loss = 1939.63297775\n",
      "Iteration 2582, loss = 1939.52923875\n",
      "Iteration 2583, loss = 1939.42541159\n",
      "Iteration 2584, loss = 1939.33584513\n",
      "Iteration 2585, loss = 1939.24077009\n",
      "Iteration 2586, loss = 1939.14169353\n",
      "Iteration 2587, loss = 1939.02302915\n",
      "Iteration 2588, loss = 1938.93338510\n",
      "Iteration 2589, loss = 1938.83709652\n",
      "Iteration 2590, loss = 1938.70786909\n",
      "Iteration 2591, loss = 1938.61719846\n",
      "Iteration 2592, loss = 1938.51853415\n",
      "Iteration 2593, loss = 1938.41615173\n",
      "Iteration 2594, loss = 1938.32312452\n",
      "Iteration 2595, loss = 1938.20417153\n",
      "Iteration 2596, loss = 1938.15025632\n",
      "Iteration 2597, loss = 1938.04537377\n",
      "Iteration 2598, loss = 1937.90826662\n",
      "Iteration 2599, loss = 1937.81535059\n",
      "Iteration 2600, loss = 1937.71374289\n",
      "Iteration 2601, loss = 1937.62070247\n",
      "Iteration 2602, loss = 1937.50076473\n",
      "Iteration 2603, loss = 1937.39361771\n",
      "Iteration 2604, loss = 1937.31859097\n",
      "Iteration 2605, loss = 1937.20627480\n",
      "Iteration 2606, loss = 1937.10530628\n",
      "Iteration 2607, loss = 1937.02461351\n",
      "Iteration 2608, loss = 1936.89061485\n",
      "Iteration 2609, loss = 1936.81840052\n",
      "Iteration 2610, loss = 1936.69310766\n",
      "Iteration 2611, loss = 1936.59260746\n",
      "Iteration 2612, loss = 1936.51169977\n",
      "Iteration 2613, loss = 1936.41983604\n",
      "Iteration 2614, loss = 1936.30290516\n",
      "Iteration 2615, loss = 1936.19287799\n",
      "Iteration 2616, loss = 1936.08324478\n",
      "Iteration 2617, loss = 1935.97918470\n",
      "Iteration 2618, loss = 1935.89591188\n",
      "Iteration 2619, loss = 1935.78908484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2620, loss = 1935.68724764\n",
      "Iteration 2621, loss = 1935.57963417\n",
      "Iteration 2622, loss = 1935.47420634\n",
      "Iteration 2623, loss = 1935.37514762\n",
      "Iteration 2624, loss = 1935.29331659\n",
      "Iteration 2625, loss = 1935.18346324\n",
      "Iteration 2626, loss = 1935.08850890\n",
      "Iteration 2627, loss = 1934.96799675\n",
      "Iteration 2628, loss = 1934.87011229\n",
      "Iteration 2629, loss = 1934.76895939\n",
      "Iteration 2630, loss = 1934.67072911\n",
      "Iteration 2631, loss = 1934.57990826\n",
      "Iteration 2632, loss = 1934.46893918\n",
      "Iteration 2633, loss = 1934.36749692\n",
      "Iteration 2634, loss = 1934.25737662\n",
      "Iteration 2635, loss = 1934.23531625\n",
      "Iteration 2636, loss = 1934.06051593\n",
      "Iteration 2637, loss = 1933.96069317\n",
      "Iteration 2638, loss = 1933.91855331\n",
      "Iteration 2639, loss = 1933.76499109\n",
      "Iteration 2640, loss = 1933.66137520\n",
      "Iteration 2641, loss = 1933.55432567\n",
      "Iteration 2642, loss = 1933.46825275\n",
      "Iteration 2643, loss = 1933.36778912\n",
      "Iteration 2644, loss = 1933.26078305\n",
      "Iteration 2645, loss = 1933.15153316\n",
      "Iteration 2646, loss = 1933.05029309\n",
      "Iteration 2647, loss = 1932.96330204\n",
      "Iteration 2648, loss = 1932.85443552\n",
      "Iteration 2649, loss = 1932.77268265\n",
      "Iteration 2650, loss = 1932.64615294\n",
      "Iteration 2651, loss = 1932.55420569\n",
      "Iteration 2652, loss = 1932.44164999\n",
      "Iteration 2653, loss = 1932.34373001\n",
      "Iteration 2654, loss = 1932.25066386\n",
      "Iteration 2655, loss = 1932.14189085\n",
      "Iteration 2656, loss = 1932.04208598\n",
      "Iteration 2657, loss = 1931.96239298\n",
      "Iteration 2658, loss = 1931.83179122\n",
      "Iteration 2659, loss = 1931.73488899\n",
      "Iteration 2660, loss = 1931.64582437\n",
      "Iteration 2661, loss = 1931.53202149\n",
      "Iteration 2662, loss = 1931.43162515\n",
      "Iteration 2663, loss = 1931.33305825\n",
      "Iteration 2664, loss = 1931.25335494\n",
      "Iteration 2665, loss = 1931.12502411\n",
      "Iteration 2666, loss = 1931.03246096\n",
      "Iteration 2667, loss = 1930.92597928\n",
      "Iteration 2668, loss = 1930.83768403\n",
      "Iteration 2669, loss = 1930.73393402\n",
      "Iteration 2670, loss = 1930.65102958\n",
      "Iteration 2671, loss = 1930.59239911\n",
      "Iteration 2672, loss = 1930.42488612\n",
      "Iteration 2673, loss = 1930.32500379\n",
      "Iteration 2674, loss = 1930.22876034\n",
      "Iteration 2675, loss = 1930.12759725\n",
      "Iteration 2676, loss = 1930.03078157\n",
      "Iteration 2677, loss = 1929.95744876\n",
      "Iteration 2678, loss = 1929.82203324\n",
      "Iteration 2679, loss = 1929.72096760\n",
      "Iteration 2680, loss = 1929.62301619\n",
      "Iteration 2681, loss = 1929.55641755\n",
      "Iteration 2682, loss = 1929.53441352\n",
      "Iteration 2683, loss = 1929.33687696\n",
      "Iteration 2684, loss = 1929.22656318\n",
      "Iteration 2685, loss = 1929.13578978\n",
      "Iteration 2686, loss = 1929.02045340\n",
      "Iteration 2687, loss = 1928.90822716\n",
      "Iteration 2688, loss = 1928.81211886\n",
      "Iteration 2689, loss = 1928.72365007\n",
      "Iteration 2690, loss = 1928.61390718\n",
      "Iteration 2691, loss = 1928.51682050\n",
      "Iteration 2692, loss = 1928.43254818\n",
      "Iteration 2693, loss = 1928.31670087\n",
      "Iteration 2694, loss = 1928.22115875\n",
      "Iteration 2695, loss = 1928.12520740\n",
      "Iteration 2696, loss = 1928.01377612\n",
      "Iteration 2697, loss = 1927.89872037\n",
      "Iteration 2698, loss = 1927.82341407\n",
      "Iteration 2699, loss = 1927.74672422\n",
      "Iteration 2700, loss = 1927.59645690\n",
      "Iteration 2701, loss = 1927.52053680\n",
      "Iteration 2702, loss = 1927.41827915\n",
      "Iteration 2703, loss = 1927.35510349\n",
      "Iteration 2704, loss = 1927.21747724\n",
      "Iteration 2705, loss = 1927.10466240\n",
      "Iteration 2706, loss = 1927.02238726\n",
      "Iteration 2707, loss = 1926.92433346\n",
      "Iteration 2708, loss = 1926.80735652\n",
      "Iteration 2709, loss = 1926.70736901\n",
      "Iteration 2710, loss = 1926.59515103\n",
      "Iteration 2711, loss = 1926.55149571\n",
      "Iteration 2712, loss = 1926.40262064\n",
      "Iteration 2713, loss = 1926.29695365\n",
      "Iteration 2714, loss = 1926.19839208\n",
      "Iteration 2715, loss = 1926.10713271\n",
      "Iteration 2716, loss = 1926.00900842\n",
      "Iteration 2717, loss = 1925.90373694\n",
      "Iteration 2718, loss = 1925.79725864\n",
      "Iteration 2719, loss = 1925.71739112\n",
      "Iteration 2720, loss = 1925.58461286\n",
      "Iteration 2721, loss = 1925.49769473\n",
      "Iteration 2722, loss = 1925.39473867\n",
      "Iteration 2723, loss = 1925.29795608\n",
      "Iteration 2724, loss = 1925.18559700\n",
      "Iteration 2725, loss = 1925.10554770\n",
      "Iteration 2726, loss = 1925.01512920\n",
      "Iteration 2727, loss = 1924.91780154\n",
      "Iteration 2728, loss = 1924.78780860\n",
      "Iteration 2729, loss = 1924.69008191\n",
      "Iteration 2730, loss = 1924.58175275\n",
      "Iteration 2731, loss = 1924.48478076\n",
      "Iteration 2732, loss = 1924.38397824\n",
      "Iteration 2733, loss = 1924.27908276\n",
      "Iteration 2734, loss = 1924.19320828\n",
      "Iteration 2735, loss = 1924.09755973\n",
      "Iteration 2736, loss = 1923.98640290\n",
      "Iteration 2737, loss = 1923.93480180\n",
      "Iteration 2738, loss = 1923.77624862\n",
      "Iteration 2739, loss = 1923.68770439\n",
      "Iteration 2740, loss = 1923.59796550\n",
      "Iteration 2741, loss = 1923.49761004\n",
      "Iteration 2742, loss = 1923.37323493\n",
      "Iteration 2743, loss = 1923.28351893\n",
      "Iteration 2744, loss = 1923.18847656\n",
      "Iteration 2745, loss = 1923.09047429\n",
      "Iteration 2746, loss = 1922.97913297\n",
      "Iteration 2747, loss = 1922.89028213\n",
      "Iteration 2748, loss = 1922.76862801\n",
      "Iteration 2749, loss = 1922.67187735\n",
      "Iteration 2750, loss = 1922.58206166\n",
      "Iteration 2751, loss = 1922.46532503\n",
      "Iteration 2752, loss = 1922.37918597\n",
      "Iteration 2753, loss = 1922.27747791\n",
      "Iteration 2754, loss = 1922.20281638\n",
      "Iteration 2755, loss = 1922.10755317\n",
      "Iteration 2756, loss = 1921.97455159\n",
      "Iteration 2757, loss = 1921.88157233\n",
      "Iteration 2758, loss = 1921.82922475\n",
      "Iteration 2759, loss = 1921.66221563\n",
      "Iteration 2760, loss = 1921.56380774\n",
      "Iteration 2761, loss = 1921.46730717\n",
      "Iteration 2762, loss = 1921.38070517\n",
      "Iteration 2763, loss = 1921.26275800\n",
      "Iteration 2764, loss = 1921.16355375\n",
      "Iteration 2765, loss = 1921.07139386\n",
      "Iteration 2766, loss = 1920.96009779\n",
      "Iteration 2767, loss = 1920.85873742\n",
      "Iteration 2768, loss = 1920.76525346\n",
      "Iteration 2769, loss = 1920.65413874\n",
      "Iteration 2770, loss = 1920.57208778\n",
      "Iteration 2771, loss = 1920.46369463\n",
      "Iteration 2772, loss = 1920.35887804\n",
      "Iteration 2773, loss = 1920.30010452\n",
      "Iteration 2774, loss = 1920.15899241\n",
      "Iteration 2775, loss = 1920.05788491\n",
      "Iteration 2776, loss = 1919.94986248\n",
      "Iteration 2777, loss = 1919.85806456\n",
      "Iteration 2778, loss = 1919.75779171\n",
      "Iteration 2779, loss = 1919.65398384\n",
      "Iteration 2780, loss = 1919.54958029\n",
      "Iteration 2781, loss = 1919.46698549\n",
      "Iteration 2782, loss = 1919.36475361\n",
      "Iteration 2783, loss = 1919.24856120\n",
      "Iteration 2784, loss = 1919.14846990\n",
      "Iteration 2785, loss = 1919.06001802\n",
      "Iteration 2786, loss = 1918.94345265\n",
      "Iteration 2787, loss = 1918.84887680\n",
      "Iteration 2788, loss = 1918.76417191\n",
      "Iteration 2789, loss = 1918.65938100\n",
      "Iteration 2790, loss = 1918.54046349\n",
      "Iteration 2791, loss = 1918.44827899\n",
      "Iteration 2792, loss = 1918.34650622\n",
      "Iteration 2793, loss = 1918.31527691\n",
      "Iteration 2794, loss = 1918.14423585\n",
      "Iteration 2795, loss = 1918.07718529\n",
      "Iteration 2796, loss = 1917.97817192\n",
      "Iteration 2797, loss = 1917.85445056\n",
      "Iteration 2798, loss = 1917.74556271\n",
      "Iteration 2799, loss = 1917.65955925\n",
      "Iteration 2800, loss = 1917.58820252\n",
      "Iteration 2801, loss = 1917.48438401\n",
      "Iteration 2802, loss = 1917.33532625\n",
      "Iteration 2803, loss = 1917.24288285\n",
      "Iteration 2804, loss = 1917.12837735\n",
      "Iteration 2805, loss = 1917.02723779\n",
      "Iteration 2806, loss = 1916.94276735\n",
      "Iteration 2807, loss = 1916.83086899\n",
      "Iteration 2808, loss = 1916.73021498\n",
      "Iteration 2809, loss = 1916.63936361\n",
      "Iteration 2810, loss = 1916.52442302\n",
      "Iteration 2811, loss = 1916.43685725\n",
      "Iteration 2812, loss = 1916.34552442\n",
      "Iteration 2813, loss = 1916.24566392\n",
      "Iteration 2814, loss = 1916.13425091\n",
      "Iteration 2815, loss = 1916.03389750\n",
      "Iteration 2816, loss = 1915.92822741\n",
      "Iteration 2817, loss = 1915.82922823\n",
      "Iteration 2818, loss = 1915.72215746\n",
      "Iteration 2819, loss = 1915.64190904\n",
      "Iteration 2820, loss = 1915.52548023\n",
      "Iteration 2821, loss = 1915.42375691\n",
      "Iteration 2822, loss = 1915.33370151\n",
      "Iteration 2823, loss = 1915.24562188\n",
      "Iteration 2824, loss = 1915.14919725\n",
      "Iteration 2825, loss = 1915.02382538\n",
      "Iteration 2826, loss = 1914.92890718\n",
      "Iteration 2827, loss = 1914.90555953\n",
      "Iteration 2828, loss = 1914.72962685\n",
      "Iteration 2829, loss = 1914.61967816\n",
      "Iteration 2830, loss = 1914.56470253\n",
      "Iteration 2831, loss = 1914.43917209\n",
      "Iteration 2832, loss = 1914.34890320\n",
      "Iteration 2833, loss = 1914.22149199\n",
      "Iteration 2834, loss = 1914.11302115\n",
      "Iteration 2835, loss = 1914.02885277\n",
      "Iteration 2836, loss = 1913.91460541\n",
      "Iteration 2837, loss = 1913.81315897\n",
      "Iteration 2838, loss = 1913.74068458\n",
      "Iteration 2839, loss = 1913.60035625\n",
      "Iteration 2840, loss = 1913.52611882\n",
      "Iteration 2841, loss = 1913.42953989\n",
      "Iteration 2842, loss = 1913.32407302\n",
      "Iteration 2843, loss = 1913.21755571\n",
      "Iteration 2844, loss = 1913.15717557\n",
      "Iteration 2845, loss = 1913.05237881\n",
      "Iteration 2846, loss = 1912.95282415\n",
      "Iteration 2847, loss = 1912.83197707\n",
      "Iteration 2848, loss = 1912.71777854\n",
      "Iteration 2849, loss = 1912.61119538\n",
      "Iteration 2850, loss = 1912.52885816\n",
      "Iteration 2851, loss = 1912.41418374\n",
      "Iteration 2852, loss = 1912.29922353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2853, loss = 1912.20010671\n",
      "Iteration 2854, loss = 1912.09966009\n",
      "Iteration 2855, loss = 1912.01419677\n",
      "Iteration 2856, loss = 1911.89508124\n",
      "Iteration 2857, loss = 1911.83074652\n",
      "Iteration 2858, loss = 1911.73620534\n",
      "Iteration 2859, loss = 1911.58500297\n",
      "Iteration 2860, loss = 1911.51400655\n",
      "Iteration 2861, loss = 1911.39116172\n",
      "Iteration 2862, loss = 1911.29175082\n",
      "Iteration 2863, loss = 1911.19416284\n",
      "Iteration 2864, loss = 1911.10617993\n",
      "Iteration 2865, loss = 1910.99498154\n",
      "Iteration 2866, loss = 1910.93464001\n",
      "Iteration 2867, loss = 1910.86357074\n",
      "Iteration 2868, loss = 1910.68929674\n",
      "Iteration 2869, loss = 1910.57897448\n",
      "Iteration 2870, loss = 1910.50522416\n",
      "Iteration 2871, loss = 1910.38964669\n",
      "Iteration 2872, loss = 1910.28826250\n",
      "Iteration 2873, loss = 1910.18389097\n",
      "Iteration 2874, loss = 1910.09333523\n",
      "Iteration 2875, loss = 1909.98544708\n",
      "Iteration 2876, loss = 1909.88980055\n",
      "Iteration 2877, loss = 1909.78454529\n",
      "Iteration 2878, loss = 1909.68919095\n",
      "Iteration 2879, loss = 1909.60405233\n",
      "Iteration 2880, loss = 1909.52038618\n",
      "Iteration 2881, loss = 1909.40660311\n",
      "Iteration 2882, loss = 1909.28456925\n",
      "Iteration 2883, loss = 1909.18014084\n",
      "Iteration 2884, loss = 1909.09034511\n",
      "Iteration 2885, loss = 1909.04103958\n",
      "Iteration 2886, loss = 1908.87952784\n",
      "Iteration 2887, loss = 1908.77270247\n",
      "Iteration 2888, loss = 1908.68189034\n",
      "Iteration 2889, loss = 1908.63117072\n",
      "Iteration 2890, loss = 1908.49010536\n",
      "Iteration 2891, loss = 1908.38692323\n",
      "Iteration 2892, loss = 1908.28269181\n",
      "Iteration 2893, loss = 1908.17508141\n",
      "Iteration 2894, loss = 1908.08937343\n",
      "Iteration 2895, loss = 1907.98422768\n",
      "Iteration 2896, loss = 1907.87747838\n",
      "Iteration 2897, loss = 1907.78200606\n",
      "Iteration 2898, loss = 1907.67375539\n",
      "Iteration 2899, loss = 1907.57757604\n",
      "Iteration 2900, loss = 1907.46849617\n",
      "Iteration 2901, loss = 1907.37867273\n",
      "Iteration 2902, loss = 1907.27843623\n",
      "Iteration 2903, loss = 1907.16294028\n",
      "Iteration 2904, loss = 1907.08301066\n",
      "Iteration 2905, loss = 1906.96070362\n",
      "Iteration 2906, loss = 1906.85615925\n",
      "Iteration 2907, loss = 1906.79580715\n",
      "Iteration 2908, loss = 1906.65970503\n",
      "Iteration 2909, loss = 1906.58644720\n",
      "Iteration 2910, loss = 1906.45607442\n",
      "Iteration 2911, loss = 1906.36701823\n",
      "Iteration 2912, loss = 1906.26526183\n",
      "Iteration 2913, loss = 1906.14776734\n",
      "Iteration 2914, loss = 1906.06503854\n",
      "Iteration 2915, loss = 1905.96150688\n",
      "Iteration 2916, loss = 1905.84731326\n",
      "Iteration 2917, loss = 1905.79279611\n",
      "Iteration 2918, loss = 1905.65773255\n",
      "Iteration 2919, loss = 1905.55569468\n",
      "Iteration 2920, loss = 1905.46008627\n",
      "Iteration 2921, loss = 1905.35663307\n",
      "Iteration 2922, loss = 1905.26466139\n",
      "Iteration 2923, loss = 1905.18053561\n",
      "Iteration 2924, loss = 1905.06406869\n",
      "Iteration 2925, loss = 1904.95388946\n",
      "Iteration 2926, loss = 1904.85619983\n",
      "Iteration 2927, loss = 1904.75574111\n",
      "Iteration 2928, loss = 1904.65600019\n",
      "Iteration 2929, loss = 1904.57871060\n",
      "Iteration 2930, loss = 1904.46979662\n",
      "Iteration 2931, loss = 1904.34600774\n",
      "Iteration 2932, loss = 1904.25355991\n",
      "Iteration 2933, loss = 1904.14988412\n",
      "Iteration 2934, loss = 1904.04901894\n",
      "Iteration 2935, loss = 1903.94465893\n",
      "Iteration 2936, loss = 1903.84011524\n",
      "Iteration 2937, loss = 1903.72833674\n",
      "Iteration 2938, loss = 1903.63074719\n",
      "Iteration 2939, loss = 1903.54719564\n",
      "Iteration 2940, loss = 1903.44710926\n",
      "Iteration 2941, loss = 1903.33537722\n",
      "Iteration 2942, loss = 1903.29193844\n",
      "Iteration 2943, loss = 1903.13995986\n",
      "Iteration 2944, loss = 1903.04456224\n",
      "Iteration 2945, loss = 1902.98018528\n",
      "Iteration 2946, loss = 1902.83731801\n",
      "Iteration 2947, loss = 1902.71957990\n",
      "Iteration 2948, loss = 1902.64646275\n",
      "Iteration 2949, loss = 1902.52965427\n",
      "Iteration 2950, loss = 1902.48835660\n",
      "Iteration 2951, loss = 1902.32200347\n",
      "Iteration 2952, loss = 1902.21649876\n",
      "Iteration 2953, loss = 1902.11377249\n",
      "Iteration 2954, loss = 1902.04060732\n",
      "Iteration 2955, loss = 1901.91809406\n",
      "Iteration 2956, loss = 1901.83109727\n",
      "Iteration 2957, loss = 1901.72782822\n",
      "Iteration 2958, loss = 1901.62739055\n",
      "Iteration 2959, loss = 1901.55446036\n",
      "Iteration 2960, loss = 1901.40980418\n",
      "Iteration 2961, loss = 1901.31580867\n",
      "Iteration 2962, loss = 1901.21440803\n",
      "Iteration 2963, loss = 1901.12599510\n",
      "Iteration 2964, loss = 1901.00803176\n",
      "Iteration 2965, loss = 1900.91005916\n",
      "Iteration 2966, loss = 1900.84265779\n",
      "Iteration 2967, loss = 1900.74253496\n",
      "Iteration 2968, loss = 1900.68954722\n",
      "Iteration 2969, loss = 1900.53728375\n",
      "Iteration 2970, loss = 1900.41532983\n",
      "Iteration 2971, loss = 1900.31427600\n",
      "Iteration 2972, loss = 1900.20954542\n",
      "Iteration 2973, loss = 1900.09249558\n",
      "Iteration 2974, loss = 1900.01055298\n",
      "Iteration 2975, loss = 1899.93686570\n",
      "Iteration 2976, loss = 1899.81543440\n",
      "Iteration 2977, loss = 1899.69495943\n",
      "Iteration 2978, loss = 1899.60430360\n",
      "Iteration 2979, loss = 1899.49950189\n",
      "Iteration 2980, loss = 1899.39722372\n",
      "Iteration 2981, loss = 1899.29994414\n",
      "Iteration 2982, loss = 1899.23664403\n",
      "Iteration 2983, loss = 1899.10360315\n",
      "Iteration 2984, loss = 1899.17986616\n",
      "Iteration 2985, loss = 1898.86111391\n",
      "Iteration 2986, loss = 1898.80583791\n",
      "Iteration 2987, loss = 1898.70745310\n",
      "Iteration 2988, loss = 1898.59882390\n",
      "Iteration 2989, loss = 1898.47646999\n",
      "Iteration 2990, loss = 1898.40136756\n",
      "Iteration 2991, loss = 1898.30901102\n",
      "Iteration 2992, loss = 1898.18815267\n",
      "Iteration 2993, loss = 1898.08064717\n",
      "Iteration 2994, loss = 1897.97914497\n",
      "Iteration 2995, loss = 1897.87187379\n",
      "Iteration 2996, loss = 1897.77831839\n",
      "Iteration 2997, loss = 1897.69154249\n",
      "Iteration 2998, loss = 1897.59612415\n",
      "Iteration 2999, loss = 1897.46927246\n",
      "Iteration 3000, loss = 1897.39190117\n",
      "Iteration 3001, loss = 1897.28735325\n",
      "Iteration 3002, loss = 1897.17898059\n",
      "Iteration 3003, loss = 1897.09147095\n",
      "Iteration 3004, loss = 1896.96341958\n",
      "Iteration 3005, loss = 1896.87133087\n",
      "Iteration 3006, loss = 1896.77711126\n",
      "Iteration 3007, loss = 1896.68620620\n",
      "Iteration 3008, loss = 1896.59017828\n",
      "Iteration 3009, loss = 1896.48499609\n",
      "Iteration 3010, loss = 1896.38266624\n",
      "Iteration 3011, loss = 1896.28145768\n",
      "Iteration 3012, loss = 1896.15692749\n",
      "Iteration 3013, loss = 1896.10169751\n",
      "Iteration 3014, loss = 1895.94692379\n",
      "Iteration 3015, loss = 1895.90248842\n",
      "Iteration 3016, loss = 1895.76401407\n",
      "Iteration 3017, loss = 1895.66285134\n",
      "Iteration 3018, loss = 1895.55253266\n",
      "Iteration 3019, loss = 1895.43898724\n",
      "Iteration 3020, loss = 1895.35464180\n",
      "Iteration 3021, loss = 1895.24234274\n",
      "Iteration 3022, loss = 1895.21253866\n",
      "Iteration 3023, loss = 1895.04625208\n",
      "Iteration 3024, loss = 1894.97192355\n",
      "Iteration 3025, loss = 1894.86635520\n",
      "Iteration 3026, loss = 1894.75841924\n",
      "Iteration 3027, loss = 1894.63550005\n",
      "Iteration 3028, loss = 1894.53889053\n",
      "Iteration 3029, loss = 1894.43485780\n",
      "Iteration 3030, loss = 1894.35968167\n",
      "Iteration 3031, loss = 1894.25347572\n",
      "Iteration 3032, loss = 1894.14203118\n",
      "Iteration 3033, loss = 1894.05626031\n",
      "Iteration 3034, loss = 1894.03343050\n",
      "Iteration 3035, loss = 1893.84305260\n",
      "Iteration 3036, loss = 1893.78741860\n",
      "Iteration 3037, loss = 1893.61373202\n",
      "Iteration 3038, loss = 1893.53416222\n",
      "Iteration 3039, loss = 1893.42264790\n",
      "Iteration 3040, loss = 1893.32165028\n",
      "Iteration 3041, loss = 1893.36238119\n",
      "Iteration 3042, loss = 1893.11559037\n",
      "Iteration 3043, loss = 1893.06450672\n",
      "Iteration 3044, loss = 1892.95557536\n",
      "Iteration 3045, loss = 1892.87324915\n",
      "Iteration 3046, loss = 1892.75738243\n",
      "Iteration 3047, loss = 1892.61602233\n",
      "Iteration 3048, loss = 1892.50673940\n",
      "Iteration 3049, loss = 1892.40094602\n",
      "Iteration 3050, loss = 1892.31564507\n",
      "Iteration 3051, loss = 1892.26322047\n",
      "Iteration 3052, loss = 1892.12581894\n",
      "Iteration 3053, loss = 1892.02219103\n",
      "Iteration 3054, loss = 1891.89400967\n",
      "Iteration 3055, loss = 1891.83463405\n",
      "Iteration 3056, loss = 1891.68232934\n",
      "Iteration 3057, loss = 1891.58483367\n",
      "Iteration 3058, loss = 1891.50868819\n",
      "Iteration 3059, loss = 1891.37912713\n",
      "Iteration 3060, loss = 1891.28786198\n",
      "Iteration 3061, loss = 1891.28076037\n",
      "Iteration 3062, loss = 1891.14846654\n",
      "Iteration 3063, loss = 1890.98986797\n",
      "Iteration 3064, loss = 1890.88395005\n",
      "Iteration 3065, loss = 1890.77676772\n",
      "Iteration 3066, loss = 1890.66695556\n",
      "Iteration 3067, loss = 1890.62205335\n",
      "Iteration 3068, loss = 1890.47056952\n",
      "Iteration 3069, loss = 1890.37368480\n",
      "Iteration 3070, loss = 1890.31055437\n",
      "Iteration 3071, loss = 1890.18524093\n",
      "Iteration 3072, loss = 1890.07382127\n",
      "Iteration 3073, loss = 1889.95274542\n",
      "Iteration 3074, loss = 1889.86300002\n",
      "Iteration 3075, loss = 1889.77944673\n",
      "Iteration 3076, loss = 1889.65877166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3077, loss = 1889.56081206\n",
      "Iteration 3078, loss = 1889.49435937\n",
      "Iteration 3079, loss = 1889.37530915\n",
      "Iteration 3080, loss = 1889.26872047\n",
      "Iteration 3081, loss = 1889.13735936\n",
      "Iteration 3082, loss = 1889.03573614\n",
      "Iteration 3083, loss = 1888.94914048\n",
      "Iteration 3084, loss = 1888.85561119\n",
      "Iteration 3085, loss = 1888.80662017\n",
      "Iteration 3086, loss = 1888.62677750\n",
      "Iteration 3087, loss = 1888.53109219\n",
      "Iteration 3088, loss = 1888.42519373\n",
      "Iteration 3089, loss = 1888.34818347\n",
      "Iteration 3090, loss = 1888.26473816\n",
      "Iteration 3091, loss = 1888.11708921\n",
      "Iteration 3092, loss = 1888.00481487\n",
      "Iteration 3093, loss = 1888.00325248\n",
      "Iteration 3094, loss = 1887.91831648\n",
      "Iteration 3095, loss = 1887.74529128\n",
      "Iteration 3096, loss = 1887.63997983\n",
      "Iteration 3097, loss = 1887.51630333\n",
      "Iteration 3098, loss = 1887.46407688\n",
      "Iteration 3099, loss = 1887.30848851\n",
      "Iteration 3100, loss = 1887.21035616\n",
      "Iteration 3101, loss = 1887.10946366\n",
      "Iteration 3102, loss = 1886.98089636\n",
      "Iteration 3103, loss = 1886.92093980\n",
      "Iteration 3104, loss = 1886.79442742\n",
      "Iteration 3105, loss = 1886.70272812\n",
      "Iteration 3106, loss = 1886.56997133\n",
      "Iteration 3107, loss = 1886.54435023\n",
      "Iteration 3108, loss = 1886.38258183\n",
      "Iteration 3109, loss = 1886.27275463\n",
      "Iteration 3110, loss = 1886.21673625\n",
      "Iteration 3111, loss = 1886.05567270\n",
      "Iteration 3112, loss = 1885.96644051\n",
      "Iteration 3113, loss = 1885.86802324\n",
      "Iteration 3114, loss = 1885.78122608\n",
      "Iteration 3115, loss = 1885.66353729\n",
      "Iteration 3116, loss = 1885.63166639\n",
      "Iteration 3117, loss = 1885.44591140\n",
      "Iteration 3118, loss = 1885.33404511\n",
      "Iteration 3119, loss = 1885.25310954\n",
      "Iteration 3120, loss = 1885.25185829\n",
      "Iteration 3121, loss = 1885.04570744\n",
      "Iteration 3122, loss = 1884.95461564\n",
      "Iteration 3123, loss = 1884.85170945\n",
      "Iteration 3124, loss = 1884.78787799\n",
      "Iteration 3125, loss = 1884.73015631\n",
      "Iteration 3126, loss = 1884.52606003\n",
      "Iteration 3127, loss = 1884.43599379\n",
      "Iteration 3128, loss = 1884.39724312\n",
      "Iteration 3129, loss = 1884.23701986\n",
      "Iteration 3130, loss = 1884.12400796\n",
      "Iteration 3131, loss = 1884.00636800\n",
      "Iteration 3132, loss = 1883.92847425\n",
      "Iteration 3133, loss = 1883.81781302\n",
      "Iteration 3134, loss = 1883.71442727\n",
      "Iteration 3135, loss = 1883.61280032\n",
      "Iteration 3136, loss = 1883.50300927\n",
      "Iteration 3137, loss = 1883.45293992\n",
      "Iteration 3138, loss = 1883.33095990\n",
      "Iteration 3139, loss = 1883.18389884\n",
      "Iteration 3140, loss = 1883.09976998\n",
      "Iteration 3141, loss = 1883.03653007\n",
      "Iteration 3142, loss = 1882.92142500\n",
      "Iteration 3143, loss = 1882.79758983\n",
      "Iteration 3144, loss = 1882.78493763\n",
      "Iteration 3145, loss = 1882.56459942\n",
      "Iteration 3146, loss = 1882.49428926\n",
      "Iteration 3147, loss = 1882.38086773\n",
      "Iteration 3148, loss = 1882.32182285\n",
      "Iteration 3149, loss = 1882.23956017\n",
      "Iteration 3150, loss = 1882.06194011\n",
      "Iteration 3151, loss = 1881.96321927\n",
      "Iteration 3152, loss = 1881.90961090\n",
      "Iteration 3153, loss = 1881.83896596\n",
      "Iteration 3154, loss = 1881.63446932\n",
      "Iteration 3155, loss = 1881.53784814\n",
      "Iteration 3156, loss = 1881.46089349\n",
      "Iteration 3157, loss = 1881.38634055\n",
      "Iteration 3158, loss = 1881.23278381\n",
      "Iteration 3159, loss = 1881.14800067\n",
      "Iteration 3160, loss = 1881.07765420\n",
      "Iteration 3161, loss = 1881.02876645\n",
      "Iteration 3162, loss = 1880.83297034\n",
      "Iteration 3163, loss = 1880.75270043\n",
      "Iteration 3164, loss = 1880.61795367\n",
      "Iteration 3165, loss = 1880.51201998\n",
      "Iteration 3166, loss = 1880.42900573\n",
      "Iteration 3167, loss = 1880.34726648\n",
      "Iteration 3168, loss = 1880.20308389\n",
      "Iteration 3169, loss = 1880.13971911\n",
      "Iteration 3170, loss = 1880.00873205\n",
      "Iteration 3171, loss = 1879.88853180\n",
      "Iteration 3172, loss = 1879.82151484\n",
      "Iteration 3173, loss = 1879.68488591\n",
      "Iteration 3174, loss = 1879.61244454\n",
      "Iteration 3175, loss = 1879.48349811\n",
      "Iteration 3176, loss = 1879.41042519\n",
      "Iteration 3177, loss = 1879.37828957\n",
      "Iteration 3178, loss = 1879.20595011\n",
      "Iteration 3179, loss = 1879.09105523\n",
      "Iteration 3180, loss = 1878.98008790\n",
      "Iteration 3181, loss = 1878.86532301\n",
      "Iteration 3182, loss = 1878.79584995\n",
      "Iteration 3183, loss = 1878.67441956\n",
      "Iteration 3184, loss = 1878.57155024\n",
      "Iteration 3185, loss = 1878.45884345\n",
      "Iteration 3186, loss = 1878.34642316\n",
      "Iteration 3187, loss = 1878.26216518\n",
      "Iteration 3188, loss = 1878.16935905\n",
      "Iteration 3189, loss = 1878.03132049\n",
      "Iteration 3190, loss = 1877.94580689\n",
      "Iteration 3191, loss = 1877.83042745\n",
      "Iteration 3192, loss = 1877.71612669\n",
      "Iteration 3193, loss = 1877.61278983\n",
      "Iteration 3194, loss = 1877.54048781\n",
      "Iteration 3195, loss = 1877.42762239\n",
      "Iteration 3196, loss = 1877.31433219\n",
      "Iteration 3197, loss = 1877.20566014\n",
      "Iteration 3198, loss = 1877.13234596\n",
      "Iteration 3199, loss = 1876.99185252\n",
      "Iteration 3200, loss = 1876.93667293\n",
      "Iteration 3201, loss = 1876.80621740\n",
      "Iteration 3202, loss = 1876.69633381\n",
      "Iteration 3203, loss = 1876.58165553\n",
      "Iteration 3204, loss = 1876.46293759\n",
      "Iteration 3205, loss = 1876.44508272\n",
      "Iteration 3206, loss = 1876.27609334\n",
      "Iteration 3207, loss = 1876.16439280\n",
      "Iteration 3208, loss = 1876.06172811\n",
      "Iteration 3209, loss = 1876.00108867\n",
      "Iteration 3210, loss = 1875.91800108\n",
      "Iteration 3211, loss = 1875.91576464\n",
      "Iteration 3212, loss = 1875.68670820\n",
      "Iteration 3213, loss = 1875.54574208\n",
      "Iteration 3214, loss = 1875.44369143\n",
      "Iteration 3215, loss = 1875.35179454\n",
      "Iteration 3216, loss = 1875.25183443\n",
      "Iteration 3217, loss = 1875.13794792\n",
      "Iteration 3218, loss = 1875.03009386\n",
      "Iteration 3219, loss = 1874.93744904\n",
      "Iteration 3220, loss = 1875.04391840\n",
      "Iteration 3221, loss = 1874.72594021\n",
      "Iteration 3222, loss = 1874.63466139\n",
      "Iteration 3223, loss = 1874.53353219\n",
      "Iteration 3224, loss = 1874.41569544\n",
      "Iteration 3225, loss = 1874.30603370\n",
      "Iteration 3226, loss = 1874.18556911\n",
      "Iteration 3227, loss = 1874.08377145\n",
      "Iteration 3228, loss = 1874.05288330\n",
      "Iteration 3229, loss = 1873.88250619\n",
      "Iteration 3230, loss = 1873.75936313\n",
      "Iteration 3231, loss = 1873.68254995\n",
      "Iteration 3232, loss = 1873.62141978\n",
      "Iteration 3233, loss = 1873.45783018\n",
      "Iteration 3234, loss = 1873.39752609\n",
      "Iteration 3235, loss = 1873.24859873\n",
      "Iteration 3236, loss = 1873.19407677\n",
      "Iteration 3237, loss = 1873.05739976\n",
      "Iteration 3238, loss = 1872.97241817\n",
      "Iteration 3239, loss = 1872.94712075\n",
      "Iteration 3240, loss = 1872.73111311\n",
      "Iteration 3241, loss = 1872.64944693\n",
      "Iteration 3242, loss = 1872.52374137\n",
      "Iteration 3243, loss = 1872.42596863\n",
      "Iteration 3244, loss = 1872.33289140\n",
      "Iteration 3245, loss = 1872.23528983\n",
      "Iteration 3246, loss = 1872.13705460\n",
      "Iteration 3247, loss = 1871.98212988\n",
      "Iteration 3248, loss = 1871.90248386\n",
      "Iteration 3249, loss = 1871.81756669\n",
      "Iteration 3250, loss = 1871.68645145\n",
      "Iteration 3251, loss = 1871.58976066\n",
      "Iteration 3252, loss = 1871.55194042\n",
      "Iteration 3253, loss = 1871.41844136\n",
      "Iteration 3254, loss = 1871.34643631\n",
      "Iteration 3255, loss = 1871.23755640\n",
      "Iteration 3256, loss = 1871.19085628\n",
      "Iteration 3257, loss = 1871.01084501\n",
      "Iteration 3258, loss = 1870.96097831\n",
      "Iteration 3259, loss = 1870.71842673\n",
      "Iteration 3260, loss = 1870.72343381\n",
      "Iteration 3261, loss = 1870.56312314\n",
      "Iteration 3262, loss = 1870.44588954\n",
      "Iteration 3263, loss = 1870.33280068\n",
      "Iteration 3264, loss = 1870.22504308\n",
      "Iteration 3265, loss = 1870.15778393\n",
      "Iteration 3266, loss = 1870.01030476\n",
      "Iteration 3267, loss = 1869.91446877\n",
      "Iteration 3268, loss = 1869.80836011\n",
      "Iteration 3269, loss = 1869.70539810\n",
      "Iteration 3270, loss = 1869.58996516\n",
      "Iteration 3271, loss = 1869.51155833\n",
      "Iteration 3272, loss = 1869.36433339\n",
      "Iteration 3273, loss = 1869.27101248\n",
      "Iteration 3274, loss = 1869.16488594\n",
      "Iteration 3275, loss = 1869.09508032\n",
      "Iteration 3276, loss = 1868.98031934\n",
      "Iteration 3277, loss = 1868.93929995\n",
      "Iteration 3278, loss = 1868.73200653\n",
      "Iteration 3279, loss = 1868.63050820\n",
      "Iteration 3280, loss = 1868.57415602\n",
      "Iteration 3281, loss = 1868.42560174\n",
      "Iteration 3282, loss = 1868.36039199\n",
      "Iteration 3283, loss = 1868.23878839\n",
      "Iteration 3284, loss = 1868.25496807\n",
      "Iteration 3285, loss = 1868.04266919\n",
      "Iteration 3286, loss = 1867.89106397\n",
      "Iteration 3287, loss = 1867.81717729\n",
      "Iteration 3288, loss = 1867.71436377\n",
      "Iteration 3289, loss = 1867.58883809\n",
      "Iteration 3290, loss = 1867.47624217\n",
      "Iteration 3291, loss = 1867.37398590\n",
      "Iteration 3292, loss = 1867.31016036\n",
      "Iteration 3293, loss = 1867.18006224\n",
      "Iteration 3294, loss = 1867.04964676\n",
      "Iteration 3295, loss = 1867.00267294\n",
      "Iteration 3296, loss = 1866.85307329\n",
      "Iteration 3297, loss = 1866.75231421\n",
      "Iteration 3298, loss = 1866.65007836\n",
      "Iteration 3299, loss = 1866.53846530\n",
      "Iteration 3300, loss = 1866.47223375\n",
      "Iteration 3301, loss = 1866.29276157\n",
      "Iteration 3302, loss = 1866.21555033\n",
      "Iteration 3303, loss = 1866.10729306\n",
      "Iteration 3304, loss = 1865.99266839\n",
      "Iteration 3305, loss = 1865.89229969\n",
      "Iteration 3306, loss = 1865.83789429\n",
      "Iteration 3307, loss = 1865.66736405\n",
      "Iteration 3308, loss = 1865.56668634\n",
      "Iteration 3309, loss = 1865.45962184\n",
      "Iteration 3310, loss = 1865.44529342\n",
      "Iteration 3311, loss = 1865.27608443\n",
      "Iteration 3312, loss = 1865.17113874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3313, loss = 1865.06636795\n",
      "Iteration 3314, loss = 1864.92164088\n",
      "Iteration 3315, loss = 1864.80180991\n",
      "Iteration 3316, loss = 1864.71885082\n",
      "Iteration 3317, loss = 1864.66915851\n",
      "Iteration 3318, loss = 1864.54032975\n",
      "Iteration 3319, loss = 1864.38803570\n",
      "Iteration 3320, loss = 1864.45812624\n",
      "Iteration 3321, loss = 1864.21378914\n",
      "Iteration 3322, loss = 1864.06999508\n",
      "Iteration 3323, loss = 1864.03329458\n",
      "Iteration 3324, loss = 1863.87162265\n",
      "Iteration 3325, loss = 1863.79824760\n",
      "Iteration 3326, loss = 1863.66742021\n",
      "Iteration 3327, loss = 1863.57500324\n",
      "Iteration 3328, loss = 1863.55065050\n",
      "Iteration 3329, loss = 1863.33679472\n",
      "Iteration 3330, loss = 1863.22341894\n",
      "Iteration 3331, loss = 1863.12232149\n",
      "Iteration 3332, loss = 1863.02993360\n",
      "Iteration 3333, loss = 1862.93307138\n",
      "Iteration 3334, loss = 1862.79869344\n",
      "Iteration 3335, loss = 1862.74368809\n",
      "Iteration 3336, loss = 1862.58043408\n",
      "Iteration 3337, loss = 1862.45296940\n",
      "Iteration 3338, loss = 1862.45249463\n",
      "Iteration 3339, loss = 1862.31238196\n",
      "Iteration 3340, loss = 1862.20176843\n",
      "Iteration 3341, loss = 1862.10656835\n",
      "Iteration 3342, loss = 1862.01210374\n",
      "Iteration 3343, loss = 1861.95189750\n",
      "Iteration 3344, loss = 1861.75184336\n",
      "Iteration 3345, loss = 1861.67948521\n",
      "Iteration 3346, loss = 1861.50099701\n",
      "Iteration 3347, loss = 1861.39169866\n",
      "Iteration 3348, loss = 1861.30085183\n",
      "Iteration 3349, loss = 1861.21325330\n",
      "Iteration 3350, loss = 1861.10888417\n",
      "Iteration 3351, loss = 1860.95384479\n",
      "Iteration 3352, loss = 1860.90527134\n",
      "Iteration 3353, loss = 1860.77032544\n",
      "Iteration 3354, loss = 1860.66757003\n",
      "Iteration 3355, loss = 1860.61668127\n",
      "Iteration 3356, loss = 1860.49633416\n",
      "Iteration 3357, loss = 1860.37650355\n",
      "Iteration 3358, loss = 1860.23082519\n",
      "Iteration 3359, loss = 1860.19149652\n",
      "Iteration 3360, loss = 1860.02302516\n",
      "Iteration 3361, loss = 1860.01489023\n",
      "Iteration 3362, loss = 1859.78373184\n",
      "Iteration 3363, loss = 1859.69388015\n",
      "Iteration 3364, loss = 1859.55988700\n",
      "Iteration 3365, loss = 1859.48850795\n",
      "Iteration 3366, loss = 1859.38827996\n",
      "Iteration 3367, loss = 1859.25049691\n",
      "Iteration 3368, loss = 1859.11728703\n",
      "Iteration 3369, loss = 1859.06393378\n",
      "Iteration 3370, loss = 1858.92721580\n",
      "Iteration 3371, loss = 1858.81914782\n",
      "Iteration 3372, loss = 1858.71012653\n",
      "Iteration 3373, loss = 1858.73560206\n",
      "Iteration 3374, loss = 1858.53584583\n",
      "Iteration 3375, loss = 1858.37852853\n",
      "Iteration 3376, loss = 1858.28458943\n",
      "Iteration 3377, loss = 1858.24521307\n",
      "Iteration 3378, loss = 1858.31298450\n",
      "Iteration 3379, loss = 1857.95250333\n",
      "Iteration 3380, loss = 1857.81713820\n",
      "Iteration 3381, loss = 1857.71843152\n",
      "Iteration 3382, loss = 1857.62838407\n",
      "Iteration 3383, loss = 1857.51319809\n",
      "Iteration 3384, loss = 1857.39299915\n",
      "Iteration 3385, loss = 1857.29931219\n",
      "Iteration 3386, loss = 1857.19781839\n",
      "Iteration 3387, loss = 1857.08945278\n",
      "Iteration 3388, loss = 1856.96597080\n",
      "Iteration 3389, loss = 1856.85387665\n",
      "Iteration 3390, loss = 1856.74617537\n",
      "Iteration 3391, loss = 1856.73508012\n",
      "Iteration 3392, loss = 1856.63369588\n",
      "Iteration 3393, loss = 1856.43814457\n",
      "Iteration 3394, loss = 1856.30065798\n",
      "Iteration 3395, loss = 1856.20597970\n",
      "Iteration 3396, loss = 1856.15116038\n",
      "Iteration 3397, loss = 1855.94324982\n",
      "Iteration 3398, loss = 1855.85138962\n",
      "Iteration 3399, loss = 1855.81656012\n",
      "Iteration 3400, loss = 1855.67447428\n",
      "Iteration 3401, loss = 1855.65866523\n",
      "Iteration 3402, loss = 1855.52972786\n",
      "Iteration 3403, loss = 1855.33719799\n",
      "Iteration 3404, loss = 1855.19752944\n",
      "Iteration 3405, loss = 1855.11017683\n",
      "Iteration 3406, loss = 1854.96938027\n",
      "Iteration 3407, loss = 1854.91825021\n",
      "Iteration 3408, loss = 1854.76837139\n",
      "Iteration 3409, loss = 1854.68642619\n",
      "Iteration 3410, loss = 1854.53884923\n",
      "Iteration 3411, loss = 1854.45164648\n",
      "Iteration 3412, loss = 1854.34923227\n",
      "Iteration 3413, loss = 1854.22604159\n",
      "Iteration 3414, loss = 1854.11052503\n",
      "Iteration 3415, loss = 1854.00622938\n",
      "Iteration 3416, loss = 1853.91009414\n",
      "Iteration 3417, loss = 1853.87581521\n",
      "Iteration 3418, loss = 1853.72945661\n",
      "Iteration 3419, loss = 1853.54074262\n",
      "Iteration 3420, loss = 1853.46923659\n",
      "Iteration 3421, loss = 1853.43804889\n",
      "Iteration 3422, loss = 1853.22176355\n",
      "Iteration 3423, loss = 1853.19292765\n",
      "Iteration 3424, loss = 1853.07974992\n",
      "Iteration 3425, loss = 1853.02402211\n",
      "Iteration 3426, loss = 1852.83639690\n",
      "Iteration 3427, loss = 1852.82517568\n",
      "Iteration 3428, loss = 1852.67540050\n",
      "Iteration 3429, loss = 1852.42448344\n",
      "Iteration 3430, loss = 1852.35309270\n",
      "Iteration 3431, loss = 1852.23232322\n",
      "Iteration 3432, loss = 1852.10789733\n",
      "Iteration 3433, loss = 1851.99350024\n",
      "Iteration 3434, loss = 1851.90155714\n",
      "Iteration 3435, loss = 1851.78571528\n",
      "Iteration 3436, loss = 1851.67494408\n",
      "Iteration 3437, loss = 1851.68714627\n",
      "Iteration 3438, loss = 1851.47443582\n",
      "Iteration 3439, loss = 1851.31389542\n",
      "Iteration 3440, loss = 1851.19654278\n",
      "Iteration 3441, loss = 1851.10981187\n",
      "Iteration 3442, loss = 1851.00749494\n",
      "Iteration 3443, loss = 1851.05525736\n",
      "Iteration 3444, loss = 1850.76843486\n",
      "Iteration 3445, loss = 1850.66425136\n",
      "Iteration 3446, loss = 1850.54385290\n",
      "Iteration 3447, loss = 1850.53306286\n",
      "Iteration 3448, loss = 1850.40350906\n",
      "Iteration 3449, loss = 1850.23138506\n",
      "Iteration 3450, loss = 1850.11727945\n",
      "Iteration 3451, loss = 1850.06552911\n",
      "Iteration 3452, loss = 1849.88810007\n",
      "Iteration 3453, loss = 1849.82055469\n",
      "Iteration 3454, loss = 1849.64045592\n",
      "Iteration 3455, loss = 1849.57742886\n",
      "Iteration 3456, loss = 1849.45794625\n",
      "Iteration 3457, loss = 1849.29408159\n",
      "Iteration 3458, loss = 1849.22871431\n",
      "Iteration 3459, loss = 1849.10511074\n",
      "Iteration 3460, loss = 1849.00140651\n",
      "Iteration 3461, loss = 1848.83947329\n",
      "Iteration 3462, loss = 1848.81012214\n",
      "Iteration 3463, loss = 1848.63828521\n",
      "Iteration 3464, loss = 1848.69510651\n",
      "Iteration 3465, loss = 1848.46573106\n",
      "Iteration 3466, loss = 1848.28649977\n",
      "Iteration 3467, loss = 1848.24557694\n",
      "Iteration 3468, loss = 1848.07141809\n",
      "Iteration 3469, loss = 1848.05062961\n",
      "Iteration 3470, loss = 1847.85752052\n",
      "Iteration 3471, loss = 1847.72451008\n",
      "Iteration 3472, loss = 1847.67328471\n",
      "Iteration 3473, loss = 1847.54591314\n",
      "Iteration 3474, loss = 1847.45858085\n",
      "Iteration 3475, loss = 1847.26509030\n",
      "Iteration 3476, loss = 1847.14909863\n",
      "Iteration 3477, loss = 1847.05618828\n",
      "Iteration 3478, loss = 1847.09608130\n",
      "Iteration 3479, loss = 1846.77724447\n",
      "Iteration 3480, loss = 1846.76374343\n",
      "Iteration 3481, loss = 1846.55280355\n",
      "Iteration 3482, loss = 1846.59722628\n",
      "Iteration 3483, loss = 1846.38134187\n",
      "Iteration 3484, loss = 1846.20603786\n",
      "Iteration 3485, loss = 1846.10433772\n",
      "Iteration 3486, loss = 1846.01082862\n",
      "Iteration 3487, loss = 1845.93130305\n",
      "Iteration 3488, loss = 1845.84601920\n",
      "Iteration 3489, loss = 1845.68607715\n",
      "Iteration 3490, loss = 1845.69717413\n",
      "Iteration 3491, loss = 1845.49669652\n",
      "Iteration 3492, loss = 1845.56573730\n",
      "Iteration 3493, loss = 1845.23307977\n",
      "Iteration 3494, loss = 1845.09163978\n",
      "Iteration 3495, loss = 1845.02614452\n",
      "Iteration 3496, loss = 1844.84289135\n",
      "Iteration 3497, loss = 1844.86131247\n",
      "Iteration 3498, loss = 1844.64605137\n",
      "Iteration 3499, loss = 1844.51440912\n",
      "Iteration 3500, loss = 1844.38390720\n",
      "Iteration 3501, loss = 1844.34765394\n",
      "Iteration 3502, loss = 1844.22273736\n",
      "Iteration 3503, loss = 1844.23378419\n",
      "Iteration 3504, loss = 1843.93917681\n",
      "Iteration 3505, loss = 1843.89825682\n",
      "Iteration 3506, loss = 1843.68769991\n",
      "Iteration 3507, loss = 1843.83602825\n",
      "Iteration 3508, loss = 1843.49834003\n",
      "Iteration 3509, loss = 1843.33281602\n",
      "Iteration 3510, loss = 1843.30087268\n",
      "Iteration 3511, loss = 1843.14008373\n",
      "Iteration 3512, loss = 1842.99119038\n",
      "Iteration 3513, loss = 1842.99161154\n",
      "Iteration 3514, loss = 1842.79248891\n",
      "Iteration 3515, loss = 1842.71480755\n",
      "Iteration 3516, loss = 1842.56187508\n",
      "Iteration 3517, loss = 1842.42811572\n",
      "Iteration 3518, loss = 1842.28445696\n",
      "Iteration 3519, loss = 1842.21195947\n",
      "Iteration 3520, loss = 1842.05229118\n",
      "Iteration 3521, loss = 1841.94174343\n",
      "Iteration 3522, loss = 1841.79886506\n",
      "Iteration 3523, loss = 1841.70795997\n",
      "Iteration 3524, loss = 1841.57933044\n",
      "Iteration 3525, loss = 1841.43294797\n",
      "Iteration 3526, loss = 1841.40172525\n",
      "Iteration 3527, loss = 1841.21110659\n",
      "Iteration 3528, loss = 1841.10291103\n",
      "Iteration 3529, loss = 1840.98824801\n",
      "Iteration 3530, loss = 1840.84789340\n",
      "Iteration 3531, loss = 1840.75150241\n",
      "Iteration 3532, loss = 1840.67467050\n",
      "Iteration 3533, loss = 1840.70942525\n",
      "Iteration 3534, loss = 1840.41856948\n",
      "Iteration 3535, loss = 1840.32705539\n",
      "Iteration 3536, loss = 1840.30369558\n",
      "Iteration 3537, loss = 1840.15111206\n",
      "Iteration 3538, loss = 1839.93144513\n",
      "Iteration 3539, loss = 1839.79200255\n",
      "Iteration 3540, loss = 1839.68756990\n",
      "Iteration 3541, loss = 1839.65395606\n",
      "Iteration 3542, loss = 1839.50616706\n",
      "Iteration 3543, loss = 1839.60662303\n",
      "Iteration 3544, loss = 1839.13392290\n",
      "Iteration 3545, loss = 1839.17360866\n",
      "Iteration 3546, loss = 1838.89114899\n",
      "Iteration 3547, loss = 1838.76602037\n",
      "Iteration 3548, loss = 1839.06100883\n",
      "Iteration 3549, loss = 1838.49736186\n",
      "Iteration 3550, loss = 1838.45506265\n",
      "Iteration 3551, loss = 1838.29892913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3552, loss = 1838.16531363\n",
      "Iteration 3553, loss = 1838.02904925\n",
      "Iteration 3554, loss = 1837.91474118\n",
      "Iteration 3555, loss = 1837.82461477\n",
      "Iteration 3556, loss = 1837.71949284\n",
      "Iteration 3557, loss = 1837.55948967\n",
      "Iteration 3558, loss = 1837.50376697\n",
      "Iteration 3559, loss = 1837.27983148\n",
      "Iteration 3560, loss = 1837.15889100\n",
      "Iteration 3561, loss = 1837.01036537\n",
      "Iteration 3562, loss = 1837.01805656\n",
      "Iteration 3563, loss = 1836.79653579\n",
      "Iteration 3564, loss = 1836.66171533\n",
      "Iteration 3565, loss = 1836.54584563\n",
      "Iteration 3566, loss = 1836.36042926\n",
      "Iteration 3567, loss = 1836.23280782\n",
      "Iteration 3568, loss = 1836.16433258\n",
      "Iteration 3569, loss = 1835.97190573\n",
      "Iteration 3570, loss = 1835.84135869\n",
      "Iteration 3571, loss = 1835.71781721\n",
      "Iteration 3572, loss = 1835.59387941\n",
      "Iteration 3573, loss = 1835.43759277\n",
      "Iteration 3574, loss = 1835.33690968\n",
      "Iteration 3575, loss = 1835.26988241\n",
      "Iteration 3576, loss = 1835.01637989\n",
      "Iteration 3577, loss = 1834.98634341\n",
      "Iteration 3578, loss = 1834.82554703\n",
      "Iteration 3579, loss = 1834.72964598\n",
      "Iteration 3580, loss = 1834.54328462\n",
      "Iteration 3581, loss = 1834.39440215\n",
      "Iteration 3582, loss = 1834.29115605\n",
      "Iteration 3583, loss = 1834.08499657\n",
      "Iteration 3584, loss = 1834.01879913\n",
      "Iteration 3585, loss = 1834.03382002\n",
      "Iteration 3586, loss = 1833.70086492\n",
      "Iteration 3587, loss = 1833.59776340\n",
      "Iteration 3588, loss = 1833.42924188\n",
      "Iteration 3589, loss = 1833.19188934\n",
      "Iteration 3590, loss = 1833.24412348\n",
      "Iteration 3591, loss = 1832.94504081\n",
      "Iteration 3592, loss = 1832.88377972\n",
      "Iteration 3593, loss = 1832.64050504\n",
      "Iteration 3594, loss = 1832.53752521\n",
      "Iteration 3595, loss = 1832.29165982\n",
      "Iteration 3596, loss = 1832.15669456\n",
      "Iteration 3597, loss = 1831.99622576\n",
      "Iteration 3598, loss = 1831.82309755\n",
      "Iteration 3599, loss = 1831.74043928\n",
      "Iteration 3600, loss = 1831.49946875\n",
      "Iteration 3601, loss = 1831.34109053\n",
      "Iteration 3602, loss = 1831.19781346\n",
      "Iteration 3603, loss = 1831.01887336\n",
      "Iteration 3604, loss = 1830.88467048\n",
      "Iteration 3605, loss = 1830.76401487\n",
      "Iteration 3606, loss = 1830.71697602\n",
      "Iteration 3607, loss = 1830.31102838\n",
      "Iteration 3608, loss = 1830.12061597\n",
      "Iteration 3609, loss = 1829.98140075\n",
      "Iteration 3610, loss = 1829.69427337\n",
      "Iteration 3611, loss = 1829.49145711\n",
      "Iteration 3612, loss = 1829.25985069\n",
      "Iteration 3613, loss = 1829.15659821\n",
      "Iteration 3614, loss = 1828.90548965\n",
      "Iteration 3615, loss = 1828.75794372\n",
      "Iteration 3616, loss = 1828.44717317\n",
      "Iteration 3617, loss = 1828.13754265\n",
      "Iteration 3618, loss = 1827.87850302\n",
      "Iteration 3619, loss = 1827.58928945\n",
      "Iteration 3620, loss = 1827.30182125\n",
      "Iteration 3621, loss = 1827.06857846\n",
      "Iteration 3622, loss = 1826.74003184\n",
      "Iteration 3623, loss = 1826.37772720\n",
      "Iteration 3624, loss = 1826.07452079\n",
      "Iteration 3625, loss = 1825.68384625\n",
      "Iteration 3626, loss = 1825.30784171\n",
      "Iteration 3627, loss = 1824.85054367\n",
      "Iteration 3628, loss = 1824.71689120\n",
      "Iteration 3629, loss = 1823.90900160\n",
      "Iteration 3630, loss = 1823.38237044\n",
      "Iteration 3631, loss = 1822.84313001\n",
      "Iteration 3632, loss = 1822.34404159\n",
      "Iteration 3633, loss = 1821.72231200\n",
      "Iteration 3634, loss = 1820.88448981\n",
      "Iteration 3635, loss = 1820.13881905\n",
      "Iteration 3636, loss = 1819.34098863\n",
      "Iteration 3637, loss = 1818.46493768\n",
      "Iteration 3638, loss = 1817.57979032\n",
      "Iteration 3639, loss = 1816.59832478\n",
      "Iteration 3640, loss = 1815.52006775\n",
      "Iteration 3641, loss = 1814.47796707\n",
      "Iteration 3642, loss = 1813.37983238\n",
      "Iteration 3643, loss = 1812.21261164\n",
      "Iteration 3644, loss = 1811.10678364\n",
      "Iteration 3645, loss = 1809.85033255\n",
      "Iteration 3646, loss = 1808.81887868\n",
      "Iteration 3647, loss = 1807.52016739\n",
      "Iteration 3648, loss = 1806.29623043\n",
      "Iteration 3649, loss = 1805.12912689\n",
      "Iteration 3650, loss = 1804.00325449\n",
      "Iteration 3651, loss = 1802.83206941\n",
      "Iteration 3652, loss = 1801.71377463\n",
      "Iteration 3653, loss = 1800.69223989\n",
      "Iteration 3654, loss = 1799.46556930\n",
      "Iteration 3655, loss = 1798.37921027\n",
      "Iteration 3656, loss = 1797.33489416\n",
      "Iteration 3657, loss = 1796.37386561\n",
      "Iteration 3658, loss = 1795.62658348\n",
      "Iteration 3659, loss = 1794.34885632\n",
      "Iteration 3660, loss = 1793.36488298\n",
      "Iteration 3661, loss = 1792.37742225\n",
      "Iteration 3662, loss = 1791.51476748\n",
      "Iteration 3663, loss = 1790.52187221\n",
      "Iteration 3664, loss = 1789.64978901\n",
      "Iteration 3665, loss = 1788.68972878\n",
      "Iteration 3666, loss = 1787.80292345\n",
      "Iteration 3667, loss = 1787.04569202\n",
      "Iteration 3668, loss = 1786.08245596\n",
      "Iteration 3669, loss = 1785.26989682\n",
      "Iteration 3670, loss = 1784.47290560\n",
      "Iteration 3671, loss = 1783.57901568\n",
      "Iteration 3672, loss = 1782.78500526\n",
      "Iteration 3673, loss = 1781.96481212\n",
      "Iteration 3674, loss = 1781.20170776\n",
      "Iteration 3675, loss = 1780.37893625\n",
      "Iteration 3676, loss = 1779.80249457\n",
      "Iteration 3677, loss = 1778.81225462\n",
      "Iteration 3678, loss = 1778.07453993\n",
      "Iteration 3679, loss = 1777.38081183\n",
      "Iteration 3680, loss = 1776.61231297\n",
      "Iteration 3681, loss = 1775.87984503\n",
      "Iteration 3682, loss = 1775.14562459\n",
      "Iteration 3683, loss = 1774.36764396\n",
      "Iteration 3684, loss = 1773.65359387\n",
      "Iteration 3685, loss = 1772.93401288\n",
      "Iteration 3686, loss = 1772.31304006\n",
      "Iteration 3687, loss = 1771.53706648\n",
      "Iteration 3688, loss = 1770.85314051\n",
      "Iteration 3689, loss = 1770.15435312\n",
      "Iteration 3690, loss = 1769.55422681\n",
      "Iteration 3691, loss = 1768.94536344\n",
      "Iteration 3692, loss = 1768.09474715\n",
      "Iteration 3693, loss = 1767.41713008\n",
      "Iteration 3694, loss = 1766.82471233\n",
      "Iteration 3695, loss = 1766.07907059\n",
      "Iteration 3696, loss = 1765.56177577\n",
      "Iteration 3697, loss = 1764.82154735\n",
      "Iteration 3698, loss = 1764.23721156\n",
      "Iteration 3699, loss = 1763.51482653\n",
      "Iteration 3700, loss = 1762.84616559\n",
      "Iteration 3701, loss = 1762.20257742\n",
      "Iteration 3702, loss = 1761.70669585\n",
      "Iteration 3703, loss = 1760.98178329\n",
      "Iteration 3704, loss = 1760.45425955\n",
      "Iteration 3705, loss = 1759.79833404\n",
      "Iteration 3706, loss = 1759.11296971\n",
      "Iteration 3707, loss = 1758.55747860\n",
      "Iteration 3708, loss = 1757.89287843\n",
      "Iteration 3709, loss = 1757.48679613\n",
      "Iteration 3710, loss = 1756.67226496\n",
      "Iteration 3711, loss = 1756.11386499\n",
      "Iteration 3712, loss = 1755.45375743\n",
      "Iteration 3713, loss = 1754.90923543\n",
      "Iteration 3714, loss = 1754.33297365\n",
      "Iteration 3715, loss = 1753.77616908\n",
      "Iteration 3716, loss = 1753.09974406\n",
      "Iteration 3717, loss = 1752.54716660\n",
      "Iteration 3718, loss = 1751.96547593\n",
      "Iteration 3719, loss = 1751.33562904\n",
      "Iteration 3720, loss = 1750.78476464\n",
      "Iteration 3721, loss = 1750.21599643\n",
      "Iteration 3722, loss = 1749.66120888\n",
      "Iteration 3723, loss = 1749.11779212\n",
      "Iteration 3724, loss = 1748.68680990\n",
      "Iteration 3725, loss = 1747.94738394\n",
      "Iteration 3726, loss = 1747.41036821\n",
      "Iteration 3727, loss = 1746.85165852\n",
      "Iteration 3728, loss = 1746.31767456\n",
      "Iteration 3729, loss = 1745.79983107\n",
      "Iteration 3730, loss = 1745.19650747\n",
      "Iteration 3731, loss = 1744.70923934\n",
      "Iteration 3732, loss = 1744.15355047\n",
      "Iteration 3733, loss = 1743.58731972\n",
      "Iteration 3734, loss = 1743.04132704\n",
      "Iteration 3735, loss = 1742.58268891\n",
      "Iteration 3736, loss = 1742.09085741\n",
      "Iteration 3737, loss = 1741.49387192\n",
      "Iteration 3738, loss = 1740.94550006\n",
      "Iteration 3739, loss = 1740.41228548\n",
      "Iteration 3740, loss = 1739.86995307\n",
      "Iteration 3741, loss = 1739.39419164\n",
      "Iteration 3742, loss = 1738.85480941\n",
      "Iteration 3743, loss = 1738.30296147\n",
      "Iteration 3744, loss = 1737.84314715\n",
      "Iteration 3745, loss = 1737.39432969\n",
      "Iteration 3746, loss = 1736.83206757\n",
      "Iteration 3747, loss = 1736.30719081\n",
      "Iteration 3748, loss = 1735.87109460\n",
      "Iteration 3749, loss = 1735.29417087\n",
      "Iteration 3750, loss = 1734.83951209\n",
      "Iteration 3751, loss = 1734.24952752\n",
      "Iteration 3752, loss = 1733.82709230\n",
      "Iteration 3753, loss = 1733.33601395\n",
      "Iteration 3754, loss = 1732.85663895\n",
      "Iteration 3755, loss = 1732.34338222\n",
      "Iteration 3756, loss = 1731.82380889\n",
      "Iteration 3757, loss = 1731.30358337\n",
      "Iteration 3758, loss = 1730.84885735\n",
      "Iteration 3759, loss = 1730.63869245\n",
      "Iteration 3760, loss = 1729.87338004\n",
      "Iteration 3761, loss = 1729.36990733\n",
      "Iteration 3762, loss = 1728.90771745\n",
      "Iteration 3763, loss = 1728.42215945\n",
      "Iteration 3764, loss = 1727.91705201\n",
      "Iteration 3765, loss = 1727.45102593\n",
      "Iteration 3766, loss = 1726.96194486\n",
      "Iteration 3767, loss = 1726.87339557\n",
      "Iteration 3768, loss = 1726.02798032\n",
      "Iteration 3769, loss = 1725.60744499\n",
      "Iteration 3770, loss = 1725.11886953\n",
      "Iteration 3771, loss = 1724.63517167\n",
      "Iteration 3772, loss = 1724.21088285\n",
      "Iteration 3773, loss = 1723.78237688\n",
      "Iteration 3774, loss = 1723.24827061\n",
      "Iteration 3775, loss = 1722.78944276\n",
      "Iteration 3776, loss = 1722.33478794\n",
      "Iteration 3777, loss = 1721.85608225\n",
      "Iteration 3778, loss = 1721.40198860\n",
      "Iteration 3779, loss = 1720.97812830\n",
      "Iteration 3780, loss = 1720.56707546\n",
      "Iteration 3781, loss = 1720.09110182\n",
      "Iteration 3782, loss = 1719.63041016\n",
      "Iteration 3783, loss = 1719.12766212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3784, loss = 1718.77811019\n",
      "Iteration 3785, loss = 1718.21945928\n",
      "Iteration 3786, loss = 1717.80385684\n",
      "Iteration 3787, loss = 1717.36589285\n",
      "Iteration 3788, loss = 1716.92524670\n",
      "Iteration 3789, loss = 1716.45135996\n",
      "Iteration 3790, loss = 1716.19979610\n",
      "Iteration 3791, loss = 1715.57019842\n",
      "Iteration 3792, loss = 1715.14052197\n",
      "Iteration 3793, loss = 1714.70743121\n",
      "Iteration 3794, loss = 1714.32287586\n",
      "Iteration 3795, loss = 1713.89732227\n",
      "Iteration 3796, loss = 1713.39630504\n",
      "Iteration 3797, loss = 1712.95665851\n",
      "Iteration 3798, loss = 1712.57021571\n",
      "Iteration 3799, loss = 1712.09080104\n",
      "Iteration 3800, loss = 1711.72067093\n",
      "Iteration 3801, loss = 1711.27251798\n",
      "Iteration 3802, loss = 1710.84908180\n",
      "Iteration 3803, loss = 1710.39809981\n",
      "Iteration 3804, loss = 1709.96276636\n",
      "Iteration 3805, loss = 1709.65260946\n",
      "Iteration 3806, loss = 1709.19690453\n",
      "Iteration 3807, loss = 1708.70382845\n",
      "Iteration 3808, loss = 1708.30500572\n",
      "Iteration 3809, loss = 1707.96440411\n",
      "Iteration 3810, loss = 1707.54824563\n",
      "Iteration 3811, loss = 1707.05640698\n",
      "Iteration 3812, loss = 1706.59523498\n",
      "Iteration 3813, loss = 1706.17551717\n",
      "Iteration 3814, loss = 1705.80713599\n",
      "Iteration 3815, loss = 1705.36575138\n",
      "Iteration 3816, loss = 1704.96822463\n",
      "Iteration 3817, loss = 1704.52810741\n",
      "Iteration 3818, loss = 1704.27829421\n",
      "Iteration 3819, loss = 1703.70783050\n",
      "Iteration 3820, loss = 1703.33756404\n",
      "Iteration 3821, loss = 1702.95664890\n",
      "Iteration 3822, loss = 1702.52423814\n",
      "Iteration 3823, loss = 1702.08696538\n",
      "Iteration 3824, loss = 1701.65240086\n",
      "Iteration 3825, loss = 1701.40220328\n",
      "Iteration 3826, loss = 1701.06747019\n",
      "Iteration 3827, loss = 1700.85829950\n",
      "Iteration 3828, loss = 1700.04547031\n",
      "Iteration 3829, loss = 1699.73768369\n",
      "Iteration 3830, loss = 1699.27888832\n",
      "Iteration 3831, loss = 1698.85686468\n",
      "Iteration 3832, loss = 1698.55782847\n",
      "Iteration 3833, loss = 1698.03728684\n",
      "Iteration 3834, loss = 1697.64189767\n",
      "Iteration 3835, loss = 1697.34462125\n",
      "Iteration 3836, loss = 1696.90527337\n",
      "Iteration 3837, loss = 1696.47311007\n",
      "Iteration 3838, loss = 1696.15814770\n",
      "Iteration 3839, loss = 1696.15479506\n",
      "Iteration 3840, loss = 1695.29169401\n",
      "Iteration 3841, loss = 1694.93323239\n",
      "Iteration 3842, loss = 1694.50579172\n",
      "Iteration 3843, loss = 1694.22823666\n",
      "Iteration 3844, loss = 1693.75229234\n",
      "Iteration 3845, loss = 1693.35968460\n",
      "Iteration 3846, loss = 1692.92575975\n",
      "Iteration 3847, loss = 1692.58415947\n",
      "Iteration 3848, loss = 1692.20766024\n",
      "Iteration 3849, loss = 1691.77462441\n",
      "Iteration 3850, loss = 1691.43460541\n",
      "Iteration 3851, loss = 1691.01629096\n",
      "Iteration 3852, loss = 1690.69284446\n",
      "Iteration 3853, loss = 1690.30108071\n",
      "Iteration 3854, loss = 1689.86965163\n",
      "Iteration 3855, loss = 1689.47225080\n",
      "Iteration 3856, loss = 1689.16823890\n",
      "Iteration 3857, loss = 1688.84222791\n",
      "Iteration 3858, loss = 1688.35780781\n",
      "Iteration 3859, loss = 1687.96620878\n",
      "Iteration 3860, loss = 1687.63655367\n",
      "Iteration 3861, loss = 1687.20333674\n",
      "Iteration 3862, loss = 1686.85446635\n",
      "Iteration 3863, loss = 1686.48533734\n",
      "Iteration 3864, loss = 1686.12171168\n",
      "Iteration 3865, loss = 1685.70578113\n",
      "Iteration 3866, loss = 1685.39001612\n",
      "Iteration 3867, loss = 1684.99120378\n",
      "Iteration 3868, loss = 1684.58336304\n",
      "Iteration 3869, loss = 1684.24802553\n",
      "Iteration 3870, loss = 1683.89222063\n",
      "Iteration 3871, loss = 1683.48755893\n",
      "Iteration 3872, loss = 1683.18255451\n",
      "Iteration 3873, loss = 1682.78369594\n",
      "Iteration 3874, loss = 1682.46091338\n",
      "Iteration 3875, loss = 1681.99293423\n",
      "Iteration 3876, loss = 1681.64945423\n",
      "Iteration 3877, loss = 1681.32044067\n",
      "Iteration 3878, loss = 1680.92909044\n",
      "Iteration 3879, loss = 1680.51254940\n",
      "Iteration 3880, loss = 1680.13896629\n",
      "Iteration 3881, loss = 1679.78342452\n",
      "Iteration 3882, loss = 1679.41342085\n",
      "Iteration 3883, loss = 1679.03751227\n",
      "Iteration 3884, loss = 1678.69235459\n",
      "Iteration 3885, loss = 1678.62484739\n",
      "Iteration 3886, loss = 1677.93743346\n",
      "Iteration 3887, loss = 1677.71962532\n",
      "Iteration 3888, loss = 1677.24359730\n",
      "Iteration 3889, loss = 1676.94101141\n",
      "Iteration 3890, loss = 1676.51648484\n",
      "Iteration 3891, loss = 1676.11535661\n",
      "Iteration 3892, loss = 1675.80054091\n",
      "Iteration 3893, loss = 1675.49955492\n",
      "Iteration 3894, loss = 1675.06110885\n",
      "Iteration 3895, loss = 1674.68510290\n",
      "Iteration 3896, loss = 1674.33715058\n",
      "Iteration 3897, loss = 1674.08131924\n",
      "Iteration 3898, loss = 1673.62632430\n",
      "Iteration 3899, loss = 1673.27072794\n",
      "Iteration 3900, loss = 1673.05438983\n",
      "Iteration 3901, loss = 1672.59224926\n",
      "Iteration 3902, loss = 1672.21749607\n",
      "Iteration 3903, loss = 1671.88957680\n",
      "Iteration 3904, loss = 1671.52586116\n",
      "Iteration 3905, loss = 1671.15727706\n",
      "Iteration 3906, loss = 1670.80218520\n",
      "Iteration 3907, loss = 1670.42881068\n",
      "Iteration 3908, loss = 1670.13199363\n",
      "Iteration 3909, loss = 1669.77705882\n",
      "Iteration 3910, loss = 1669.43524693\n",
      "Iteration 3911, loss = 1669.01283634\n",
      "Iteration 3912, loss = 1668.65748247\n",
      "Iteration 3913, loss = 1668.46264934\n",
      "Iteration 3914, loss = 1668.01202368\n",
      "Iteration 3915, loss = 1667.60291675\n",
      "Iteration 3916, loss = 1667.37565946\n",
      "Iteration 3917, loss = 1666.99193161\n",
      "Iteration 3918, loss = 1666.58117893\n",
      "Iteration 3919, loss = 1666.22137616\n",
      "Iteration 3920, loss = 1665.86702057\n",
      "Iteration 3921, loss = 1665.56326544\n",
      "Iteration 3922, loss = 1665.15349338\n",
      "Iteration 3923, loss = 1664.83394919\n",
      "Iteration 3924, loss = 1664.47265372\n",
      "Iteration 3925, loss = 1664.23637530\n",
      "Iteration 3926, loss = 1663.79318696\n",
      "Iteration 3927, loss = 1663.46188065\n",
      "Iteration 3928, loss = 1663.09612580\n",
      "Iteration 3929, loss = 1662.74553445\n",
      "Iteration 3930, loss = 1662.50298474\n",
      "Iteration 3931, loss = 1662.05610872\n",
      "Iteration 3932, loss = 1661.73030263\n",
      "Iteration 3933, loss = 1661.39714967\n",
      "Iteration 3934, loss = 1661.04414693\n",
      "Iteration 3935, loss = 1660.72824889\n",
      "Iteration 3936, loss = 1660.44463788\n",
      "Iteration 3937, loss = 1660.01236007\n",
      "Iteration 3938, loss = 1659.92162931\n",
      "Iteration 3939, loss = 1659.39107502\n",
      "Iteration 3940, loss = 1659.07710529\n",
      "Iteration 3941, loss = 1658.64929649\n",
      "Iteration 3942, loss = 1658.36985177\n",
      "Iteration 3943, loss = 1657.98353034\n",
      "Iteration 3944, loss = 1657.63902063\n",
      "Iteration 3945, loss = 1657.49635993\n",
      "Iteration 3946, loss = 1656.97153511\n",
      "Iteration 3947, loss = 1656.67442227\n",
      "Iteration 3948, loss = 1656.28083554\n",
      "Iteration 3949, loss = 1655.97822477\n",
      "Iteration 3950, loss = 1655.62317373\n",
      "Iteration 3951, loss = 1655.30771696\n",
      "Iteration 3952, loss = 1654.95618312\n",
      "Iteration 3953, loss = 1654.58401957\n",
      "Iteration 3954, loss = 1654.33899635\n",
      "Iteration 3955, loss = 1653.96705882\n",
      "Iteration 3956, loss = 1653.73892774\n",
      "Iteration 3957, loss = 1653.22979916\n",
      "Iteration 3958, loss = 1652.95925556\n",
      "Iteration 3959, loss = 1652.73203417\n",
      "Iteration 3960, loss = 1652.25541126\n",
      "Iteration 3961, loss = 1651.96378873\n",
      "Iteration 3962, loss = 1651.62834929\n",
      "Iteration 3963, loss = 1651.29994022\n",
      "Iteration 3964, loss = 1651.01979686\n",
      "Iteration 3965, loss = 1650.59769274\n",
      "Iteration 3966, loss = 1650.25732208\n",
      "Iteration 3967, loss = 1649.96531823\n",
      "Iteration 3968, loss = 1649.63409143\n",
      "Iteration 3969, loss = 1649.33205406\n",
      "Iteration 3970, loss = 1648.92629185\n",
      "Iteration 3971, loss = 1648.64844418\n",
      "Iteration 3972, loss = 1648.50527469\n",
      "Iteration 3973, loss = 1647.96958760\n",
      "Iteration 3974, loss = 1647.61674960\n",
      "Iteration 3975, loss = 1647.30309366\n",
      "Iteration 3976, loss = 1646.93379127\n",
      "Iteration 3977, loss = 1646.61493489\n",
      "Iteration 3978, loss = 1646.30621515\n",
      "Iteration 3979, loss = 1646.03360234\n",
      "Iteration 3980, loss = 1645.63165060\n",
      "Iteration 3981, loss = 1645.38002257\n",
      "Iteration 3982, loss = 1645.09910022\n",
      "Iteration 3983, loss = 1644.66015771\n",
      "Iteration 3984, loss = 1644.34618576\n",
      "Iteration 3985, loss = 1644.03593241\n",
      "Iteration 3986, loss = 1643.68801928\n",
      "Iteration 3987, loss = 1643.37486333\n",
      "Iteration 3988, loss = 1643.04581547\n",
      "Iteration 3989, loss = 1642.72915683\n",
      "Iteration 3990, loss = 1642.44407131\n",
      "Iteration 3991, loss = 1642.04277420\n",
      "Iteration 3992, loss = 1641.75408706\n",
      "Iteration 3993, loss = 1641.45569817\n",
      "Iteration 3994, loss = 1641.10171067\n",
      "Iteration 3995, loss = 1640.74780884\n",
      "Iteration 3996, loss = 1640.53311853\n",
      "Iteration 3997, loss = 1640.11745987\n",
      "Iteration 3998, loss = 1639.78978546\n",
      "Iteration 3999, loss = 1639.51896944\n",
      "Iteration 4000, loss = 1639.13394770\n",
      "Iteration 4001, loss = 1638.81334701\n",
      "Iteration 4002, loss = 1638.47115393\n",
      "Iteration 4003, loss = 1638.20903357\n",
      "Iteration 4004, loss = 1637.92045125\n",
      "Iteration 4005, loss = 1637.55260638\n",
      "Iteration 4006, loss = 1637.23503778\n",
      "Iteration 4007, loss = 1636.97479969\n",
      "Iteration 4008, loss = 1636.58755722\n",
      "Iteration 4009, loss = 1636.19841070\n",
      "Iteration 4010, loss = 1636.03410686\n",
      "Iteration 4011, loss = 1635.61814100\n",
      "Iteration 4012, loss = 1635.28399953\n",
      "Iteration 4013, loss = 1635.02051437\n",
      "Iteration 4014, loss = 1634.62682081\n",
      "Iteration 4015, loss = 1634.51210713\n",
      "Iteration 4016, loss = 1633.99368948\n",
      "Iteration 4017, loss = 1633.79138168\n",
      "Iteration 4018, loss = 1633.41806768\n",
      "Iteration 4019, loss = 1633.17113018\n",
      "Iteration 4020, loss = 1632.71397941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4021, loss = 1632.41239486\n",
      "Iteration 4022, loss = 1632.09427468\n",
      "Iteration 4023, loss = 1631.73863675\n",
      "Iteration 4024, loss = 1631.43509325\n",
      "Iteration 4025, loss = 1631.09307480\n",
      "Iteration 4026, loss = 1630.76570544\n",
      "Iteration 4027, loss = 1630.44943444\n",
      "Iteration 4028, loss = 1630.21538091\n",
      "Iteration 4029, loss = 1629.91618135\n",
      "Iteration 4030, loss = 1629.50264083\n",
      "Iteration 4031, loss = 1629.16911449\n",
      "Iteration 4032, loss = 1628.81539967\n",
      "Iteration 4033, loss = 1628.60366856\n",
      "Iteration 4034, loss = 1628.26423260\n",
      "Iteration 4035, loss = 1628.00802219\n",
      "Iteration 4036, loss = 1627.55723226\n",
      "Iteration 4037, loss = 1627.39726127\n",
      "Iteration 4038, loss = 1626.95490123\n",
      "Iteration 4039, loss = 1626.59556928\n",
      "Iteration 4040, loss = 1626.28854907\n",
      "Iteration 4041, loss = 1625.99895878\n",
      "Iteration 4042, loss = 1625.70167596\n",
      "Iteration 4043, loss = 1625.34659789\n",
      "Iteration 4044, loss = 1625.05957101\n",
      "Iteration 4045, loss = 1624.74172082\n",
      "Iteration 4046, loss = 1624.35030067\n",
      "Iteration 4047, loss = 1624.07791160\n",
      "Iteration 4048, loss = 1623.74914417\n",
      "Iteration 4049, loss = 1623.45717722\n",
      "Iteration 4050, loss = 1623.14272982\n",
      "Iteration 4051, loss = 1622.81229960\n",
      "Iteration 4052, loss = 1622.59703482\n",
      "Iteration 4053, loss = 1622.13466054\n",
      "Iteration 4054, loss = 1621.81177260\n",
      "Iteration 4055, loss = 1621.59366325\n",
      "Iteration 4056, loss = 1621.42133459\n",
      "Iteration 4057, loss = 1620.89832836\n",
      "Iteration 4058, loss = 1620.69085524\n",
      "Iteration 4059, loss = 1620.33290777\n",
      "Iteration 4060, loss = 1619.90831988\n",
      "Iteration 4061, loss = 1619.68712154\n",
      "Iteration 4062, loss = 1619.24184878\n",
      "Iteration 4063, loss = 1618.93333088\n",
      "Iteration 4064, loss = 1618.74285819\n",
      "Iteration 4065, loss = 1618.63225533\n",
      "Iteration 4066, loss = 1617.95493068\n",
      "Iteration 4067, loss = 1617.77991734\n",
      "Iteration 4068, loss = 1617.39198657\n",
      "Iteration 4069, loss = 1617.00066813\n",
      "Iteration 4070, loss = 1616.65956076\n",
      "Iteration 4071, loss = 1616.32013852\n",
      "Iteration 4072, loss = 1616.27118693\n",
      "Iteration 4073, loss = 1615.68905524\n",
      "Iteration 4074, loss = 1615.35245741\n",
      "Iteration 4075, loss = 1615.00326188\n",
      "Iteration 4076, loss = 1614.70762197\n",
      "Iteration 4077, loss = 1614.39637650\n",
      "Iteration 4078, loss = 1614.02097892\n",
      "Iteration 4079, loss = 1613.74061516\n",
      "Iteration 4080, loss = 1613.40889870\n",
      "Iteration 4081, loss = 1613.04036825\n",
      "Iteration 4082, loss = 1612.64912137\n",
      "Iteration 4083, loss = 1612.38802246\n",
      "Iteration 4084, loss = 1612.08507702\n",
      "Iteration 4085, loss = 1611.66830836\n",
      "Iteration 4086, loss = 1611.34014243\n",
      "Iteration 4087, loss = 1610.94139270\n",
      "Iteration 4088, loss = 1610.60427114\n",
      "Iteration 4089, loss = 1610.27127311\n",
      "Iteration 4090, loss = 1609.96319954\n",
      "Iteration 4091, loss = 1609.52697714\n",
      "Iteration 4092, loss = 1609.18655673\n",
      "Iteration 4093, loss = 1608.81492000\n",
      "Iteration 4094, loss = 1608.47822928\n",
      "Iteration 4095, loss = 1608.12047323\n",
      "Iteration 4096, loss = 1607.77098789\n",
      "Iteration 4097, loss = 1607.46939473\n",
      "Iteration 4098, loss = 1607.08450860\n",
      "Iteration 4099, loss = 1606.56352150\n",
      "Iteration 4100, loss = 1606.08522356\n",
      "Iteration 4101, loss = 1605.90631337\n",
      "Iteration 4102, loss = 1605.29801127\n",
      "Iteration 4103, loss = 1604.94892149\n",
      "Iteration 4104, loss = 1604.38513330\n",
      "Iteration 4105, loss = 1603.96349600\n",
      "Iteration 4106, loss = 1603.56236957\n",
      "Iteration 4107, loss = 1603.03731798\n",
      "Iteration 4108, loss = 1602.47834266\n",
      "Iteration 4109, loss = 1601.94241764\n",
      "Iteration 4110, loss = 1601.60116947\n",
      "Iteration 4111, loss = 1600.93982693\n",
      "Iteration 4112, loss = 1600.31675968\n",
      "Iteration 4113, loss = 1599.58295496\n",
      "Iteration 4114, loss = 1598.93512136\n",
      "Iteration 4115, loss = 1598.23617421\n",
      "Iteration 4116, loss = 1597.47599666\n",
      "Iteration 4117, loss = 1596.61419830\n",
      "Iteration 4118, loss = 1595.89084220\n",
      "Iteration 4119, loss = 1594.99908460\n",
      "Iteration 4120, loss = 1593.95122391\n",
      "Iteration 4121, loss = 1592.95333472\n",
      "Iteration 4122, loss = 1592.07949532\n",
      "Iteration 4123, loss = 1590.82326939\n",
      "Iteration 4124, loss = 1589.79252748\n",
      "Iteration 4125, loss = 1588.67903666\n",
      "Iteration 4126, loss = 1587.44564800\n",
      "Iteration 4127, loss = 1586.26759086\n",
      "Iteration 4128, loss = 1584.99166673\n",
      "Iteration 4129, loss = 1583.77285068\n",
      "Iteration 4130, loss = 1582.52827546\n",
      "Iteration 4131, loss = 1581.24899964\n",
      "Iteration 4132, loss = 1580.03831685\n",
      "Iteration 4133, loss = 1578.80695758\n",
      "Iteration 4134, loss = 1577.57365152\n",
      "Iteration 4135, loss = 1576.49281417\n",
      "Iteration 4136, loss = 1575.19647682\n",
      "Iteration 4137, loss = 1574.03869040\n",
      "Iteration 4138, loss = 1572.86586973\n",
      "Iteration 4139, loss = 1571.87592485\n",
      "Iteration 4140, loss = 1570.62480748\n",
      "Iteration 4141, loss = 1569.53123040\n",
      "Iteration 4142, loss = 1568.57478273\n",
      "Iteration 4143, loss = 1567.36433175\n",
      "Iteration 4144, loss = 1566.35508192\n",
      "Iteration 4145, loss = 1565.24241148\n",
      "Iteration 4146, loss = 1564.24702928\n",
      "Iteration 4147, loss = 1563.20722653\n",
      "Iteration 4148, loss = 1562.38158938\n",
      "Iteration 4149, loss = 1561.23852228\n",
      "Iteration 4150, loss = 1560.26194004\n",
      "Iteration 4151, loss = 1559.38072520\n",
      "Iteration 4152, loss = 1558.30119461\n",
      "Iteration 4153, loss = 1557.38215192\n",
      "Iteration 4154, loss = 1556.45039516\n",
      "Iteration 4155, loss = 1555.58020346\n",
      "Iteration 4156, loss = 1554.58876634\n",
      "Iteration 4157, loss = 1553.68122773\n",
      "Iteration 4158, loss = 1552.82031120\n",
      "Iteration 4159, loss = 1551.91294842\n",
      "Iteration 4160, loss = 1551.03373556\n",
      "Iteration 4161, loss = 1550.19068183\n",
      "Iteration 4162, loss = 1549.26588933\n",
      "Iteration 4163, loss = 1548.43911630\n",
      "Iteration 4164, loss = 1547.70217092\n",
      "Iteration 4165, loss = 1546.72646388\n",
      "Iteration 4166, loss = 1545.90129585\n",
      "Iteration 4167, loss = 1545.02026479\n",
      "Iteration 4168, loss = 1544.20648073\n",
      "Iteration 4169, loss = 1543.43720073\n",
      "Iteration 4170, loss = 1542.56617003\n",
      "Iteration 4171, loss = 1541.80476087\n",
      "Iteration 4172, loss = 1540.99994501\n",
      "Iteration 4173, loss = 1540.16412433\n",
      "Iteration 4174, loss = 1539.43160819\n",
      "Iteration 4175, loss = 1538.58457766\n",
      "Iteration 4176, loss = 1537.89127062\n",
      "Iteration 4177, loss = 1537.06023119\n",
      "Iteration 4178, loss = 1536.24473013\n",
      "Iteration 4179, loss = 1535.44516670\n",
      "Iteration 4180, loss = 1534.68957484\n",
      "Iteration 4181, loss = 1533.96096123\n",
      "Iteration 4182, loss = 1533.21665158\n",
      "Iteration 4183, loss = 1532.43635333\n",
      "Iteration 4184, loss = 1531.69863965\n",
      "Iteration 4185, loss = 1530.99111282\n",
      "Iteration 4186, loss = 1530.19708426\n",
      "Iteration 4187, loss = 1529.47242618\n",
      "Iteration 4188, loss = 1528.74567426\n",
      "Iteration 4189, loss = 1527.98031560\n",
      "Iteration 4190, loss = 1527.24508973\n",
      "Iteration 4191, loss = 1526.53581549\n",
      "Iteration 4192, loss = 1525.84106686\n",
      "Iteration 4193, loss = 1525.07060322\n",
      "Iteration 4194, loss = 1524.36157926\n",
      "Iteration 4195, loss = 1523.69778491\n",
      "Iteration 4196, loss = 1522.95275500\n",
      "Iteration 4197, loss = 1522.25426129\n",
      "Iteration 4198, loss = 1521.63309733\n",
      "Iteration 4199, loss = 1520.83645622\n",
      "Iteration 4200, loss = 1520.16536412\n",
      "Iteration 4201, loss = 1519.45394986\n",
      "Iteration 4202, loss = 1518.76401437\n",
      "Iteration 4203, loss = 1518.07459684\n",
      "Iteration 4204, loss = 1517.38853604\n",
      "Iteration 4205, loss = 1516.75108731\n",
      "Iteration 4206, loss = 1516.01179207\n",
      "Iteration 4207, loss = 1515.46287702\n",
      "Iteration 4208, loss = 1514.66676908\n",
      "Iteration 4209, loss = 1514.01400418\n",
      "Iteration 4210, loss = 1513.33844843\n",
      "Iteration 4211, loss = 1512.73764292\n",
      "Iteration 4212, loss = 1512.14837755\n",
      "Iteration 4213, loss = 1511.41055899\n",
      "Iteration 4214, loss = 1510.68889487\n",
      "Iteration 4215, loss = 1510.05819279\n",
      "Iteration 4216, loss = 1509.43179178\n",
      "Iteration 4217, loss = 1508.75160360\n",
      "Iteration 4218, loss = 1508.07393697\n",
      "Iteration 4219, loss = 1507.47643172\n",
      "Iteration 4220, loss = 1506.92344652\n",
      "Iteration 4221, loss = 1506.21581582\n",
      "Iteration 4222, loss = 1505.54003871\n",
      "Iteration 4223, loss = 1504.99080210\n",
      "Iteration 4224, loss = 1504.32407267\n",
      "Iteration 4225, loss = 1503.64749727\n",
      "Iteration 4226, loss = 1502.95905207\n",
      "Iteration 4227, loss = 1502.42540860\n",
      "Iteration 4228, loss = 1501.71724161\n",
      "Iteration 4229, loss = 1501.13644428\n",
      "Iteration 4230, loss = 1500.47226745\n",
      "Iteration 4231, loss = 1499.84836694\n",
      "Iteration 4232, loss = 1499.23210225\n",
      "Iteration 4233, loss = 1498.61491246\n",
      "Iteration 4234, loss = 1498.13811918\n",
      "Iteration 4235, loss = 1497.42124783\n",
      "Iteration 4236, loss = 1496.79368457\n",
      "Iteration 4237, loss = 1496.22337105\n",
      "Iteration 4238, loss = 1495.56629567\n",
      "Iteration 4239, loss = 1494.95921101\n",
      "Iteration 4240, loss = 1494.35914290\n",
      "Iteration 4241, loss = 1493.78272901\n",
      "Iteration 4242, loss = 1493.14538211\n",
      "Iteration 4243, loss = 1492.56037001\n",
      "Iteration 4244, loss = 1491.97050748\n",
      "Iteration 4245, loss = 1491.34523935\n",
      "Iteration 4246, loss = 1490.81774374\n",
      "Iteration 4247, loss = 1490.23779172\n",
      "Iteration 4248, loss = 1489.59300177\n",
      "Iteration 4249, loss = 1488.96493217\n",
      "Iteration 4250, loss = 1488.40436696\n",
      "Iteration 4251, loss = 1487.82960265\n",
      "Iteration 4252, loss = 1487.23825339\n",
      "Iteration 4253, loss = 1486.67486232\n",
      "Iteration 4254, loss = 1486.18245311\n",
      "Iteration 4255, loss = 1485.49075597\n",
      "Iteration 4256, loss = 1484.89897180\n",
      "Iteration 4257, loss = 1484.32905978\n",
      "Iteration 4258, loss = 1483.71759044\n",
      "Iteration 4259, loss = 1483.17772977\n",
      "Iteration 4260, loss = 1482.69712829\n",
      "Iteration 4261, loss = 1481.99142552\n",
      "Iteration 4262, loss = 1481.44512738\n",
      "Iteration 4263, loss = 1480.86404900\n",
      "Iteration 4264, loss = 1480.28769082\n",
      "Iteration 4265, loss = 1480.05755664\n",
      "Iteration 4266, loss = 1479.21681917\n",
      "Iteration 4267, loss = 1478.78685170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4268, loss = 1478.03375391\n",
      "Iteration 4269, loss = 1477.48044595\n",
      "Iteration 4270, loss = 1477.07615164\n",
      "Iteration 4271, loss = 1476.36327303\n",
      "Iteration 4272, loss = 1475.81638087\n",
      "Iteration 4273, loss = 1475.26649569\n",
      "Iteration 4274, loss = 1474.69071686\n",
      "Iteration 4275, loss = 1474.23926356\n",
      "Iteration 4276, loss = 1473.59246681\n",
      "Iteration 4277, loss = 1473.02756502\n",
      "Iteration 4278, loss = 1472.44444701\n",
      "Iteration 4279, loss = 1471.93003939\n",
      "Iteration 4280, loss = 1471.42272643\n",
      "Iteration 4281, loss = 1470.78027806\n",
      "Iteration 4282, loss = 1470.27591666\n",
      "Iteration 4283, loss = 1469.79799496\n",
      "Iteration 4284, loss = 1469.15143294\n",
      "Iteration 4285, loss = 1468.62097535\n",
      "Iteration 4286, loss = 1468.11421191\n",
      "Iteration 4287, loss = 1467.59307103\n",
      "Iteration 4288, loss = 1466.97656098\n",
      "Iteration 4289, loss = 1466.44588892\n",
      "Iteration 4290, loss = 1466.08967018\n",
      "Iteration 4291, loss = 1465.36023610\n",
      "Iteration 4292, loss = 1464.82512762\n",
      "Iteration 4293, loss = 1464.26187338\n",
      "Iteration 4294, loss = 1463.74274836\n",
      "Iteration 4295, loss = 1463.19594127\n",
      "Iteration 4296, loss = 1462.63049944\n",
      "Iteration 4297, loss = 1462.13744890\n",
      "Iteration 4298, loss = 1461.58995022\n",
      "Iteration 4299, loss = 1461.07497256\n",
      "Iteration 4300, loss = 1460.52931103\n",
      "Iteration 4301, loss = 1460.04017301\n",
      "Iteration 4302, loss = 1459.42730432\n",
      "Iteration 4303, loss = 1458.88152461\n",
      "Iteration 4304, loss = 1458.36193225\n",
      "Iteration 4305, loss = 1457.86744352\n",
      "Iteration 4306, loss = 1457.30834244\n",
      "Iteration 4307, loss = 1456.76479039\n",
      "Iteration 4308, loss = 1456.23802440\n",
      "Iteration 4309, loss = 1455.69549626\n",
      "Iteration 4310, loss = 1455.16807343\n",
      "Iteration 4311, loss = 1454.67056493\n",
      "Iteration 4312, loss = 1454.18883060\n",
      "Iteration 4313, loss = 1453.60704143\n",
      "Iteration 4314, loss = 1453.03881937\n",
      "Iteration 4315, loss = 1452.56457920\n",
      "Iteration 4316, loss = 1452.00303904\n",
      "Iteration 4317, loss = 1451.49587025\n",
      "Iteration 4318, loss = 1450.89879764\n",
      "Iteration 4319, loss = 1450.38221362\n",
      "Iteration 4320, loss = 1449.89280897\n",
      "Iteration 4321, loss = 1449.46784042\n",
      "Iteration 4322, loss = 1448.72582246\n",
      "Iteration 4323, loss = 1448.22405191\n",
      "Iteration 4324, loss = 1447.67909261\n",
      "Iteration 4325, loss = 1447.23643253\n",
      "Iteration 4326, loss = 1446.61597129\n",
      "Iteration 4327, loss = 1446.16572841\n",
      "Iteration 4328, loss = 1445.56301102\n",
      "Iteration 4329, loss = 1444.98336424\n",
      "Iteration 4330, loss = 1444.44731270\n",
      "Iteration 4331, loss = 1443.98771506\n",
      "Iteration 4332, loss = 1443.30356270\n",
      "Iteration 4333, loss = 1442.74210378\n",
      "Iteration 4334, loss = 1442.21576332\n",
      "Iteration 4335, loss = 1441.67852009\n",
      "Iteration 4336, loss = 1441.05402436\n",
      "Iteration 4337, loss = 1440.48334461\n",
      "Iteration 4338, loss = 1439.87062936\n",
      "Iteration 4339, loss = 1439.28516370\n",
      "Iteration 4340, loss = 1438.67711615\n",
      "Iteration 4341, loss = 1438.10528704\n",
      "Iteration 4342, loss = 1437.43387812\n",
      "Iteration 4343, loss = 1436.77747669\n",
      "Iteration 4344, loss = 1436.11297146\n",
      "Iteration 4345, loss = 1435.46032062\n",
      "Iteration 4346, loss = 1434.76584358\n",
      "Iteration 4347, loss = 1434.13446163\n",
      "Iteration 4348, loss = 1433.37748953\n",
      "Iteration 4349, loss = 1432.51405059\n",
      "Iteration 4350, loss = 1431.68626825\n",
      "Iteration 4351, loss = 1430.89621739\n",
      "Iteration 4352, loss = 1430.00724460\n",
      "Iteration 4353, loss = 1429.12823087\n",
      "Iteration 4354, loss = 1428.21489437\n",
      "Iteration 4355, loss = 1427.16689855\n",
      "Iteration 4356, loss = 1426.20554838\n",
      "Iteration 4357, loss = 1425.05634653\n",
      "Iteration 4358, loss = 1423.92290802\n",
      "Iteration 4359, loss = 1422.80616829\n",
      "Iteration 4360, loss = 1421.56266917\n",
      "Iteration 4361, loss = 1420.33554584\n",
      "Iteration 4362, loss = 1419.05316273\n",
      "Iteration 4363, loss = 1417.77075491\n",
      "Iteration 4364, loss = 1416.45408921\n",
      "Iteration 4365, loss = 1415.12261732\n",
      "Iteration 4366, loss = 1413.81904876\n",
      "Iteration 4367, loss = 1412.44000132\n",
      "Iteration 4368, loss = 1411.08368999\n",
      "Iteration 4369, loss = 1409.74983303\n",
      "Iteration 4370, loss = 1408.43830186\n",
      "Iteration 4371, loss = 1407.07525250\n",
      "Iteration 4372, loss = 1405.82625337\n",
      "Iteration 4373, loss = 1404.47647670\n",
      "Iteration 4374, loss = 1403.15644781\n",
      "Iteration 4375, loss = 1401.95760149\n",
      "Iteration 4376, loss = 1400.67191108\n",
      "Iteration 4377, loss = 1399.41494511\n",
      "Iteration 4378, loss = 1398.17049335\n",
      "Iteration 4379, loss = 1396.95031406\n",
      "Iteration 4380, loss = 1395.73499224\n",
      "Iteration 4381, loss = 1394.65437414\n",
      "Iteration 4382, loss = 1393.37450172\n",
      "Iteration 4383, loss = 1392.27007018\n",
      "Iteration 4384, loss = 1391.08838780\n",
      "Iteration 4385, loss = 1389.97539422\n",
      "Iteration 4386, loss = 1388.82877249\n",
      "Iteration 4387, loss = 1387.72476956\n",
      "Iteration 4388, loss = 1386.66902888\n",
      "Iteration 4389, loss = 1385.76139566\n",
      "Iteration 4390, loss = 1384.56960055\n",
      "Iteration 4391, loss = 1383.43197675\n",
      "Iteration 4392, loss = 1382.35067349\n",
      "Iteration 4393, loss = 1381.40537890\n",
      "Iteration 4394, loss = 1380.29118890\n",
      "Iteration 4395, loss = 1379.28306961\n",
      "Iteration 4396, loss = 1378.23857966\n",
      "Iteration 4397, loss = 1377.23707613\n",
      "Iteration 4398, loss = 1376.23074430\n",
      "Iteration 4399, loss = 1375.28045778\n",
      "Iteration 4400, loss = 1374.26853923\n",
      "Iteration 4401, loss = 1373.30738330\n",
      "Iteration 4402, loss = 1372.30608202\n",
      "Iteration 4403, loss = 1371.31788866\n",
      "Iteration 4404, loss = 1370.37743436\n",
      "Iteration 4405, loss = 1369.40048808\n",
      "Iteration 4406, loss = 1368.61793434\n",
      "Iteration 4407, loss = 1367.59131955\n",
      "Iteration 4408, loss = 1366.59126982\n",
      "Iteration 4409, loss = 1365.66513389\n",
      "Iteration 4410, loss = 1364.78015060\n",
      "Iteration 4411, loss = 1363.89870279\n",
      "Iteration 4412, loss = 1362.91882298\n",
      "Iteration 4413, loss = 1361.99978950\n",
      "Iteration 4414, loss = 1361.10587560\n",
      "Iteration 4415, loss = 1360.19087473\n",
      "Iteration 4416, loss = 1359.33498741\n",
      "Iteration 4417, loss = 1358.43053642\n",
      "Iteration 4418, loss = 1357.52071379\n",
      "Iteration 4419, loss = 1356.65971361\n",
      "Iteration 4420, loss = 1355.84118244\n",
      "Iteration 4421, loss = 1354.92624665\n",
      "Iteration 4422, loss = 1354.08543610\n",
      "Iteration 4423, loss = 1353.17762246\n",
      "Iteration 4424, loss = 1352.34552864\n",
      "Iteration 4425, loss = 1351.50036412\n",
      "Iteration 4426, loss = 1350.65878476\n",
      "Iteration 4427, loss = 1349.80165273\n",
      "Iteration 4428, loss = 1348.98033056\n",
      "Iteration 4429, loss = 1348.13862808\n",
      "Iteration 4430, loss = 1347.28798501\n",
      "Iteration 4431, loss = 1346.45822410\n",
      "Iteration 4432, loss = 1345.64152726\n",
      "Iteration 4433, loss = 1344.83625259\n",
      "Iteration 4434, loss = 1344.00539241\n",
      "Iteration 4435, loss = 1343.16221648\n",
      "Iteration 4436, loss = 1342.35053745\n",
      "Iteration 4437, loss = 1341.56810377\n",
      "Iteration 4438, loss = 1340.72852908\n",
      "Iteration 4439, loss = 1339.97224813\n",
      "Iteration 4440, loss = 1339.24304332\n",
      "Iteration 4441, loss = 1338.39901977\n",
      "Iteration 4442, loss = 1337.54349475\n",
      "Iteration 4443, loss = 1336.74992628\n",
      "Iteration 4444, loss = 1335.96975556\n",
      "Iteration 4445, loss = 1335.17697894\n",
      "Iteration 4446, loss = 1334.37953316\n",
      "Iteration 4447, loss = 1333.65979982\n",
      "Iteration 4448, loss = 1332.85317061\n",
      "Iteration 4449, loss = 1332.08833709\n",
      "Iteration 4450, loss = 1331.29462901\n",
      "Iteration 4451, loss = 1330.52389121\n",
      "Iteration 4452, loss = 1329.90550610\n",
      "Iteration 4453, loss = 1328.99793442\n",
      "Iteration 4454, loss = 1328.27478337\n",
      "Iteration 4455, loss = 1327.56771757\n",
      "Iteration 4456, loss = 1326.83299361\n",
      "Iteration 4457, loss = 1326.12786216\n",
      "Iteration 4458, loss = 1325.24154587\n",
      "Iteration 4459, loss = 1324.49143486\n",
      "Iteration 4460, loss = 1323.75304840\n",
      "Iteration 4461, loss = 1323.07824621\n",
      "Iteration 4462, loss = 1322.30334992\n",
      "Iteration 4463, loss = 1321.53701491\n",
      "Iteration 4464, loss = 1320.86039913\n",
      "Iteration 4465, loss = 1320.12661126\n",
      "Iteration 4466, loss = 1319.31519638\n",
      "Iteration 4467, loss = 1318.61425566\n",
      "Iteration 4468, loss = 1317.93312750\n",
      "Iteration 4469, loss = 1317.14827758\n",
      "Iteration 4470, loss = 1316.45296703\n",
      "Iteration 4471, loss = 1315.72577546\n",
      "Iteration 4472, loss = 1314.98564708\n",
      "Iteration 4473, loss = 1314.27800421\n",
      "Iteration 4474, loss = 1313.57281905\n",
      "Iteration 4475, loss = 1312.87682921\n",
      "Iteration 4476, loss = 1312.16823789\n",
      "Iteration 4477, loss = 1311.52591510\n",
      "Iteration 4478, loss = 1310.75974530\n",
      "Iteration 4479, loss = 1310.01963752\n",
      "Iteration 4480, loss = 1309.31562702\n",
      "Iteration 4481, loss = 1308.65104872\n",
      "Iteration 4482, loss = 1307.97928347\n",
      "Iteration 4483, loss = 1307.23576820\n",
      "Iteration 4484, loss = 1306.57291559\n",
      "Iteration 4485, loss = 1305.89046567\n",
      "Iteration 4486, loss = 1305.17698386\n",
      "Iteration 4487, loss = 1304.47395230\n",
      "Iteration 4488, loss = 1303.78868937\n",
      "Iteration 4489, loss = 1303.13260005\n",
      "Iteration 4490, loss = 1302.49676543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4491, loss = 1301.77549845\n",
      "Iteration 4492, loss = 1301.06184924\n",
      "Iteration 4493, loss = 1300.37837408\n",
      "Iteration 4494, loss = 1299.70926228\n",
      "Iteration 4495, loss = 1299.01529314\n",
      "Iteration 4496, loss = 1298.33982015\n",
      "Iteration 4497, loss = 1297.70248039\n",
      "Iteration 4498, loss = 1296.99811442\n",
      "Iteration 4499, loss = 1296.33536095\n",
      "Iteration 4500, loss = 1295.68254339\n",
      "Iteration 4501, loss = 1295.00571458\n",
      "Iteration 4502, loss = 1294.34994809\n",
      "Iteration 4503, loss = 1293.69807172\n",
      "Iteration 4504, loss = 1293.02875552\n",
      "Iteration 4505, loss = 1292.35856306\n",
      "Iteration 4506, loss = 1291.80321372\n",
      "Iteration 4507, loss = 1291.05789841\n",
      "Iteration 4508, loss = 1290.40825728\n",
      "Iteration 4509, loss = 1289.76535531\n",
      "Iteration 4510, loss = 1289.14440859\n",
      "Iteration 4511, loss = 1288.43066609\n",
      "Iteration 4512, loss = 1287.79663798\n",
      "Iteration 4513, loss = 1287.14395398\n",
      "Iteration 4514, loss = 1286.48250243\n",
      "Iteration 4515, loss = 1285.88826661\n",
      "Iteration 4516, loss = 1285.20860114\n",
      "Iteration 4517, loss = 1284.56427179\n",
      "Iteration 4518, loss = 1283.96357623\n",
      "Iteration 4519, loss = 1283.26396760\n",
      "Iteration 4520, loss = 1282.66391675\n",
      "Iteration 4521, loss = 1282.02869778\n",
      "Iteration 4522, loss = 1281.36676667\n",
      "Iteration 4523, loss = 1280.75003507\n",
      "Iteration 4524, loss = 1280.12960375\n",
      "Iteration 4525, loss = 1279.46475799\n",
      "Iteration 4526, loss = 1278.83754637\n",
      "Iteration 4527, loss = 1278.22999040\n",
      "Iteration 4528, loss = 1277.57592241\n",
      "Iteration 4529, loss = 1276.95540530\n",
      "Iteration 4530, loss = 1276.39357125\n",
      "Iteration 4531, loss = 1275.69867810\n",
      "Iteration 4532, loss = 1275.15909761\n",
      "Iteration 4533, loss = 1274.44602294\n",
      "Iteration 4534, loss = 1273.83631728\n",
      "Iteration 4535, loss = 1273.23939474\n",
      "Iteration 4536, loss = 1272.63190784\n",
      "Iteration 4537, loss = 1271.99935318\n",
      "Iteration 4538, loss = 1271.41542681\n",
      "Iteration 4539, loss = 1270.83202613\n",
      "Iteration 4540, loss = 1270.16366301\n",
      "Iteration 4541, loss = 1269.53515035\n",
      "Iteration 4542, loss = 1268.94803141\n",
      "Iteration 4543, loss = 1268.28987018\n",
      "Iteration 4544, loss = 1267.70414165\n",
      "Iteration 4545, loss = 1267.11240745\n",
      "Iteration 4546, loss = 1266.51974086\n",
      "Iteration 4547, loss = 1265.90186169\n",
      "Iteration 4548, loss = 1265.29419313\n",
      "Iteration 4549, loss = 1264.73834529\n",
      "Iteration 4550, loss = 1264.06334685\n",
      "Iteration 4551, loss = 1263.52176292\n",
      "Iteration 4552, loss = 1262.86634149\n",
      "Iteration 4553, loss = 1262.29142186\n",
      "Iteration 4554, loss = 1261.69815560\n",
      "Iteration 4555, loss = 1261.06706515\n",
      "Iteration 4556, loss = 1260.48381547\n",
      "Iteration 4557, loss = 1259.88091207\n",
      "Iteration 4558, loss = 1259.30611838\n",
      "Iteration 4559, loss = 1258.68969601\n",
      "Iteration 4560, loss = 1258.10030814\n",
      "Iteration 4561, loss = 1257.50677167\n",
      "Iteration 4562, loss = 1256.93885638\n",
      "Iteration 4563, loss = 1256.36192067\n",
      "Iteration 4564, loss = 1255.71666608\n",
      "Iteration 4565, loss = 1255.15653969\n",
      "Iteration 4566, loss = 1254.60155088\n",
      "Iteration 4567, loss = 1254.03569930\n",
      "Iteration 4568, loss = 1253.40559759\n",
      "Iteration 4569, loss = 1252.86600304\n",
      "Iteration 4570, loss = 1252.20347200\n",
      "Iteration 4571, loss = 1251.61470975\n",
      "Iteration 4572, loss = 1251.05191869\n",
      "Iteration 4573, loss = 1250.50828657\n",
      "Iteration 4574, loss = 1249.91442286\n",
      "Iteration 4575, loss = 1249.31382655\n",
      "Iteration 4576, loss = 1248.73485429\n",
      "Iteration 4577, loss = 1248.17341679\n",
      "Iteration 4578, loss = 1247.57402986\n",
      "Iteration 4579, loss = 1247.01541844\n",
      "Iteration 4580, loss = 1246.42714411\n",
      "Iteration 4581, loss = 1245.91635172\n",
      "Iteration 4582, loss = 1245.32233617\n",
      "Iteration 4583, loss = 1244.76095646\n",
      "Iteration 4584, loss = 1244.20307307\n",
      "Iteration 4585, loss = 1243.64257944\n",
      "Iteration 4586, loss = 1243.01074002\n",
      "Iteration 4587, loss = 1242.46618740\n",
      "Iteration 4588, loss = 1241.87331502\n",
      "Iteration 4589, loss = 1241.33891716\n",
      "Iteration 4590, loss = 1240.81058030\n",
      "Iteration 4591, loss = 1240.21958052\n",
      "Iteration 4592, loss = 1239.60486779\n",
      "Iteration 4593, loss = 1239.07273942\n",
      "Iteration 4594, loss = 1238.48351324\n",
      "Iteration 4595, loss = 1238.10578860\n",
      "Iteration 4596, loss = 1237.34984212\n",
      "Iteration 4597, loss = 1236.86263704\n",
      "Iteration 4598, loss = 1236.28316347\n",
      "Iteration 4599, loss = 1235.68542444\n",
      "Iteration 4600, loss = 1235.17166941\n",
      "Iteration 4601, loss = 1234.64588610\n",
      "Iteration 4602, loss = 1234.04894287\n",
      "Iteration 4603, loss = 1233.45949294\n",
      "Iteration 4604, loss = 1232.98068339\n",
      "Iteration 4605, loss = 1232.37425755\n",
      "Iteration 4606, loss = 1231.81629753\n",
      "Iteration 4607, loss = 1231.24670207\n",
      "Iteration 4608, loss = 1230.71178031\n",
      "Iteration 4609, loss = 1230.15420511\n",
      "Iteration 4610, loss = 1229.60226285\n",
      "Iteration 4611, loss = 1229.03260008\n",
      "Iteration 4612, loss = 1228.55382552\n",
      "Iteration 4613, loss = 1227.95990405\n",
      "Iteration 4614, loss = 1227.41163380\n",
      "Iteration 4615, loss = 1226.85303300\n",
      "Iteration 4616, loss = 1226.31961268\n",
      "Iteration 4617, loss = 1225.78957664\n",
      "Iteration 4618, loss = 1225.24450200\n",
      "Iteration 4619, loss = 1224.71230882\n",
      "Iteration 4620, loss = 1224.19006391\n",
      "Iteration 4621, loss = 1223.59297028\n",
      "Iteration 4622, loss = 1223.04987637\n",
      "Iteration 4623, loss = 1222.49979577\n",
      "Iteration 4624, loss = 1222.03815957\n",
      "Iteration 4625, loss = 1221.46794288\n",
      "Iteration 4626, loss = 1220.91481920\n",
      "Iteration 4627, loss = 1220.36643793\n",
      "Iteration 4628, loss = 1219.84286034\n",
      "Iteration 4629, loss = 1219.34353825\n",
      "Iteration 4630, loss = 1218.77802742\n",
      "Iteration 4631, loss = 1218.22729010\n",
      "Iteration 4632, loss = 1217.68532030\n",
      "Iteration 4633, loss = 1217.20068112\n",
      "Iteration 4634, loss = 1216.63064321\n",
      "Iteration 4635, loss = 1216.10092392\n",
      "Iteration 4636, loss = 1215.64211878\n",
      "Iteration 4637, loss = 1215.03124917\n",
      "Iteration 4638, loss = 1214.48020110\n",
      "Iteration 4639, loss = 1213.96149275\n",
      "Iteration 4640, loss = 1213.47102037\n",
      "Iteration 4641, loss = 1212.93174378\n",
      "Iteration 4642, loss = 1212.37615236\n",
      "Iteration 4643, loss = 1211.82646552\n",
      "Iteration 4644, loss = 1211.30020307\n",
      "Iteration 4645, loss = 1210.85129017\n",
      "Iteration 4646, loss = 1210.33706406\n",
      "Iteration 4647, loss = 1209.74759415\n",
      "Iteration 4648, loss = 1209.28317284\n",
      "Iteration 4649, loss = 1208.71377587\n",
      "Iteration 4650, loss = 1208.17319713\n",
      "Iteration 4651, loss = 1207.66829844\n",
      "Iteration 4652, loss = 1207.16633025\n",
      "Iteration 4653, loss = 1206.65571678\n",
      "Iteration 4654, loss = 1206.16042771\n",
      "Iteration 4655, loss = 1205.56231166\n",
      "Iteration 4656, loss = 1205.03191278\n",
      "Iteration 4657, loss = 1204.53717009\n",
      "Iteration 4658, loss = 1204.02431711\n",
      "Iteration 4659, loss = 1203.53097208\n",
      "Iteration 4660, loss = 1202.95767975\n",
      "Iteration 4661, loss = 1202.44580277\n",
      "Iteration 4662, loss = 1201.94670841\n",
      "Iteration 4663, loss = 1201.44699554\n",
      "Iteration 4664, loss = 1200.89197465\n",
      "Iteration 4665, loss = 1200.38317508\n",
      "Iteration 4666, loss = 1199.90104879\n",
      "Iteration 4667, loss = 1199.36717341\n",
      "Iteration 4668, loss = 1198.82847062\n",
      "Iteration 4669, loss = 1198.32228315\n",
      "Iteration 4670, loss = 1197.82537607\n",
      "Iteration 4671, loss = 1197.35862726\n",
      "Iteration 4672, loss = 1196.82593493\n",
      "Iteration 4673, loss = 1196.32976549\n",
      "Iteration 4674, loss = 1195.81689628\n",
      "Iteration 4675, loss = 1195.28055739\n",
      "Iteration 4676, loss = 1194.74372289\n",
      "Iteration 4677, loss = 1194.23212937\n",
      "Iteration 4678, loss = 1193.72628154\n",
      "Iteration 4679, loss = 1193.28231636\n",
      "Iteration 4680, loss = 1192.80755159\n",
      "Iteration 4681, loss = 1192.20805822\n",
      "Iteration 4682, loss = 1191.84276838\n",
      "Iteration 4683, loss = 1191.18474729\n",
      "Iteration 4684, loss = 1190.71149223\n",
      "Iteration 4685, loss = 1190.25141601\n",
      "Iteration 4686, loss = 1189.76285237\n",
      "Iteration 4687, loss = 1189.23527924\n",
      "Iteration 4688, loss = 1188.68962031\n",
      "Iteration 4689, loss = 1188.20899035\n",
      "Iteration 4690, loss = 1187.68788360\n",
      "Iteration 4691, loss = 1187.17719742\n",
      "Iteration 4692, loss = 1186.74452272\n",
      "Iteration 4693, loss = 1186.19782580\n",
      "Iteration 4694, loss = 1185.69991524\n",
      "Iteration 4695, loss = 1185.26626198\n",
      "Iteration 4696, loss = 1184.71101252\n",
      "Iteration 4697, loss = 1184.26086172\n",
      "Iteration 4698, loss = 1183.69024981\n",
      "Iteration 4699, loss = 1183.19257630\n",
      "Iteration 4700, loss = 1182.71299826\n",
      "Iteration 4701, loss = 1182.19443464\n",
      "Iteration 4702, loss = 1181.72967346\n",
      "Iteration 4703, loss = 1181.19176253\n",
      "Iteration 4704, loss = 1180.73408330\n",
      "Iteration 4705, loss = 1180.29837011\n",
      "Iteration 4706, loss = 1179.71552207\n",
      "Iteration 4707, loss = 1179.27116651\n",
      "Iteration 4708, loss = 1178.73955487\n",
      "Iteration 4709, loss = 1178.25905290\n",
      "Iteration 4710, loss = 1177.75887900\n",
      "Iteration 4711, loss = 1177.29129622\n",
      "Iteration 4712, loss = 1176.79622013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4713, loss = 1176.34533925\n",
      "Iteration 4714, loss = 1175.78920148\n",
      "Iteration 4715, loss = 1175.38047018\n",
      "Iteration 4716, loss = 1174.88356353\n",
      "Iteration 4717, loss = 1174.34550527\n",
      "Iteration 4718, loss = 1173.84273891\n",
      "Iteration 4719, loss = 1173.36098064\n",
      "Iteration 4720, loss = 1172.90471465\n",
      "Iteration 4721, loss = 1172.40708244\n",
      "Iteration 4722, loss = 1171.94561442\n",
      "Iteration 4723, loss = 1171.49503911\n",
      "Iteration 4724, loss = 1170.94453035\n",
      "Iteration 4725, loss = 1170.45279681\n",
      "Iteration 4726, loss = 1169.95422223\n",
      "Iteration 4727, loss = 1169.56420446\n",
      "Iteration 4728, loss = 1168.99323997\n",
      "Iteration 4729, loss = 1168.50159430\n",
      "Iteration 4730, loss = 1168.04752446\n",
      "Iteration 4731, loss = 1167.56022621\n",
      "Iteration 4732, loss = 1167.05257941\n",
      "Iteration 4733, loss = 1166.63939301\n",
      "Iteration 4734, loss = 1166.11079721\n",
      "Iteration 4735, loss = 1165.63726010\n",
      "Iteration 4736, loss = 1165.17660132\n",
      "Iteration 4737, loss = 1164.66083616\n",
      "Iteration 4738, loss = 1164.18483730\n",
      "Iteration 4739, loss = 1163.74976475\n",
      "Iteration 4740, loss = 1163.21582708\n",
      "Iteration 4741, loss = 1162.80766851\n",
      "Iteration 4742, loss = 1162.28010855\n",
      "Iteration 4743, loss = 1161.82646843\n",
      "Iteration 4744, loss = 1161.40662370\n",
      "Iteration 4745, loss = 1160.89265613\n",
      "Iteration 4746, loss = 1160.37689169\n",
      "Iteration 4747, loss = 1159.87862378\n",
      "Iteration 4748, loss = 1159.48430163\n",
      "Iteration 4749, loss = 1158.97820318\n",
      "Iteration 4750, loss = 1158.50314507\n",
      "Iteration 4751, loss = 1158.01304929\n",
      "Iteration 4752, loss = 1157.53168553\n",
      "Iteration 4753, loss = 1157.07680254\n",
      "Iteration 4754, loss = 1156.56273278\n",
      "Iteration 4755, loss = 1156.16809200\n",
      "Iteration 4756, loss = 1155.67866703\n",
      "Iteration 4757, loss = 1155.16219448\n",
      "Iteration 4758, loss = 1154.70029215\n",
      "Iteration 4759, loss = 1154.23571356\n",
      "Iteration 4760, loss = 1153.78335878\n",
      "Iteration 4761, loss = 1153.30017233\n",
      "Iteration 4762, loss = 1152.84341022\n",
      "Iteration 4763, loss = 1152.37459194\n",
      "Iteration 4764, loss = 1151.96596317\n",
      "Iteration 4765, loss = 1151.41132981\n",
      "Iteration 4766, loss = 1151.03155408\n",
      "Iteration 4767, loss = 1150.50815893\n",
      "Iteration 4768, loss = 1150.02975529\n",
      "Iteration 4769, loss = 1149.56438269\n",
      "Iteration 4770, loss = 1149.10876244\n",
      "Iteration 4771, loss = 1148.61805351\n",
      "Iteration 4772, loss = 1148.15142289\n",
      "Iteration 4773, loss = 1147.69157128\n",
      "Iteration 4774, loss = 1147.24409870\n",
      "Iteration 4775, loss = 1146.77699323\n",
      "Iteration 4776, loss = 1146.29484488\n",
      "Iteration 4777, loss = 1145.83460750\n",
      "Iteration 4778, loss = 1145.39058548\n",
      "Iteration 4779, loss = 1144.92865011\n",
      "Iteration 4780, loss = 1144.44676439\n",
      "Iteration 4781, loss = 1144.09039848\n",
      "Iteration 4782, loss = 1143.57985637\n",
      "Iteration 4783, loss = 1143.08190815\n",
      "Iteration 4784, loss = 1142.66026860\n",
      "Iteration 4785, loss = 1142.16352854\n",
      "Iteration 4786, loss = 1141.70115388\n",
      "Iteration 4787, loss = 1141.24240275\n",
      "Iteration 4788, loss = 1140.75738432\n",
      "Iteration 4789, loss = 1140.34527459\n",
      "Iteration 4790, loss = 1139.85313645\n",
      "Iteration 4791, loss = 1139.39886721\n",
      "Iteration 4792, loss = 1138.99756494\n",
      "Iteration 4793, loss = 1138.50105420\n",
      "Iteration 4794, loss = 1138.04320423\n",
      "Iteration 4795, loss = 1137.62281762\n",
      "Iteration 4796, loss = 1137.17198984\n",
      "Iteration 4797, loss = 1136.67272523\n",
      "Iteration 4798, loss = 1136.30295192\n",
      "Iteration 4799, loss = 1135.75214885\n",
      "Iteration 4800, loss = 1135.31677199\n",
      "Iteration 4801, loss = 1134.85941113\n",
      "Iteration 4802, loss = 1134.38824182\n",
      "Iteration 4803, loss = 1133.97611672\n",
      "Iteration 4804, loss = 1133.51205406\n",
      "Iteration 4805, loss = 1133.08676644\n",
      "Iteration 4806, loss = 1132.61372410\n",
      "Iteration 4807, loss = 1132.14002197\n",
      "Iteration 4808, loss = 1131.71785486\n",
      "Iteration 4809, loss = 1131.29038439\n",
      "Iteration 4810, loss = 1130.82431585\n",
      "Iteration 4811, loss = 1130.34937418\n",
      "Iteration 4812, loss = 1129.87228182\n",
      "Iteration 4813, loss = 1129.51349985\n",
      "Iteration 4814, loss = 1128.99888839\n",
      "Iteration 4815, loss = 1128.54677221\n",
      "Iteration 4816, loss = 1128.11961475\n",
      "Iteration 4817, loss = 1127.67770040\n",
      "Iteration 4818, loss = 1127.44109909\n",
      "Iteration 4819, loss = 1126.77336225\n",
      "Iteration 4820, loss = 1126.33238012\n",
      "Iteration 4821, loss = 1125.90553529\n",
      "Iteration 4822, loss = 1125.42723270\n",
      "Iteration 4823, loss = 1124.99842701\n",
      "Iteration 4824, loss = 1124.57358258\n",
      "Iteration 4825, loss = 1124.14196644\n",
      "Iteration 4826, loss = 1123.66342200\n",
      "Iteration 4827, loss = 1123.20203102\n",
      "Iteration 4828, loss = 1122.77987553\n",
      "Iteration 4829, loss = 1122.30410263\n",
      "Iteration 4830, loss = 1121.89134650\n",
      "Iteration 4831, loss = 1121.41468214\n",
      "Iteration 4832, loss = 1120.97840610\n",
      "Iteration 4833, loss = 1120.56786664\n",
      "Iteration 4834, loss = 1120.12023633\n",
      "Iteration 4835, loss = 1119.68792689\n",
      "Iteration 4836, loss = 1119.26861441\n",
      "Iteration 4837, loss = 1118.77453908\n",
      "Iteration 4838, loss = 1118.35814820\n",
      "Iteration 4839, loss = 1117.92158934\n",
      "Iteration 4840, loss = 1117.47909829\n",
      "Iteration 4841, loss = 1117.06033910\n",
      "Iteration 4842, loss = 1116.69276075\n",
      "Iteration 4843, loss = 1116.19286326\n",
      "Iteration 4844, loss = 1115.71545040\n",
      "Iteration 4845, loss = 1115.27649360\n",
      "Iteration 4846, loss = 1114.80940048\n",
      "Iteration 4847, loss = 1114.37540195\n",
      "Iteration 4848, loss = 1113.94680937\n",
      "Iteration 4849, loss = 1113.53700919\n",
      "Iteration 4850, loss = 1113.12985002\n",
      "Iteration 4851, loss = 1112.72034634\n",
      "Iteration 4852, loss = 1112.23549775\n",
      "Iteration 4853, loss = 1111.82118875\n",
      "Iteration 4854, loss = 1111.35985006\n",
      "Iteration 4855, loss = 1111.01106411\n",
      "Iteration 4856, loss = 1110.49258505\n",
      "Iteration 4857, loss = 1110.06923428\n",
      "Iteration 4858, loss = 1109.61484778\n",
      "Iteration 4859, loss = 1109.25709700\n",
      "Iteration 4860, loss = 1108.76880169\n",
      "Iteration 4861, loss = 1108.30152680\n",
      "Iteration 4862, loss = 1107.86274484\n",
      "Iteration 4863, loss = 1107.44637258\n",
      "Iteration 4864, loss = 1107.02190503\n",
      "Iteration 4865, loss = 1106.78644000\n",
      "Iteration 4866, loss = 1106.14392162\n",
      "Iteration 4867, loss = 1105.73249650\n",
      "Iteration 4868, loss = 1105.29220272\n",
      "Iteration 4869, loss = 1104.88013471\n",
      "Iteration 4870, loss = 1104.41326772\n",
      "Iteration 4871, loss = 1104.00892093\n",
      "Iteration 4872, loss = 1103.56067108\n",
      "Iteration 4873, loss = 1103.12834966\n",
      "Iteration 4874, loss = 1102.79002895\n",
      "Iteration 4875, loss = 1102.27614182\n",
      "Iteration 4876, loss = 1101.86884531\n",
      "Iteration 4877, loss = 1101.46798634\n",
      "Iteration 4878, loss = 1100.99311116\n",
      "Iteration 4879, loss = 1100.55512499\n",
      "Iteration 4880, loss = 1100.15851144\n",
      "Iteration 4881, loss = 1099.74562337\n",
      "Iteration 4882, loss = 1099.31806174\n",
      "Iteration 4883, loss = 1098.95377615\n",
      "Iteration 4884, loss = 1098.57101499\n",
      "Iteration 4885, loss = 1098.00044001\n",
      "Iteration 4886, loss = 1097.64711520\n",
      "Iteration 4887, loss = 1097.14887094\n",
      "Iteration 4888, loss = 1096.73663212\n",
      "Iteration 4889, loss = 1096.31039221\n",
      "Iteration 4890, loss = 1095.90860538\n",
      "Iteration 4891, loss = 1095.48386133\n",
      "Iteration 4892, loss = 1095.06790821\n",
      "Iteration 4893, loss = 1094.70561161\n",
      "Iteration 4894, loss = 1094.27022153\n",
      "Iteration 4895, loss = 1093.82252959\n",
      "Iteration 4896, loss = 1093.43712590\n",
      "Iteration 4897, loss = 1092.93470538\n",
      "Iteration 4898, loss = 1092.59310038\n",
      "Iteration 4899, loss = 1092.18336544\n",
      "Iteration 4900, loss = 1091.83319882\n",
      "Iteration 4901, loss = 1091.30254845\n",
      "Iteration 4902, loss = 1090.84771328\n",
      "Iteration 4903, loss = 1090.45297267\n",
      "Iteration 4904, loss = 1090.01591316\n",
      "Iteration 4905, loss = 1089.58141052\n",
      "Iteration 4906, loss = 1089.15420827\n",
      "Iteration 4907, loss = 1088.73157442\n",
      "Iteration 4908, loss = 1088.32799783\n",
      "Iteration 4909, loss = 1087.90102830\n",
      "Iteration 4910, loss = 1087.55173445\n",
      "Iteration 4911, loss = 1087.14290138\n",
      "Iteration 4912, loss = 1086.67267565\n",
      "Iteration 4913, loss = 1086.22606586\n",
      "Iteration 4914, loss = 1085.85034260\n",
      "Iteration 4915, loss = 1085.42473535\n",
      "Iteration 4916, loss = 1084.98900345\n",
      "Iteration 4917, loss = 1084.60352593\n",
      "Iteration 4918, loss = 1084.16026534\n",
      "Iteration 4919, loss = 1083.86951633\n",
      "Iteration 4920, loss = 1083.38474757\n",
      "Iteration 4921, loss = 1082.91786084\n",
      "Iteration 4922, loss = 1082.50328124\n",
      "Iteration 4923, loss = 1082.09114561\n",
      "Iteration 4924, loss = 1081.68279465\n",
      "Iteration 4925, loss = 1081.28555493\n",
      "Iteration 4926, loss = 1080.95428995\n",
      "Iteration 4927, loss = 1080.43370817\n",
      "Iteration 4928, loss = 1080.10254927\n",
      "Iteration 4929, loss = 1079.61021119\n",
      "Iteration 4930, loss = 1079.26120671\n",
      "Iteration 4931, loss = 1078.81500384\n",
      "Iteration 4932, loss = 1078.39127097\n",
      "Iteration 4933, loss = 1077.97787799\n",
      "Iteration 4934, loss = 1077.59355337\n",
      "Iteration 4935, loss = 1077.16819441\n",
      "Iteration 4936, loss = 1076.77406650\n",
      "Iteration 4937, loss = 1076.33230082\n",
      "Iteration 4938, loss = 1075.97768671\n",
      "Iteration 4939, loss = 1075.54361892\n",
      "Iteration 4940, loss = 1075.10412772\n",
      "Iteration 4941, loss = 1074.72840311\n",
      "Iteration 4942, loss = 1074.27610851\n",
      "Iteration 4943, loss = 1073.86164787\n",
      "Iteration 4944, loss = 1073.58085169\n",
      "Iteration 4945, loss = 1073.06232203\n",
      "Iteration 4946, loss = 1072.74692943\n",
      "Iteration 4947, loss = 1072.26843587\n",
      "Iteration 4948, loss = 1071.84853699\n",
      "Iteration 4949, loss = 1071.43397542\n",
      "Iteration 4950, loss = 1071.01632621\n",
      "Iteration 4951, loss = 1070.61649077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4952, loss = 1070.28536787\n",
      "Iteration 4953, loss = 1069.79661791\n",
      "Iteration 4954, loss = 1069.41318306\n",
      "Iteration 4955, loss = 1069.05564562\n",
      "Iteration 4956, loss = 1068.62828105\n",
      "Iteration 4957, loss = 1068.20671699\n",
      "Iteration 4958, loss = 1067.79309742\n",
      "Iteration 4959, loss = 1067.48171888\n",
      "Iteration 4960, loss = 1067.08173992\n",
      "Iteration 4961, loss = 1066.64138277\n",
      "Iteration 4962, loss = 1066.17192662\n",
      "Iteration 4963, loss = 1065.77927725\n",
      "Iteration 4964, loss = 1065.38110164\n",
      "Iteration 4965, loss = 1064.98241338\n",
      "Iteration 4966, loss = 1064.55770634\n",
      "Iteration 4967, loss = 1064.22779388\n",
      "Iteration 4968, loss = 1063.78281812\n",
      "Iteration 4969, loss = 1063.38483450\n",
      "Iteration 4970, loss = 1062.94740775\n",
      "Iteration 4971, loss = 1062.57576391\n",
      "Iteration 4972, loss = 1062.16778154\n",
      "Iteration 4973, loss = 1061.79733981\n",
      "Iteration 4974, loss = 1061.38462222\n",
      "Iteration 4975, loss = 1060.97374422\n",
      "Iteration 4976, loss = 1060.56628406\n",
      "Iteration 4977, loss = 1060.16098283\n",
      "Iteration 4978, loss = 1059.84171943\n",
      "Iteration 4979, loss = 1059.37730661\n",
      "Iteration 4980, loss = 1059.01452816\n",
      "Iteration 4981, loss = 1058.57982716\n",
      "Iteration 4982, loss = 1058.17369850\n",
      "Iteration 4983, loss = 1057.77605276\n",
      "Iteration 4984, loss = 1057.44382944\n",
      "Iteration 4985, loss = 1056.97657236\n",
      "Iteration 4986, loss = 1056.61080544\n",
      "Iteration 4987, loss = 1056.20645128\n",
      "Iteration 4988, loss = 1055.83106789\n",
      "Iteration 4989, loss = 1055.44574221\n",
      "Iteration 4990, loss = 1055.27000407\n",
      "Iteration 4991, loss = 1054.69594979\n",
      "Iteration 4992, loss = 1054.22815888\n",
      "Iteration 4993, loss = 1053.81245132\n",
      "Iteration 4994, loss = 1053.45697321\n",
      "Iteration 4995, loss = 1053.04694008\n",
      "Iteration 4996, loss = 1052.68416877\n",
      "Iteration 4997, loss = 1052.22378035\n",
      "Iteration 4998, loss = 1051.87398742\n",
      "Iteration 4999, loss = 1051.47160547\n",
      "Iteration 5000, loss = 1051.05974264\n",
      "Iteration 5001, loss = 1050.68042415\n",
      "Iteration 5002, loss = 1050.28858980\n",
      "Iteration 5003, loss = 1049.87068303\n",
      "Iteration 5004, loss = 1049.51209628\n",
      "Iteration 5005, loss = 1049.10265470\n",
      "Iteration 5006, loss = 1048.77231592\n",
      "Iteration 5007, loss = 1048.34944527\n",
      "Iteration 5008, loss = 1048.07204796\n",
      "Iteration 5009, loss = 1047.60240085\n",
      "Iteration 5010, loss = 1047.17208812\n",
      "Iteration 5011, loss = 1046.73981638\n",
      "Iteration 5012, loss = 1046.36679275\n",
      "Iteration 5013, loss = 1046.03220139\n",
      "Iteration 5014, loss = 1045.58731405\n",
      "Iteration 5015, loss = 1045.18090794\n",
      "Iteration 5016, loss = 1044.80448507\n",
      "Iteration 5017, loss = 1044.46534529\n",
      "Iteration 5018, loss = 1044.03521979\n",
      "Iteration 5019, loss = 1043.64173427\n",
      "Iteration 5020, loss = 1043.30392076\n",
      "Iteration 5021, loss = 1042.87581084\n",
      "Iteration 5022, loss = 1042.47652060\n",
      "Iteration 5023, loss = 1042.07582304\n",
      "Iteration 5024, loss = 1041.69169192\n",
      "Iteration 5025, loss = 1041.30955661\n",
      "Iteration 5026, loss = 1040.91713969\n",
      "Iteration 5027, loss = 1040.57363607\n",
      "Iteration 5028, loss = 1040.22497245\n",
      "Iteration 5029, loss = 1039.75513989\n",
      "Iteration 5030, loss = 1039.38542222\n",
      "Iteration 5031, loss = 1038.98411759\n",
      "Iteration 5032, loss = 1038.68592491\n",
      "Iteration 5033, loss = 1038.24681511\n",
      "Iteration 5034, loss = 1037.85129492\n",
      "Iteration 5035, loss = 1037.44073353\n",
      "Iteration 5036, loss = 1037.08558453\n",
      "Iteration 5037, loss = 1036.84831696\n",
      "Iteration 5038, loss = 1036.35220935\n",
      "Iteration 5039, loss = 1035.91871605\n",
      "Iteration 5040, loss = 1035.53283364\n",
      "Iteration 5041, loss = 1035.23171598\n",
      "Iteration 5042, loss = 1034.78686429\n",
      "Iteration 5043, loss = 1034.39183998\n",
      "Iteration 5044, loss = 1034.00369661\n",
      "Iteration 5045, loss = 1033.70466621\n",
      "Iteration 5046, loss = 1033.32744386\n",
      "Iteration 5047, loss = 1032.96986587\n",
      "Iteration 5048, loss = 1032.49847288\n",
      "Iteration 5049, loss = 1032.10779278\n",
      "Iteration 5050, loss = 1031.71120952\n",
      "Iteration 5051, loss = 1031.33008355\n",
      "Iteration 5052, loss = 1030.94699814\n",
      "Iteration 5053, loss = 1030.58033153\n",
      "Iteration 5054, loss = 1030.20956686\n",
      "Iteration 5055, loss = 1029.81008012\n",
      "Iteration 5056, loss = 1029.47047019\n",
      "Iteration 5057, loss = 1029.05874680\n",
      "Iteration 5058, loss = 1028.70275982\n",
      "Iteration 5059, loss = 1028.42205144\n",
      "Iteration 5060, loss = 1027.90109387\n",
      "Iteration 5061, loss = 1027.65923627\n",
      "Iteration 5062, loss = 1027.22230239\n",
      "Iteration 5063, loss = 1026.79092786\n",
      "Iteration 5064, loss = 1026.44035695\n",
      "Iteration 5065, loss = 1026.05204121\n",
      "Iteration 5066, loss = 1025.65951016\n",
      "Iteration 5067, loss = 1025.27131872\n",
      "Iteration 5068, loss = 1024.93397750\n",
      "Iteration 5069, loss = 1024.51520789\n",
      "Iteration 5070, loss = 1024.13962280\n",
      "Iteration 5071, loss = 1023.77062951\n",
      "Iteration 5072, loss = 1023.49789126\n",
      "Iteration 5073, loss = 1023.04067801\n",
      "Iteration 5074, loss = 1022.65147206\n",
      "Iteration 5075, loss = 1022.26921895\n",
      "Iteration 5076, loss = 1021.94243991\n",
      "Iteration 5077, loss = 1021.51216007\n",
      "Iteration 5078, loss = 1021.15994567\n",
      "Iteration 5079, loss = 1020.78420661\n",
      "Iteration 5080, loss = 1020.43662130\n",
      "Iteration 5081, loss = 1020.12769019\n",
      "Iteration 5082, loss = 1019.65375192\n",
      "Iteration 5083, loss = 1019.27144051\n",
      "Iteration 5084, loss = 1018.90964083\n",
      "Iteration 5085, loss = 1018.53978682\n",
      "Iteration 5086, loss = 1018.26491763\n",
      "Iteration 5087, loss = 1017.91971394\n",
      "Iteration 5088, loss = 1017.40532857\n",
      "Iteration 5089, loss = 1017.10059342\n",
      "Iteration 5090, loss = 1016.68557057\n",
      "Iteration 5091, loss = 1016.34882852\n",
      "Iteration 5092, loss = 1015.94826574\n",
      "Iteration 5093, loss = 1015.56437662\n",
      "Iteration 5094, loss = 1015.24197022\n",
      "Iteration 5095, loss = 1014.80757675\n",
      "Iteration 5096, loss = 1014.48575773\n",
      "Iteration 5097, loss = 1014.08921247\n",
      "Iteration 5098, loss = 1013.70031194\n",
      "Iteration 5099, loss = 1013.33999952\n",
      "Iteration 5100, loss = 1012.95740187\n",
      "Iteration 5101, loss = 1012.58997202\n",
      "Iteration 5102, loss = 1012.32566983\n",
      "Iteration 5103, loss = 1011.89989541\n",
      "Iteration 5104, loss = 1011.49569809\n",
      "Iteration 5105, loss = 1011.11802604\n",
      "Iteration 5106, loss = 1010.74415544\n",
      "Iteration 5107, loss = 1010.38783566\n",
      "Iteration 5108, loss = 1010.01551514\n",
      "Iteration 5109, loss = 1009.69747955\n",
      "Iteration 5110, loss = 1009.32217585\n",
      "Iteration 5111, loss = 1008.90625132\n",
      "Iteration 5112, loss = 1008.57806288\n",
      "Iteration 5113, loss = 1008.19821734\n",
      "Iteration 5114, loss = 1007.81549223\n",
      "Iteration 5115, loss = 1007.47293103\n",
      "Iteration 5116, loss = 1007.12416524\n",
      "Iteration 5117, loss = 1006.75498594\n",
      "Iteration 5118, loss = 1006.33830262\n",
      "Iteration 5119, loss = 1005.97888579\n",
      "Iteration 5120, loss = 1005.62756846\n",
      "Iteration 5121, loss = 1005.25577744\n",
      "Iteration 5122, loss = 1004.89706364\n",
      "Iteration 5123, loss = 1004.53472551\n",
      "Iteration 5124, loss = 1004.18051934\n",
      "Iteration 5125, loss = 1003.80686257\n",
      "Iteration 5126, loss = 1003.44077602\n",
      "Iteration 5127, loss = 1003.08721108\n",
      "Iteration 5128, loss = 1002.74580535\n",
      "Iteration 5129, loss = 1002.34413730\n",
      "Iteration 5130, loss = 1001.99745838\n",
      "Iteration 5131, loss = 1001.63501600\n",
      "Iteration 5132, loss = 1001.25514920\n",
      "Iteration 5133, loss = 1000.94659088\n",
      "Iteration 5134, loss = 1000.56703848\n",
      "Iteration 5135, loss = 1000.16676599\n",
      "Iteration 5136, loss = 999.80948057\n",
      "Iteration 5137, loss = 999.44883361\n",
      "Iteration 5138, loss = 999.08908912\n",
      "Iteration 5139, loss = 998.74949340\n",
      "Iteration 5140, loss = 998.38473349\n",
      "Iteration 5141, loss = 998.01228979\n",
      "Iteration 5142, loss = 997.67666651\n",
      "Iteration 5143, loss = 997.26257069\n",
      "Iteration 5144, loss = 996.92299258\n",
      "Iteration 5145, loss = 996.58544074\n",
      "Iteration 5146, loss = 996.18998125\n",
      "Iteration 5147, loss = 995.89519860\n",
      "Iteration 5148, loss = 995.47255220\n",
      "Iteration 5149, loss = 995.14111394\n",
      "Iteration 5150, loss = 994.76221810\n",
      "Iteration 5151, loss = 994.41970266\n",
      "Iteration 5152, loss = 994.10380829\n",
      "Iteration 5153, loss = 993.67758858\n",
      "Iteration 5154, loss = 993.39287700\n",
      "Iteration 5155, loss = 993.01208228\n",
      "Iteration 5156, loss = 992.62196501\n",
      "Iteration 5157, loss = 992.26230492\n",
      "Iteration 5158, loss = 991.92373047\n",
      "Iteration 5159, loss = 991.52485115\n",
      "Iteration 5160, loss = 991.19993841\n",
      "Iteration 5161, loss = 990.82685195\n",
      "Iteration 5162, loss = 990.48993049\n",
      "Iteration 5163, loss = 990.11881200\n",
      "Iteration 5164, loss = 989.74714118\n",
      "Iteration 5165, loss = 989.38424348\n",
      "Iteration 5166, loss = 989.14464039\n",
      "Iteration 5167, loss = 988.69682170\n",
      "Iteration 5168, loss = 988.36404697\n",
      "Iteration 5169, loss = 987.99507293\n",
      "Iteration 5170, loss = 987.63993655\n",
      "Iteration 5171, loss = 987.33867209\n",
      "Iteration 5172, loss = 986.92021751\n",
      "Iteration 5173, loss = 986.58279707\n",
      "Iteration 5174, loss = 986.23545779\n",
      "Iteration 5175, loss = 985.85588575\n",
      "Iteration 5176, loss = 985.49749967\n",
      "Iteration 5177, loss = 985.14765560\n",
      "Iteration 5178, loss = 984.79212204\n",
      "Iteration 5179, loss = 984.44763290\n",
      "Iteration 5180, loss = 984.09703995\n",
      "Iteration 5181, loss = 983.78774113\n",
      "Iteration 5182, loss = 983.41164698\n",
      "Iteration 5183, loss = 983.14372893\n",
      "Iteration 5184, loss = 982.67158936\n",
      "Iteration 5185, loss = 982.35007557\n",
      "Iteration 5186, loss = 981.97679190\n",
      "Iteration 5187, loss = 981.63122092\n",
      "Iteration 5188, loss = 981.24321443\n",
      "Iteration 5189, loss = 980.92829693\n",
      "Iteration 5190, loss = 980.60733056\n",
      "Iteration 5191, loss = 980.22063830\n",
      "Iteration 5192, loss = 979.86282666\n",
      "Iteration 5193, loss = 979.52394548\n",
      "Iteration 5194, loss = 979.17054042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5195, loss = 978.86645402\n",
      "Iteration 5196, loss = 978.45682835\n",
      "Iteration 5197, loss = 978.16967524\n",
      "Iteration 5198, loss = 977.76312916\n",
      "Iteration 5199, loss = 977.41711113\n",
      "Iteration 5200, loss = 977.10009285\n",
      "Iteration 5201, loss = 976.79469991\n",
      "Iteration 5202, loss = 976.36859125\n",
      "Iteration 5203, loss = 976.01589741\n",
      "Iteration 5204, loss = 975.67240849\n",
      "Iteration 5205, loss = 975.34755830\n",
      "Iteration 5206, loss = 974.99465133\n",
      "Iteration 5207, loss = 974.62681452\n",
      "Iteration 5208, loss = 974.32790242\n",
      "Iteration 5209, loss = 973.94557697\n",
      "Iteration 5210, loss = 973.59443957\n",
      "Iteration 5211, loss = 973.26866767\n",
      "Iteration 5212, loss = 972.91328679\n",
      "Iteration 5213, loss = 972.55706803\n",
      "Iteration 5214, loss = 972.21558970\n",
      "Iteration 5215, loss = 971.86940125\n",
      "Iteration 5216, loss = 971.52236794\n",
      "Iteration 5217, loss = 971.20796411\n",
      "Iteration 5218, loss = 970.84389667\n",
      "Iteration 5219, loss = 970.45308388\n",
      "Iteration 5220, loss = 970.18227714\n",
      "Iteration 5221, loss = 969.77732488\n",
      "Iteration 5222, loss = 969.51541228\n",
      "Iteration 5223, loss = 969.14127020\n",
      "Iteration 5224, loss = 968.75228703\n",
      "Iteration 5225, loss = 968.40994382\n",
      "Iteration 5226, loss = 968.05344641\n",
      "Iteration 5227, loss = 967.70057146\n",
      "Iteration 5228, loss = 967.36053234\n",
      "Iteration 5229, loss = 967.03614392\n",
      "Iteration 5230, loss = 966.86024948\n",
      "Iteration 5231, loss = 966.33799019\n",
      "Iteration 5232, loss = 966.01927699\n",
      "Iteration 5233, loss = 965.69877534\n",
      "Iteration 5234, loss = 965.30578536\n",
      "Iteration 5235, loss = 964.98739022\n",
      "Iteration 5236, loss = 964.61710984\n",
      "Iteration 5237, loss = 964.34051633\n",
      "Iteration 5238, loss = 963.99857879\n",
      "Iteration 5239, loss = 963.61495158\n",
      "Iteration 5240, loss = 963.29609801\n",
      "Iteration 5241, loss = 962.90850103\n",
      "Iteration 5242, loss = 962.59536212\n",
      "Iteration 5243, loss = 962.24598738\n",
      "Iteration 5244, loss = 961.94127670\n",
      "Iteration 5245, loss = 961.60143993\n",
      "Iteration 5246, loss = 961.21320042\n",
      "Iteration 5247, loss = 960.87153082\n",
      "Iteration 5248, loss = 960.57367489\n",
      "Iteration 5249, loss = 960.20480168\n",
      "Iteration 5250, loss = 959.92152505\n",
      "Iteration 5251, loss = 959.56112737\n",
      "Iteration 5252, loss = 959.20473798\n",
      "Iteration 5253, loss = 958.82139901\n",
      "Iteration 5254, loss = 958.54430562\n",
      "Iteration 5255, loss = 958.18158749\n",
      "Iteration 5256, loss = 957.81617663\n",
      "Iteration 5257, loss = 957.47192431\n",
      "Iteration 5258, loss = 957.12155495\n",
      "Iteration 5259, loss = 956.79051040\n",
      "Iteration 5260, loss = 956.50892046\n",
      "Iteration 5261, loss = 956.21743387\n",
      "Iteration 5262, loss = 955.85832010\n",
      "Iteration 5263, loss = 955.53245733\n",
      "Iteration 5264, loss = 955.09262923\n",
      "Iteration 5265, loss = 954.76320426\n",
      "Iteration 5266, loss = 954.44649690\n",
      "Iteration 5267, loss = 954.20405465\n",
      "Iteration 5268, loss = 953.76921196\n",
      "Iteration 5269, loss = 953.42296889\n",
      "Iteration 5270, loss = 953.12072365\n",
      "Iteration 5271, loss = 952.73904771\n",
      "Iteration 5272, loss = 952.48275245\n",
      "Iteration 5273, loss = 952.09561239\n",
      "Iteration 5274, loss = 951.73607726\n",
      "Iteration 5275, loss = 951.40404427\n",
      "Iteration 5276, loss = 951.06170357\n",
      "Iteration 5277, loss = 950.74051286\n",
      "Iteration 5278, loss = 950.42310225\n",
      "Iteration 5279, loss = 950.07495947\n",
      "Iteration 5280, loss = 949.73121650\n",
      "Iteration 5281, loss = 949.39445672\n",
      "Iteration 5282, loss = 949.04486332\n",
      "Iteration 5283, loss = 948.73615235\n",
      "Iteration 5284, loss = 948.43525566\n",
      "Iteration 5285, loss = 948.17313105\n",
      "Iteration 5286, loss = 947.75576182\n",
      "Iteration 5287, loss = 947.37860100\n",
      "Iteration 5288, loss = 947.11428966\n",
      "Iteration 5289, loss = 946.74550486\n",
      "Iteration 5290, loss = 946.38798653\n",
      "Iteration 5291, loss = 946.08907694\n",
      "Iteration 5292, loss = 945.87508725\n",
      "Iteration 5293, loss = 945.46261663\n",
      "Iteration 5294, loss = 945.07053854\n",
      "Iteration 5295, loss = 944.76241259\n",
      "Iteration 5296, loss = 944.46119986\n",
      "Iteration 5297, loss = 944.07307923\n",
      "Iteration 5298, loss = 943.75057897\n",
      "Iteration 5299, loss = 943.42146714\n",
      "Iteration 5300, loss = 943.09106480\n",
      "Iteration 5301, loss = 942.80919454\n",
      "Iteration 5302, loss = 942.44515058\n",
      "Iteration 5303, loss = 942.11603660\n",
      "Iteration 5304, loss = 941.75529338\n",
      "Iteration 5305, loss = 941.46338813\n",
      "Iteration 5306, loss = 941.10427080\n",
      "Iteration 5307, loss = 940.77912869\n",
      "Iteration 5308, loss = 940.58118803\n",
      "Iteration 5309, loss = 940.21981868\n",
      "Iteration 5310, loss = 939.89977564\n",
      "Iteration 5311, loss = 939.46085795\n",
      "Iteration 5312, loss = 939.11785067\n",
      "Iteration 5313, loss = 938.82425598\n",
      "Iteration 5314, loss = 938.45650558\n",
      "Iteration 5315, loss = 938.12335383\n",
      "Iteration 5316, loss = 937.83927637\n",
      "Iteration 5317, loss = 937.49026328\n",
      "Iteration 5318, loss = 937.16203659\n",
      "Iteration 5319, loss = 936.84092771\n",
      "Iteration 5320, loss = 936.49306909\n",
      "Iteration 5321, loss = 936.16649982\n",
      "Iteration 5322, loss = 935.84887453\n",
      "Iteration 5323, loss = 935.56495199\n",
      "Iteration 5324, loss = 935.17904581\n",
      "Iteration 5325, loss = 934.89759097\n",
      "Iteration 5326, loss = 934.53006458\n",
      "Iteration 5327, loss = 934.23274009\n",
      "Iteration 5328, loss = 933.88295910\n",
      "Iteration 5329, loss = 933.64600876\n",
      "Iteration 5330, loss = 933.29813979\n",
      "Iteration 5331, loss = 932.91803193\n",
      "Iteration 5332, loss = 932.56368027\n",
      "Iteration 5333, loss = 932.30830692\n",
      "Iteration 5334, loss = 931.94962817\n",
      "Iteration 5335, loss = 931.63333549\n",
      "Iteration 5336, loss = 931.32099967\n",
      "Iteration 5337, loss = 931.06006093\n",
      "Iteration 5338, loss = 930.64276243\n",
      "Iteration 5339, loss = 930.29236755\n",
      "Iteration 5340, loss = 930.00425764\n",
      "Iteration 5341, loss = 929.65270147\n",
      "Iteration 5342, loss = 929.37608885\n",
      "Iteration 5343, loss = 928.99798531\n",
      "Iteration 5344, loss = 928.71030780\n",
      "Iteration 5345, loss = 928.36003733\n",
      "Iteration 5346, loss = 928.03635150\n",
      "Iteration 5347, loss = 927.72696892\n",
      "Iteration 5348, loss = 927.39381645\n",
      "Iteration 5349, loss = 927.08058132\n",
      "Iteration 5350, loss = 926.80024443\n",
      "Iteration 5351, loss = 926.42069120\n",
      "Iteration 5352, loss = 926.09726890\n",
      "Iteration 5353, loss = 925.78653328\n",
      "Iteration 5354, loss = 925.46542570\n",
      "Iteration 5355, loss = 925.14974186\n",
      "Iteration 5356, loss = 924.87825834\n",
      "Iteration 5357, loss = 924.62939548\n",
      "Iteration 5358, loss = 924.28763379\n",
      "Iteration 5359, loss = 923.86613394\n",
      "Iteration 5360, loss = 923.54154846\n",
      "Iteration 5361, loss = 923.22477672\n",
      "Iteration 5362, loss = 922.91384273\n",
      "Iteration 5363, loss = 922.56508676\n",
      "Iteration 5364, loss = 922.31182767\n",
      "Iteration 5365, loss = 921.91915951\n",
      "Iteration 5366, loss = 921.61315339\n",
      "Iteration 5367, loss = 921.30551346\n",
      "Iteration 5368, loss = 921.00481508\n",
      "Iteration 5369, loss = 920.66373807\n",
      "Iteration 5370, loss = 920.32359856\n",
      "Iteration 5371, loss = 920.06853257\n",
      "Iteration 5372, loss = 919.69900418\n",
      "Iteration 5373, loss = 919.37160588\n",
      "Iteration 5374, loss = 919.13575348\n",
      "Iteration 5375, loss = 918.77844719\n",
      "Iteration 5376, loss = 918.43203328\n",
      "Iteration 5377, loss = 918.10284348\n",
      "Iteration 5378, loss = 917.79920009\n",
      "Iteration 5379, loss = 917.47917592\n",
      "Iteration 5380, loss = 917.16907835\n",
      "Iteration 5381, loss = 916.86199294\n",
      "Iteration 5382, loss = 916.53915728\n",
      "Iteration 5383, loss = 916.19120810\n",
      "Iteration 5384, loss = 915.89068088\n",
      "Iteration 5385, loss = 915.56335230\n",
      "Iteration 5386, loss = 915.25454623\n",
      "Iteration 5387, loss = 915.01051974\n",
      "Iteration 5388, loss = 914.59899876\n",
      "Iteration 5389, loss = 914.29687651\n",
      "Iteration 5390, loss = 913.99441119\n",
      "Iteration 5391, loss = 913.68670588\n",
      "Iteration 5392, loss = 913.35459046\n",
      "Iteration 5393, loss = 913.14562311\n",
      "Iteration 5394, loss = 912.79229179\n",
      "Iteration 5395, loss = 912.39564060\n",
      "Iteration 5396, loss = 912.28420339\n",
      "Iteration 5397, loss = 911.76806022\n",
      "Iteration 5398, loss = 911.45212182\n",
      "Iteration 5399, loss = 911.19717744\n",
      "Iteration 5400, loss = 910.83591519\n",
      "Iteration 5401, loss = 910.52240247\n",
      "Iteration 5402, loss = 910.32690454\n",
      "Iteration 5403, loss = 909.93645298\n",
      "Iteration 5404, loss = 909.65683065\n",
      "Iteration 5405, loss = 909.27115678\n",
      "Iteration 5406, loss = 908.94386076\n",
      "Iteration 5407, loss = 908.65333666\n",
      "Iteration 5408, loss = 908.30089971\n",
      "Iteration 5409, loss = 908.00286983\n",
      "Iteration 5410, loss = 907.70859302\n",
      "Iteration 5411, loss = 907.37606116\n",
      "Iteration 5412, loss = 907.07317851\n",
      "Iteration 5413, loss = 906.79931095\n",
      "Iteration 5414, loss = 906.47356357\n",
      "Iteration 5415, loss = 906.15293972\n",
      "Iteration 5416, loss = 905.82966681\n",
      "Iteration 5417, loss = 905.49497066\n",
      "Iteration 5418, loss = 905.22520538\n",
      "Iteration 5419, loss = 904.89884932\n",
      "Iteration 5420, loss = 904.57872110\n",
      "Iteration 5421, loss = 904.26236347\n",
      "Iteration 5422, loss = 903.94044085\n",
      "Iteration 5423, loss = 903.63911264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5424, loss = 903.30653184\n",
      "Iteration 5425, loss = 903.00991940\n",
      "Iteration 5426, loss = 902.73628504\n",
      "Iteration 5427, loss = 902.42290602\n",
      "Iteration 5428, loss = 902.09052898\n",
      "Iteration 5429, loss = 901.76677460\n",
      "Iteration 5430, loss = 901.53156855\n",
      "Iteration 5431, loss = 901.21868532\n",
      "Iteration 5432, loss = 900.82295011\n",
      "Iteration 5433, loss = 900.54625179\n",
      "Iteration 5434, loss = 900.27637646\n",
      "Iteration 5435, loss = 899.91878102\n",
      "Iteration 5436, loss = 899.73001757\n",
      "Iteration 5437, loss = 899.39679745\n",
      "Iteration 5438, loss = 898.98432401\n",
      "Iteration 5439, loss = 898.70455825\n",
      "Iteration 5440, loss = 898.42274276\n",
      "Iteration 5441, loss = 898.07281343\n",
      "Iteration 5442, loss = 897.76710889\n",
      "Iteration 5443, loss = 897.47794220\n",
      "Iteration 5444, loss = 897.15381263\n",
      "Iteration 5445, loss = 896.82839518\n",
      "Iteration 5446, loss = 896.52729274\n",
      "Iteration 5447, loss = 896.21399114\n",
      "Iteration 5448, loss = 895.93337365\n",
      "Iteration 5449, loss = 895.64216240\n",
      "Iteration 5450, loss = 895.29315530\n",
      "Iteration 5451, loss = 894.98409942\n",
      "Iteration 5452, loss = 894.68429574\n",
      "Iteration 5453, loss = 894.39418702\n",
      "Iteration 5454, loss = 894.07189869\n",
      "Iteration 5455, loss = 893.78046509\n",
      "Iteration 5456, loss = 893.46844895\n",
      "Iteration 5457, loss = 893.20557687\n",
      "Iteration 5458, loss = 892.82336995\n",
      "Iteration 5459, loss = 892.53912862\n",
      "Iteration 5460, loss = 892.40712331\n",
      "Iteration 5461, loss = 891.92676624\n",
      "Iteration 5462, loss = 891.63101348\n",
      "Iteration 5463, loss = 891.35545798\n",
      "Iteration 5464, loss = 891.09102364\n",
      "Iteration 5465, loss = 890.73595538\n",
      "Iteration 5466, loss = 890.40592633\n",
      "Iteration 5467, loss = 890.11689009\n",
      "Iteration 5468, loss = 889.83597442\n",
      "Iteration 5469, loss = 889.50392682\n",
      "Iteration 5470, loss = 889.18334639\n",
      "Iteration 5471, loss = 888.88840133\n",
      "Iteration 5472, loss = 888.61436162\n",
      "Iteration 5473, loss = 888.31705210\n",
      "Iteration 5474, loss = 888.02995147\n",
      "Iteration 5475, loss = 887.67051202\n",
      "Iteration 5476, loss = 887.37145039\n",
      "Iteration 5477, loss = 887.13791522\n",
      "Iteration 5478, loss = 886.78001956\n",
      "Iteration 5479, loss = 886.45581794\n",
      "Iteration 5480, loss = 886.16211050\n",
      "Iteration 5481, loss = 885.86445585\n",
      "Iteration 5482, loss = 885.58524840\n",
      "Iteration 5483, loss = 885.27621112\n",
      "Iteration 5484, loss = 884.93386146\n",
      "Iteration 5485, loss = 884.77036203\n",
      "Iteration 5486, loss = 884.36750274\n",
      "Iteration 5487, loss = 884.05018992\n",
      "Iteration 5488, loss = 883.73120696\n",
      "Iteration 5489, loss = 883.45386103\n",
      "Iteration 5490, loss = 883.15941079\n",
      "Iteration 5491, loss = 882.84250909\n",
      "Iteration 5492, loss = 882.54528622\n",
      "Iteration 5493, loss = 882.28694527\n",
      "Iteration 5494, loss = 881.93602385\n",
      "Iteration 5495, loss = 881.67856649\n",
      "Iteration 5496, loss = 881.40195252\n",
      "Iteration 5497, loss = 881.09249675\n",
      "Iteration 5498, loss = 880.73161699\n",
      "Iteration 5499, loss = 880.44093678\n",
      "Iteration 5500, loss = 880.18806734\n",
      "Iteration 5501, loss = 879.88182872\n",
      "Iteration 5502, loss = 879.62048224\n",
      "Iteration 5503, loss = 879.29378850\n",
      "Iteration 5504, loss = 878.93954888\n",
      "Iteration 5505, loss = 878.71665309\n",
      "Iteration 5506, loss = 878.34264895\n",
      "Iteration 5507, loss = 878.06446960\n",
      "Iteration 5508, loss = 877.75944272\n",
      "Iteration 5509, loss = 877.45504722\n",
      "Iteration 5510, loss = 877.19123727\n",
      "Iteration 5511, loss = 876.85287875\n",
      "Iteration 5512, loss = 876.55044718\n",
      "Iteration 5513, loss = 876.29748048\n",
      "Iteration 5514, loss = 876.05595038\n",
      "Iteration 5515, loss = 875.71891160\n",
      "Iteration 5516, loss = 875.38778228\n",
      "Iteration 5517, loss = 875.06341292\n",
      "Iteration 5518, loss = 874.78995984\n",
      "Iteration 5519, loss = 874.51431793\n",
      "Iteration 5520, loss = 874.17258544\n",
      "Iteration 5521, loss = 873.88148090\n",
      "Iteration 5522, loss = 873.58424613\n",
      "Iteration 5523, loss = 873.39232560\n",
      "Iteration 5524, loss = 873.01683919\n",
      "Iteration 5525, loss = 872.70095231\n",
      "Iteration 5526, loss = 872.43721605\n",
      "Iteration 5527, loss = 872.17358513\n",
      "Iteration 5528, loss = 871.81281489\n",
      "Iteration 5529, loss = 871.51535609\n",
      "Iteration 5530, loss = 871.22514159\n",
      "Iteration 5531, loss = 870.95391883\n",
      "Iteration 5532, loss = 870.79455180\n",
      "Iteration 5533, loss = 870.34268332\n",
      "Iteration 5534, loss = 870.04217076\n",
      "Iteration 5535, loss = 869.80435846\n",
      "Iteration 5536, loss = 869.46885055\n",
      "Iteration 5537, loss = 869.14810125\n",
      "Iteration 5538, loss = 868.87464915\n",
      "Iteration 5539, loss = 868.60034206\n",
      "Iteration 5540, loss = 868.39280209\n",
      "Iteration 5541, loss = 868.01115507\n",
      "Iteration 5542, loss = 867.68124246\n",
      "Iteration 5543, loss = 867.51852658\n",
      "Iteration 5544, loss = 867.11378922\n",
      "Iteration 5545, loss = 866.80949915\n",
      "Iteration 5546, loss = 866.52243535\n",
      "Iteration 5547, loss = 866.22326458\n",
      "Iteration 5548, loss = 865.92833001\n",
      "Iteration 5549, loss = 865.75976122\n",
      "Iteration 5550, loss = 865.35550476\n",
      "Iteration 5551, loss = 865.12488407\n",
      "Iteration 5552, loss = 864.77875416\n",
      "Iteration 5553, loss = 864.47017131\n",
      "Iteration 5554, loss = 864.16219274\n",
      "Iteration 5555, loss = 863.91632450\n",
      "Iteration 5556, loss = 863.60350767\n",
      "Iteration 5557, loss = 863.30049861\n",
      "Iteration 5558, loss = 863.02660155\n",
      "Iteration 5559, loss = 862.71331689\n",
      "Iteration 5560, loss = 862.46040678\n",
      "Iteration 5561, loss = 862.18009962\n",
      "Iteration 5562, loss = 861.82975045\n",
      "Iteration 5563, loss = 861.54801939\n",
      "Iteration 5564, loss = 861.27093378\n",
      "Iteration 5565, loss = 861.02503120\n",
      "Iteration 5566, loss = 860.70065955\n",
      "Iteration 5567, loss = 860.43971412\n",
      "Iteration 5568, loss = 860.09451557\n",
      "Iteration 5569, loss = 859.85726001\n",
      "Iteration 5570, loss = 859.50614731\n",
      "Iteration 5571, loss = 859.23920952\n",
      "Iteration 5572, loss = 858.97367111\n",
      "Iteration 5573, loss = 858.66321482\n",
      "Iteration 5574, loss = 858.37835608\n",
      "Iteration 5575, loss = 858.11812615\n",
      "Iteration 5576, loss = 857.78961195\n",
      "Iteration 5577, loss = 857.54580409\n",
      "Iteration 5578, loss = 857.25938275\n",
      "Iteration 5579, loss = 856.93228921\n",
      "Iteration 5580, loss = 856.62312677\n",
      "Iteration 5581, loss = 856.33787874\n",
      "Iteration 5582, loss = 856.09660826\n",
      "Iteration 5583, loss = 856.03531051\n",
      "Iteration 5584, loss = 855.48086367\n",
      "Iteration 5585, loss = 855.19867752\n",
      "Iteration 5586, loss = 854.87720577\n",
      "Iteration 5587, loss = 854.59986826\n",
      "Iteration 5588, loss = 854.31695707\n",
      "Iteration 5589, loss = 854.08099220\n",
      "Iteration 5590, loss = 853.76787617\n",
      "Iteration 5591, loss = 853.43554837\n",
      "Iteration 5592, loss = 853.16666999\n",
      "Iteration 5593, loss = 852.93675905\n",
      "Iteration 5594, loss = 852.69134515\n",
      "Iteration 5595, loss = 852.31415741\n",
      "Iteration 5596, loss = 852.02772065\n",
      "Iteration 5597, loss = 851.81752674\n",
      "Iteration 5598, loss = 851.56812952\n",
      "Iteration 5599, loss = 851.23010557\n",
      "Iteration 5600, loss = 850.89468813\n",
      "Iteration 5601, loss = 850.63019112\n",
      "Iteration 5602, loss = 850.34873287\n",
      "Iteration 5603, loss = 850.02045433\n",
      "Iteration 5604, loss = 849.73205768\n",
      "Iteration 5605, loss = 849.50385562\n",
      "Iteration 5606, loss = 849.18407930\n",
      "Iteration 5607, loss = 848.87211295\n",
      "Iteration 5608, loss = 848.60103139\n",
      "Iteration 5609, loss = 848.32566659\n",
      "Iteration 5610, loss = 848.06461265\n",
      "Iteration 5611, loss = 847.90531400\n",
      "Iteration 5612, loss = 847.61449470\n",
      "Iteration 5613, loss = 847.17609851\n",
      "Iteration 5614, loss = 846.88123873\n",
      "Iteration 5615, loss = 846.69989404\n",
      "Iteration 5616, loss = 846.31401237\n",
      "Iteration 5617, loss = 846.03269130\n",
      "Iteration 5618, loss = 845.78724504\n",
      "Iteration 5619, loss = 845.50047408\n",
      "Iteration 5620, loss = 845.18847727\n",
      "Iteration 5621, loss = 844.91929892\n",
      "Iteration 5622, loss = 844.79334480\n",
      "Iteration 5623, loss = 844.30048996\n",
      "Iteration 5624, loss = 844.04046789\n",
      "Iteration 5625, loss = 843.80038978\n",
      "Iteration 5626, loss = 843.48204031\n",
      "Iteration 5627, loss = 843.24983507\n",
      "Iteration 5628, loss = 842.93618823\n",
      "Iteration 5629, loss = 842.63099118\n",
      "Iteration 5630, loss = 842.33346048\n",
      "Iteration 5631, loss = 842.11511577\n",
      "Iteration 5632, loss = 841.84585196\n",
      "Iteration 5633, loss = 841.51251399\n",
      "Iteration 5634, loss = 841.25339799\n",
      "Iteration 5635, loss = 840.97660816\n",
      "Iteration 5636, loss = 840.68468458\n",
      "Iteration 5637, loss = 840.38509826\n",
      "Iteration 5638, loss = 840.11788386\n",
      "Iteration 5639, loss = 839.84753289\n",
      "Iteration 5640, loss = 839.54754024\n",
      "Iteration 5641, loss = 839.24757503\n",
      "Iteration 5642, loss = 838.97794993\n",
      "Iteration 5643, loss = 838.69676226\n",
      "Iteration 5644, loss = 838.43432979\n",
      "Iteration 5645, loss = 838.11436327\n",
      "Iteration 5646, loss = 837.85686297\n",
      "Iteration 5647, loss = 837.55039936\n",
      "Iteration 5648, loss = 837.29590921\n",
      "Iteration 5649, loss = 837.00818065\n",
      "Iteration 5650, loss = 836.72261425\n",
      "Iteration 5651, loss = 836.44466529\n",
      "Iteration 5652, loss = 836.18866554\n",
      "Iteration 5653, loss = 835.88732267\n",
      "Iteration 5654, loss = 835.61679168\n",
      "Iteration 5655, loss = 835.32968102\n",
      "Iteration 5656, loss = 835.02178382\n",
      "Iteration 5657, loss = 834.80497357\n",
      "Iteration 5658, loss = 834.51488644\n",
      "Iteration 5659, loss = 834.25597340\n",
      "Iteration 5660, loss = 833.95919977\n",
      "Iteration 5661, loss = 833.64206643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5662, loss = 833.40770402\n",
      "Iteration 5663, loss = 833.09771690\n",
      "Iteration 5664, loss = 832.81990046\n",
      "Iteration 5665, loss = 832.63450428\n",
      "Iteration 5666, loss = 832.25121762\n",
      "Iteration 5667, loss = 831.98347502\n",
      "Iteration 5668, loss = 831.69733975\n",
      "Iteration 5669, loss = 831.47943371\n",
      "Iteration 5670, loss = 831.21664033\n",
      "Iteration 5671, loss = 830.90532172\n",
      "Iteration 5672, loss = 830.57971079\n",
      "Iteration 5673, loss = 830.32176512\n",
      "Iteration 5674, loss = 830.06059339\n",
      "Iteration 5675, loss = 829.83142092\n",
      "Iteration 5676, loss = 829.50100734\n",
      "Iteration 5677, loss = 829.26007808\n",
      "Iteration 5678, loss = 828.92532557\n",
      "Iteration 5679, loss = 828.63564439\n",
      "Iteration 5680, loss = 828.40627716\n",
      "Iteration 5681, loss = 828.10652018\n",
      "Iteration 5682, loss = 827.82916798\n",
      "Iteration 5683, loss = 827.61508541\n",
      "Iteration 5684, loss = 827.25522573\n",
      "Iteration 5685, loss = 826.98549859\n",
      "Iteration 5686, loss = 826.77682321\n",
      "Iteration 5687, loss = 826.43224142\n",
      "Iteration 5688, loss = 826.18158975\n",
      "Iteration 5689, loss = 825.91250628\n",
      "Iteration 5690, loss = 825.60312729\n",
      "Iteration 5691, loss = 825.44547021\n",
      "Iteration 5692, loss = 825.17263735\n",
      "Iteration 5693, loss = 824.76437712\n",
      "Iteration 5694, loss = 824.52991404\n",
      "Iteration 5695, loss = 824.24774030\n",
      "Iteration 5696, loss = 823.99410151\n",
      "Iteration 5697, loss = 823.69325571\n",
      "Iteration 5698, loss = 823.41135717\n",
      "Iteration 5699, loss = 823.11862298\n",
      "Iteration 5700, loss = 822.84765454\n",
      "Iteration 5701, loss = 822.57379554\n",
      "Iteration 5702, loss = 822.29411513\n",
      "Iteration 5703, loss = 822.01613551\n",
      "Iteration 5704, loss = 821.74846943\n",
      "Iteration 5705, loss = 821.51930534\n",
      "Iteration 5706, loss = 821.21470121\n",
      "Iteration 5707, loss = 820.92392839\n",
      "Iteration 5708, loss = 820.66552712\n",
      "Iteration 5709, loss = 820.36525067\n",
      "Iteration 5710, loss = 820.10534589\n",
      "Iteration 5711, loss = 819.85127674\n",
      "Iteration 5712, loss = 819.59216038\n",
      "Iteration 5713, loss = 819.29130893\n",
      "Iteration 5714, loss = 819.01371686\n",
      "Iteration 5715, loss = 818.75609250\n",
      "Iteration 5716, loss = 818.55643269\n",
      "Iteration 5717, loss = 818.20038352\n",
      "Iteration 5718, loss = 817.88809837\n",
      "Iteration 5719, loss = 817.63263372\n",
      "Iteration 5720, loss = 817.38701108\n",
      "Iteration 5721, loss = 817.21837192\n",
      "Iteration 5722, loss = 816.81202188\n",
      "Iteration 5723, loss = 816.63660989\n",
      "Iteration 5724, loss = 816.27914981\n",
      "Iteration 5725, loss = 815.99891666\n",
      "Iteration 5726, loss = 815.78039484\n",
      "Iteration 5727, loss = 815.50418570\n",
      "Iteration 5728, loss = 815.30277188\n",
      "Iteration 5729, loss = 815.11633076\n",
      "Iteration 5730, loss = 814.80103793\n",
      "Iteration 5731, loss = 814.36365488\n",
      "Iteration 5732, loss = 814.07724490\n",
      "Iteration 5733, loss = 813.81874072\n",
      "Iteration 5734, loss = 813.58788244\n",
      "Iteration 5735, loss = 813.28135521\n",
      "Iteration 5736, loss = 813.03946949\n",
      "Iteration 5737, loss = 812.71982402\n",
      "Iteration 5738, loss = 812.50984461\n",
      "Iteration 5739, loss = 812.18381940\n",
      "Iteration 5740, loss = 811.95871160\n",
      "Iteration 5741, loss = 811.64091555\n",
      "Iteration 5742, loss = 811.38663796\n",
      "Iteration 5743, loss = 811.11750463\n",
      "Iteration 5744, loss = 810.89469929\n",
      "Iteration 5745, loss = 810.55405741\n",
      "Iteration 5746, loss = 810.28186810\n",
      "Iteration 5747, loss = 810.02015875\n",
      "Iteration 5748, loss = 809.75362875\n",
      "Iteration 5749, loss = 809.53146575\n",
      "Iteration 5750, loss = 809.22370503\n",
      "Iteration 5751, loss = 808.95845909\n",
      "Iteration 5752, loss = 808.94654255\n",
      "Iteration 5753, loss = 808.44462483\n",
      "Iteration 5754, loss = 808.16145008\n",
      "Iteration 5755, loss = 807.85215194\n",
      "Iteration 5756, loss = 807.59894690\n",
      "Iteration 5757, loss = 807.37604404\n",
      "Iteration 5758, loss = 807.04688028\n",
      "Iteration 5759, loss = 806.76280615\n",
      "Iteration 5760, loss = 806.50148316\n",
      "Iteration 5761, loss = 806.24132566\n",
      "Iteration 5762, loss = 805.99186052\n",
      "Iteration 5763, loss = 805.71272774\n",
      "Iteration 5764, loss = 805.43627087\n",
      "Iteration 5765, loss = 805.14302167\n",
      "Iteration 5766, loss = 804.94438254\n",
      "Iteration 5767, loss = 804.69245839\n",
      "Iteration 5768, loss = 804.38124200\n",
      "Iteration 5769, loss = 804.07630944\n",
      "Iteration 5770, loss = 803.95271503\n",
      "Iteration 5771, loss = 803.52285163\n",
      "Iteration 5772, loss = 803.41200667\n",
      "Iteration 5773, loss = 803.11870828\n",
      "Iteration 5774, loss = 802.73229188\n",
      "Iteration 5775, loss = 802.45331956\n",
      "Iteration 5776, loss = 802.20196007\n",
      "Iteration 5777, loss = 801.92241608\n",
      "Iteration 5778, loss = 801.66150377\n",
      "Iteration 5779, loss = 801.37658295\n",
      "Iteration 5780, loss = 801.10169478\n",
      "Iteration 5781, loss = 800.87557176\n",
      "Iteration 5782, loss = 800.55582661\n",
      "Iteration 5783, loss = 800.30457562\n",
      "Iteration 5784, loss = 800.05926914\n",
      "Iteration 5785, loss = 799.80484439\n",
      "Iteration 5786, loss = 799.62507133\n",
      "Iteration 5787, loss = 799.27166016\n",
      "Iteration 5788, loss = 798.94188118\n",
      "Iteration 5789, loss = 798.71363577\n",
      "Iteration 5790, loss = 798.46086296\n",
      "Iteration 5791, loss = 798.15492233\n",
      "Iteration 5792, loss = 797.87454306\n",
      "Iteration 5793, loss = 797.62709578\n",
      "Iteration 5794, loss = 797.32729539\n",
      "Iteration 5795, loss = 797.07082291\n",
      "Iteration 5796, loss = 796.91530079\n",
      "Iteration 5797, loss = 796.51903581\n",
      "Iteration 5798, loss = 796.29213958\n",
      "Iteration 5799, loss = 796.11494221\n",
      "Iteration 5800, loss = 795.73570739\n",
      "Iteration 5801, loss = 795.45296445\n",
      "Iteration 5802, loss = 795.17146420\n",
      "Iteration 5803, loss = 795.05729092\n",
      "Iteration 5804, loss = 794.65225857\n",
      "Iteration 5805, loss = 794.39801631\n",
      "Iteration 5806, loss = 794.11755807\n",
      "Iteration 5807, loss = 793.87471656\n",
      "Iteration 5808, loss = 793.56267026\n",
      "Iteration 5809, loss = 793.30494674\n",
      "Iteration 5810, loss = 793.01625029\n",
      "Iteration 5811, loss = 792.78299005\n",
      "Iteration 5812, loss = 792.48304870\n",
      "Iteration 5813, loss = 792.24691796\n",
      "Iteration 5814, loss = 791.98818785\n",
      "Iteration 5815, loss = 791.68787476\n",
      "Iteration 5816, loss = 791.46630159\n",
      "Iteration 5817, loss = 791.11173884\n",
      "Iteration 5818, loss = 790.84226025\n",
      "Iteration 5819, loss = 790.60051948\n",
      "Iteration 5820, loss = 790.29950871\n",
      "Iteration 5821, loss = 790.06097013\n",
      "Iteration 5822, loss = 789.75078867\n",
      "Iteration 5823, loss = 789.47696286\n",
      "Iteration 5824, loss = 789.21768723\n",
      "Iteration 5825, loss = 788.94301383\n",
      "Iteration 5826, loss = 788.65018576\n",
      "Iteration 5827, loss = 788.38431323\n",
      "Iteration 5828, loss = 788.10755021\n",
      "Iteration 5829, loss = 787.84230310\n",
      "Iteration 5830, loss = 787.66338863\n",
      "Iteration 5831, loss = 787.27391498\n",
      "Iteration 5832, loss = 786.99195493\n",
      "Iteration 5833, loss = 786.70420375\n",
      "Iteration 5834, loss = 786.48314864\n",
      "Iteration 5835, loss = 786.21569386\n",
      "Iteration 5836, loss = 785.90145921\n",
      "Iteration 5837, loss = 785.56810733\n",
      "Iteration 5838, loss = 785.32235024\n",
      "Iteration 5839, loss = 785.00077597\n",
      "Iteration 5840, loss = 784.71185183\n",
      "Iteration 5841, loss = 784.53866204\n",
      "Iteration 5842, loss = 784.24704526\n",
      "Iteration 5843, loss = 783.83581313\n",
      "Iteration 5844, loss = 783.54695982\n",
      "Iteration 5845, loss = 783.23279267\n",
      "Iteration 5846, loss = 782.96777657\n",
      "Iteration 5847, loss = 782.66281018\n",
      "Iteration 5848, loss = 782.34079852\n",
      "Iteration 5849, loss = 782.00253487\n",
      "Iteration 5850, loss = 781.70634344\n",
      "Iteration 5851, loss = 781.35118608\n",
      "Iteration 5852, loss = 781.01572783\n",
      "Iteration 5853, loss = 780.71526311\n",
      "Iteration 5854, loss = 780.37223497\n",
      "Iteration 5855, loss = 780.00260432\n",
      "Iteration 5856, loss = 779.65105541\n",
      "Iteration 5857, loss = 779.27275670\n",
      "Iteration 5858, loss = 778.87315696\n",
      "Iteration 5859, loss = 778.48560258\n",
      "Iteration 5860, loss = 778.07228070\n",
      "Iteration 5861, loss = 777.61471931\n",
      "Iteration 5862, loss = 777.17910991\n",
      "Iteration 5863, loss = 776.81835138\n",
      "Iteration 5864, loss = 776.20909611\n",
      "Iteration 5865, loss = 775.71540387\n",
      "Iteration 5866, loss = 775.13395045\n",
      "Iteration 5867, loss = 774.57556648\n",
      "Iteration 5868, loss = 773.94389133\n",
      "Iteration 5869, loss = 773.28768413\n",
      "Iteration 5870, loss = 772.61996829\n",
      "Iteration 5871, loss = 771.95554174\n",
      "Iteration 5872, loss = 771.15851750\n",
      "Iteration 5873, loss = 770.36198428\n",
      "Iteration 5874, loss = 769.60144362\n",
      "Iteration 5875, loss = 768.70590975\n",
      "Iteration 5876, loss = 767.92458331\n",
      "Iteration 5877, loss = 767.03322367\n",
      "Iteration 5878, loss = 766.05732679\n",
      "Iteration 5879, loss = 765.17118073\n",
      "Iteration 5880, loss = 764.30271249\n",
      "Iteration 5881, loss = 763.47673428\n",
      "Iteration 5882, loss = 762.45161096\n",
      "Iteration 5883, loss = 761.55758720\n",
      "Iteration 5884, loss = 760.67381752\n",
      "Iteration 5885, loss = 759.75059896\n",
      "Iteration 5886, loss = 758.85724913\n",
      "Iteration 5887, loss = 757.98755276\n",
      "Iteration 5888, loss = 757.11587761\n",
      "Iteration 5889, loss = 756.28924645\n",
      "Iteration 5890, loss = 755.44463751\n",
      "Iteration 5891, loss = 754.62847904\n",
      "Iteration 5892, loss = 753.82683724\n",
      "Iteration 5893, loss = 753.08927595\n",
      "Iteration 5894, loss = 752.25723585\n",
      "Iteration 5895, loss = 751.50625776\n",
      "Iteration 5896, loss = 750.73379152\n",
      "Iteration 5897, loss = 749.98750591\n",
      "Iteration 5898, loss = 749.24856374\n",
      "Iteration 5899, loss = 748.60555097\n",
      "Iteration 5900, loss = 747.81048720\n",
      "Iteration 5901, loss = 747.05518069\n",
      "Iteration 5902, loss = 746.42523384\n",
      "Iteration 5903, loss = 745.69498082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5904, loss = 744.99994229\n",
      "Iteration 5905, loss = 744.34316729\n",
      "Iteration 5906, loss = 743.61419583\n",
      "Iteration 5907, loss = 742.99922069\n",
      "Iteration 5908, loss = 742.29752656\n",
      "Iteration 5909, loss = 741.73181303\n",
      "Iteration 5910, loss = 741.02538726\n",
      "Iteration 5911, loss = 740.35522527\n",
      "Iteration 5912, loss = 739.72837760\n",
      "Iteration 5913, loss = 739.07560727\n",
      "Iteration 5914, loss = 738.46029464\n",
      "Iteration 5915, loss = 737.83666780\n",
      "Iteration 5916, loss = 737.22872387\n",
      "Iteration 5917, loss = 736.61871694\n",
      "Iteration 5918, loss = 736.00528739\n",
      "Iteration 5919, loss = 735.44131664\n",
      "Iteration 5920, loss = 734.82342139\n",
      "Iteration 5921, loss = 734.34998321\n",
      "Iteration 5922, loss = 733.61048682\n",
      "Iteration 5923, loss = 733.04290282\n",
      "Iteration 5924, loss = 732.43906223\n",
      "Iteration 5925, loss = 731.86129401\n",
      "Iteration 5926, loss = 731.29628217\n",
      "Iteration 5927, loss = 730.72123156\n",
      "Iteration 5928, loss = 730.16897115\n",
      "Iteration 5929, loss = 729.60524995\n",
      "Iteration 5930, loss = 729.02244266\n",
      "Iteration 5931, loss = 728.49448010\n",
      "Iteration 5932, loss = 727.91853206\n",
      "Iteration 5933, loss = 727.35871809\n",
      "Iteration 5934, loss = 726.86350222\n",
      "Iteration 5935, loss = 726.29878420\n",
      "Iteration 5936, loss = 725.71025916\n",
      "Iteration 5937, loss = 725.16558761\n",
      "Iteration 5938, loss = 724.62322480\n",
      "Iteration 5939, loss = 724.11599214\n",
      "Iteration 5940, loss = 723.59748621\n",
      "Iteration 5941, loss = 723.03974328\n",
      "Iteration 5942, loss = 722.48914386\n",
      "Iteration 5943, loss = 722.00523361\n",
      "Iteration 5944, loss = 721.52185675\n",
      "Iteration 5945, loss = 720.97473085\n",
      "Iteration 5946, loss = 720.44334975\n",
      "Iteration 5947, loss = 719.88962424\n",
      "Iteration 5948, loss = 719.38351602\n",
      "Iteration 5949, loss = 718.86199826\n",
      "Iteration 5950, loss = 718.34590557\n",
      "Iteration 5951, loss = 717.84659892\n",
      "Iteration 5952, loss = 717.33140467\n",
      "Iteration 5953, loss = 716.82213170\n",
      "Iteration 5954, loss = 716.33596682\n",
      "Iteration 5955, loss = 715.84611676\n",
      "Iteration 5956, loss = 715.34271743\n",
      "Iteration 5957, loss = 714.82698875\n",
      "Iteration 5958, loss = 714.34054892\n",
      "Iteration 5959, loss = 713.85494781\n",
      "Iteration 5960, loss = 713.37924195\n",
      "Iteration 5961, loss = 712.86881076\n",
      "Iteration 5962, loss = 712.36452189\n",
      "Iteration 5963, loss = 711.87733110\n",
      "Iteration 5964, loss = 711.40095290\n",
      "Iteration 5965, loss = 710.95648910\n",
      "Iteration 5966, loss = 710.46659205\n",
      "Iteration 5967, loss = 709.95431662\n",
      "Iteration 5968, loss = 709.48200218\n",
      "Iteration 5969, loss = 709.03454016\n",
      "Iteration 5970, loss = 708.52033371\n",
      "Iteration 5971, loss = 708.13501637\n",
      "Iteration 5972, loss = 707.66395339\n",
      "Iteration 5973, loss = 707.12446390\n",
      "Iteration 5974, loss = 706.65471209\n",
      "Iteration 5975, loss = 706.16762014\n",
      "Iteration 5976, loss = 705.70257054\n",
      "Iteration 5977, loss = 705.23197364\n",
      "Iteration 5978, loss = 704.81858815\n",
      "Iteration 5979, loss = 704.33298639\n",
      "Iteration 5980, loss = 703.85694181\n",
      "Iteration 5981, loss = 703.39860669\n",
      "Iteration 5982, loss = 702.95832012\n",
      "Iteration 5983, loss = 702.53104480\n",
      "Iteration 5984, loss = 702.08375675\n",
      "Iteration 5985, loss = 701.58119939\n",
      "Iteration 5986, loss = 701.12193869\n",
      "Iteration 5987, loss = 700.67098893\n",
      "Iteration 5988, loss = 700.22565949\n",
      "Iteration 5989, loss = 699.81802374\n",
      "Iteration 5990, loss = 699.32150855\n",
      "Iteration 5991, loss = 698.86678931\n",
      "Iteration 5992, loss = 698.45140998\n",
      "Iteration 5993, loss = 697.98080326\n",
      "Iteration 5994, loss = 697.53632111\n",
      "Iteration 5995, loss = 697.13612600\n",
      "Iteration 5996, loss = 696.68120243\n",
      "Iteration 5997, loss = 696.22214342\n",
      "Iteration 5998, loss = 695.80651742\n",
      "Iteration 5999, loss = 695.37551873\n",
      "Iteration 6000, loss = 694.90218732\n",
      "Iteration 6001, loss = 694.46628936\n",
      "Iteration 6002, loss = 694.02848378\n",
      "Iteration 6003, loss = 693.60063234\n",
      "Iteration 6004, loss = 693.17117908\n",
      "Iteration 6005, loss = 692.73831893\n",
      "Iteration 6006, loss = 692.29655218\n",
      "Iteration 6007, loss = 691.88397740\n",
      "Iteration 6008, loss = 691.45199463\n",
      "Iteration 6009, loss = 691.05068193\n",
      "Iteration 6010, loss = 690.61070686\n",
      "Iteration 6011, loss = 690.16560356\n",
      "Iteration 6012, loss = 689.74462229\n",
      "Iteration 6013, loss = 689.33673387\n",
      "Iteration 6014, loss = 688.90370261\n",
      "Iteration 6015, loss = 688.47081158\n",
      "Iteration 6016, loss = 688.05669968\n",
      "Iteration 6017, loss = 687.64550706\n",
      "Iteration 6018, loss = 687.24245842\n",
      "Iteration 6019, loss = 686.80107956\n",
      "Iteration 6020, loss = 686.44935446\n",
      "Iteration 6021, loss = 686.04847052\n",
      "Iteration 6022, loss = 685.53839053\n",
      "Iteration 6023, loss = 685.15430088\n",
      "Iteration 6024, loss = 684.71208555\n",
      "Iteration 6025, loss = 684.30735965\n",
      "Iteration 6026, loss = 683.90978510\n",
      "Iteration 6027, loss = 683.52094747\n",
      "Iteration 6028, loss = 683.07308776\n",
      "Iteration 6029, loss = 682.67180753\n",
      "Iteration 6030, loss = 682.26802646\n",
      "Iteration 6031, loss = 681.84000825\n",
      "Iteration 6032, loss = 681.46405705\n",
      "Iteration 6033, loss = 681.05393272\n",
      "Iteration 6034, loss = 680.67572612\n",
      "Iteration 6035, loss = 680.36792351\n",
      "Iteration 6036, loss = 679.84118037\n",
      "Iteration 6037, loss = 679.42382431\n",
      "Iteration 6038, loss = 679.00412546\n",
      "Iteration 6039, loss = 678.62321037\n",
      "Iteration 6040, loss = 678.20335045\n",
      "Iteration 6041, loss = 677.79149893\n",
      "Iteration 6042, loss = 677.38908691\n",
      "Iteration 6043, loss = 676.98712181\n",
      "Iteration 6044, loss = 676.58258192\n",
      "Iteration 6045, loss = 676.19646005\n",
      "Iteration 6046, loss = 675.85307291\n",
      "Iteration 6047, loss = 675.40619991\n",
      "Iteration 6048, loss = 675.03710978\n",
      "Iteration 6049, loss = 674.62422173\n",
      "Iteration 6050, loss = 674.25828470\n",
      "Iteration 6051, loss = 673.92719054\n",
      "Iteration 6052, loss = 673.42499970\n",
      "Iteration 6053, loss = 673.05854578\n",
      "Iteration 6054, loss = 672.66677325\n",
      "Iteration 6055, loss = 672.25715096\n",
      "Iteration 6056, loss = 671.86353079\n",
      "Iteration 6057, loss = 671.47231421\n",
      "Iteration 6058, loss = 671.07947948\n",
      "Iteration 6059, loss = 670.69527428\n",
      "Iteration 6060, loss = 670.30460266\n",
      "Iteration 6061, loss = 669.93192725\n",
      "Iteration 6062, loss = 669.58967704\n",
      "Iteration 6063, loss = 669.18816148\n",
      "Iteration 6064, loss = 668.80009849\n",
      "Iteration 6065, loss = 668.35693634\n",
      "Iteration 6066, loss = 668.03549711\n",
      "Iteration 6067, loss = 667.66670237\n",
      "Iteration 6068, loss = 667.23460675\n",
      "Iteration 6069, loss = 666.84545729\n",
      "Iteration 6070, loss = 666.46403876\n",
      "Iteration 6071, loss = 666.13232932\n",
      "Iteration 6072, loss = 665.78709318\n",
      "Iteration 6073, loss = 665.38184238\n",
      "Iteration 6074, loss = 664.93446441\n",
      "Iteration 6075, loss = 664.55217376\n",
      "Iteration 6076, loss = 664.19641420\n",
      "Iteration 6077, loss = 663.78638048\n",
      "Iteration 6078, loss = 663.49590062\n",
      "Iteration 6079, loss = 663.02678296\n",
      "Iteration 6080, loss = 662.72126306\n",
      "Iteration 6081, loss = 662.31409337\n",
      "Iteration 6082, loss = 661.98149264\n",
      "Iteration 6083, loss = 661.54096831\n",
      "Iteration 6084, loss = 661.17027565\n",
      "Iteration 6085, loss = 660.81178255\n",
      "Iteration 6086, loss = 660.42919007\n",
      "Iteration 6087, loss = 660.04107405\n",
      "Iteration 6088, loss = 659.69895087\n",
      "Iteration 6089, loss = 659.28989045\n",
      "Iteration 6090, loss = 658.98612920\n",
      "Iteration 6091, loss = 658.55171520\n",
      "Iteration 6092, loss = 658.17770677\n",
      "Iteration 6093, loss = 657.83462774\n",
      "Iteration 6094, loss = 657.46320880\n",
      "Iteration 6095, loss = 657.07968655\n",
      "Iteration 6096, loss = 656.70033443\n",
      "Iteration 6097, loss = 656.33386522\n",
      "Iteration 6098, loss = 655.96839502\n",
      "Iteration 6099, loss = 655.60458455\n",
      "Iteration 6100, loss = 655.27571421\n",
      "Iteration 6101, loss = 654.84758972\n",
      "Iteration 6102, loss = 654.58609674\n",
      "Iteration 6103, loss = 654.18431481\n",
      "Iteration 6104, loss = 653.77664667\n",
      "Iteration 6105, loss = 653.42042319\n",
      "Iteration 6106, loss = 653.07087491\n",
      "Iteration 6107, loss = 652.67279855\n",
      "Iteration 6108, loss = 652.30559130\n",
      "Iteration 6109, loss = 651.95461092\n",
      "Iteration 6110, loss = 651.71185010\n",
      "Iteration 6111, loss = 651.27054537\n",
      "Iteration 6112, loss = 650.86959273\n",
      "Iteration 6113, loss = 650.54918956\n",
      "Iteration 6114, loss = 650.16403478\n",
      "Iteration 6115, loss = 649.86633881\n",
      "Iteration 6116, loss = 649.57089302\n",
      "Iteration 6117, loss = 649.10011670\n",
      "Iteration 6118, loss = 648.77358242\n",
      "Iteration 6119, loss = 648.34623159\n",
      "Iteration 6120, loss = 647.98443866\n",
      "Iteration 6121, loss = 647.63556376\n",
      "Iteration 6122, loss = 647.28286194\n",
      "Iteration 6123, loss = 646.95122990\n",
      "Iteration 6124, loss = 646.59882037\n",
      "Iteration 6125, loss = 646.22456099\n",
      "Iteration 6126, loss = 645.86200574\n",
      "Iteration 6127, loss = 645.49231792\n",
      "Iteration 6128, loss = 645.13766452\n",
      "Iteration 6129, loss = 644.79115565\n",
      "Iteration 6130, loss = 644.44875186\n",
      "Iteration 6131, loss = 644.08635457\n",
      "Iteration 6132, loss = 643.71601911\n",
      "Iteration 6133, loss = 643.38541687\n",
      "Iteration 6134, loss = 643.01765604\n",
      "Iteration 6135, loss = 642.66857636\n",
      "Iteration 6136, loss = 642.31712523\n",
      "Iteration 6137, loss = 641.98233079\n",
      "Iteration 6138, loss = 641.62997718\n",
      "Iteration 6139, loss = 641.27083070\n",
      "Iteration 6140, loss = 640.92097725\n",
      "Iteration 6141, loss = 640.55254404\n",
      "Iteration 6142, loss = 640.20725680\n",
      "Iteration 6143, loss = 639.86311961\n",
      "Iteration 6144, loss = 639.50084215\n",
      "Iteration 6145, loss = 639.19775381\n",
      "Iteration 6146, loss = 638.87229688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6147, loss = 638.46810403\n",
      "Iteration 6148, loss = 638.12283413\n",
      "Iteration 6149, loss = 637.79704961\n",
      "Iteration 6150, loss = 637.46213532\n",
      "Iteration 6151, loss = 637.08848230\n",
      "Iteration 6152, loss = 636.73822831\n",
      "Iteration 6153, loss = 636.41215243\n",
      "Iteration 6154, loss = 636.03616841\n",
      "Iteration 6155, loss = 635.70773755\n",
      "Iteration 6156, loss = 635.37738302\n",
      "Iteration 6157, loss = 634.99953014\n",
      "Iteration 6158, loss = 634.65608362\n",
      "Iteration 6159, loss = 634.31582712\n",
      "Iteration 6160, loss = 633.97481865\n",
      "Iteration 6161, loss = 633.68307183\n",
      "Iteration 6162, loss = 633.28694042\n",
      "Iteration 6163, loss = 632.94603775\n",
      "Iteration 6164, loss = 632.60980149\n",
      "Iteration 6165, loss = 632.27078350\n",
      "Iteration 6166, loss = 631.95970295\n",
      "Iteration 6167, loss = 631.58327774\n",
      "Iteration 6168, loss = 631.24996394\n",
      "Iteration 6169, loss = 630.94831252\n",
      "Iteration 6170, loss = 630.55987772\n",
      "Iteration 6171, loss = 630.25056112\n",
      "Iteration 6172, loss = 629.90957747\n",
      "Iteration 6173, loss = 629.58547341\n",
      "Iteration 6174, loss = 629.24694113\n",
      "Iteration 6175, loss = 628.91517345\n",
      "Iteration 6176, loss = 628.63085778\n",
      "Iteration 6177, loss = 628.23797827\n",
      "Iteration 6178, loss = 627.84039194\n",
      "Iteration 6179, loss = 627.54821503\n",
      "Iteration 6180, loss = 627.18747545\n",
      "Iteration 6181, loss = 626.87376435\n",
      "Iteration 6182, loss = 626.52385825\n",
      "Iteration 6183, loss = 626.17691219\n",
      "Iteration 6184, loss = 625.84077110\n",
      "Iteration 6185, loss = 625.50218779\n",
      "Iteration 6186, loss = 625.17997619\n",
      "Iteration 6187, loss = 624.85725295\n",
      "Iteration 6188, loss = 624.49149066\n",
      "Iteration 6189, loss = 624.16739200\n",
      "Iteration 6190, loss = 623.85046676\n",
      "Iteration 6191, loss = 623.49477752\n",
      "Iteration 6192, loss = 623.16873622\n",
      "Iteration 6193, loss = 622.81694765\n",
      "Iteration 6194, loss = 622.49140643\n",
      "Iteration 6195, loss = 622.16645186\n",
      "Iteration 6196, loss = 621.84027606\n",
      "Iteration 6197, loss = 621.51653646\n",
      "Iteration 6198, loss = 621.17538360\n",
      "Iteration 6199, loss = 620.83784506\n",
      "Iteration 6200, loss = 620.51782590\n",
      "Iteration 6201, loss = 620.17870392\n",
      "Iteration 6202, loss = 619.84354119\n",
      "Iteration 6203, loss = 619.51816967\n",
      "Iteration 6204, loss = 619.24501996\n",
      "Iteration 6205, loss = 618.84557841\n",
      "Iteration 6206, loss = 618.51407417\n",
      "Iteration 6207, loss = 618.22172237\n",
      "Iteration 6208, loss = 617.88517777\n",
      "Iteration 6209, loss = 617.55588152\n",
      "Iteration 6210, loss = 617.22245411\n",
      "Iteration 6211, loss = 616.87236341\n",
      "Iteration 6212, loss = 616.54476854\n",
      "Iteration 6213, loss = 616.27042690\n",
      "Iteration 6214, loss = 616.08606576\n",
      "Iteration 6215, loss = 615.59737549\n",
      "Iteration 6216, loss = 615.25072936\n",
      "Iteration 6217, loss = 614.90428392\n",
      "Iteration 6218, loss = 614.60287549\n",
      "Iteration 6219, loss = 614.34991612\n",
      "Iteration 6220, loss = 613.93462955\n",
      "Iteration 6221, loss = 613.71951583\n",
      "Iteration 6222, loss = 613.30676773\n",
      "Iteration 6223, loss = 612.96921939\n",
      "Iteration 6224, loss = 612.65281080\n",
      "Iteration 6225, loss = 612.32152193\n",
      "Iteration 6226, loss = 612.02610414\n",
      "Iteration 6227, loss = 611.69485305\n",
      "Iteration 6228, loss = 611.35802056\n",
      "Iteration 6229, loss = 611.02617170\n",
      "Iteration 6230, loss = 610.72038640\n",
      "Iteration 6231, loss = 610.39501964\n",
      "Iteration 6232, loss = 610.04102750\n",
      "Iteration 6233, loss = 609.72915369\n",
      "Iteration 6234, loss = 609.40322461\n",
      "Iteration 6235, loss = 609.08021931\n",
      "Iteration 6236, loss = 608.77220340\n",
      "Iteration 6237, loss = 608.50338211\n",
      "Iteration 6238, loss = 608.11920717\n",
      "Iteration 6239, loss = 607.81787840\n",
      "Iteration 6240, loss = 607.50576961\n",
      "Iteration 6241, loss = 607.23404832\n",
      "Iteration 6242, loss = 606.85737398\n",
      "Iteration 6243, loss = 606.53688595\n",
      "Iteration 6244, loss = 606.20309089\n",
      "Iteration 6245, loss = 605.86944928\n",
      "Iteration 6246, loss = 605.57643185\n",
      "Iteration 6247, loss = 605.25089250\n",
      "Iteration 6248, loss = 604.92430531\n",
      "Iteration 6249, loss = 604.60226566\n",
      "Iteration 6250, loss = 604.29175145\n",
      "Iteration 6251, loss = 603.96330084\n",
      "Iteration 6252, loss = 603.66931972\n",
      "Iteration 6253, loss = 603.35234238\n",
      "Iteration 6254, loss = 603.01795040\n",
      "Iteration 6255, loss = 602.70865656\n",
      "Iteration 6256, loss = 602.39816391\n",
      "Iteration 6257, loss = 602.06840100\n",
      "Iteration 6258, loss = 601.80160104\n",
      "Iteration 6259, loss = 601.45302526\n",
      "Iteration 6260, loss = 601.11994706\n",
      "Iteration 6261, loss = 600.85220701\n",
      "Iteration 6262, loss = 600.50830664\n",
      "Iteration 6263, loss = 600.18441926\n",
      "Iteration 6264, loss = 599.85413146\n",
      "Iteration 6265, loss = 599.54616650\n",
      "Iteration 6266, loss = 599.24495360\n",
      "Iteration 6267, loss = 598.92831086\n",
      "Iteration 6268, loss = 598.58984344\n",
      "Iteration 6269, loss = 598.28380531\n",
      "Iteration 6270, loss = 597.98160728\n",
      "Iteration 6271, loss = 597.65773351\n",
      "Iteration 6272, loss = 597.34900340\n",
      "Iteration 6273, loss = 597.07945573\n",
      "Iteration 6274, loss = 596.75545344\n",
      "Iteration 6275, loss = 596.44106316\n",
      "Iteration 6276, loss = 596.08638826\n",
      "Iteration 6277, loss = 595.88908201\n",
      "Iteration 6278, loss = 595.49029304\n",
      "Iteration 6279, loss = 595.17789641\n",
      "Iteration 6280, loss = 594.88546417\n",
      "Iteration 6281, loss = 594.53498043\n",
      "Iteration 6282, loss = 594.22046149\n",
      "Iteration 6283, loss = 593.98824006\n",
      "Iteration 6284, loss = 593.60646790\n",
      "Iteration 6285, loss = 593.29511822\n",
      "Iteration 6286, loss = 592.98664289\n",
      "Iteration 6287, loss = 592.72472535\n",
      "Iteration 6288, loss = 592.35356622\n",
      "Iteration 6289, loss = 592.07334225\n",
      "Iteration 6290, loss = 591.76910774\n",
      "Iteration 6291, loss = 591.47137529\n",
      "Iteration 6292, loss = 591.15062630\n",
      "Iteration 6293, loss = 590.80439436\n",
      "Iteration 6294, loss = 590.53966067\n",
      "Iteration 6295, loss = 590.21798904\n",
      "Iteration 6296, loss = 589.90053194\n",
      "Iteration 6297, loss = 589.61433248\n",
      "Iteration 6298, loss = 589.34119950\n",
      "Iteration 6299, loss = 588.97157491\n",
      "Iteration 6300, loss = 588.71485415\n",
      "Iteration 6301, loss = 588.41878246\n",
      "Iteration 6302, loss = 588.05275265\n",
      "Iteration 6303, loss = 587.73253304\n",
      "Iteration 6304, loss = 587.48487924\n",
      "Iteration 6305, loss = 587.13393728\n",
      "Iteration 6306, loss = 586.81787918\n",
      "Iteration 6307, loss = 586.50977541\n",
      "Iteration 6308, loss = 586.19597112\n",
      "Iteration 6309, loss = 585.93097560\n",
      "Iteration 6310, loss = 585.58475929\n",
      "Iteration 6311, loss = 585.28708554\n",
      "Iteration 6312, loss = 585.01644480\n",
      "Iteration 6313, loss = 584.68798845\n",
      "Iteration 6314, loss = 584.35461784\n",
      "Iteration 6315, loss = 584.08612302\n",
      "Iteration 6316, loss = 583.78541137\n",
      "Iteration 6317, loss = 583.47350298\n",
      "Iteration 6318, loss = 583.16341979\n",
      "Iteration 6319, loss = 582.85875202\n",
      "Iteration 6320, loss = 582.56594118\n",
      "Iteration 6321, loss = 582.24992730\n",
      "Iteration 6322, loss = 581.93981280\n",
      "Iteration 6323, loss = 581.79256947\n",
      "Iteration 6324, loss = 581.31643188\n",
      "Iteration 6325, loss = 581.02626643\n",
      "Iteration 6326, loss = 580.74208934\n",
      "Iteration 6327, loss = 580.42535301\n",
      "Iteration 6328, loss = 580.16510189\n",
      "Iteration 6329, loss = 579.82957756\n",
      "Iteration 6330, loss = 579.53412001\n",
      "Iteration 6331, loss = 579.21813987\n",
      "Iteration 6332, loss = 578.89373119\n",
      "Iteration 6333, loss = 578.62642084\n",
      "Iteration 6334, loss = 578.28599775\n",
      "Iteration 6335, loss = 578.02762966\n",
      "Iteration 6336, loss = 577.73181831\n",
      "Iteration 6337, loss = 577.39358565\n",
      "Iteration 6338, loss = 577.09107386\n",
      "Iteration 6339, loss = 576.81570705\n",
      "Iteration 6340, loss = 576.54729806\n",
      "Iteration 6341, loss = 576.18374570\n",
      "Iteration 6342, loss = 575.91161756\n",
      "Iteration 6343, loss = 575.58303218\n",
      "Iteration 6344, loss = 575.29630465\n",
      "Iteration 6345, loss = 574.99096990\n",
      "Iteration 6346, loss = 574.69536505\n",
      "Iteration 6347, loss = 574.39697913\n",
      "Iteration 6348, loss = 574.08912690\n",
      "Iteration 6349, loss = 573.79122261\n",
      "Iteration 6350, loss = 573.49081973\n",
      "Iteration 6351, loss = 573.20029840\n",
      "Iteration 6352, loss = 572.89051734\n",
      "Iteration 6353, loss = 572.62682973\n",
      "Iteration 6354, loss = 572.29990383\n",
      "Iteration 6355, loss = 571.98681719\n",
      "Iteration 6356, loss = 571.73705770\n",
      "Iteration 6357, loss = 571.39991793\n",
      "Iteration 6358, loss = 571.08697948\n",
      "Iteration 6359, loss = 570.82306740\n",
      "Iteration 6360, loss = 570.50115575\n",
      "Iteration 6361, loss = 570.19552219\n",
      "Iteration 6362, loss = 569.88263764\n",
      "Iteration 6363, loss = 569.59845974\n",
      "Iteration 6364, loss = 569.31370385\n",
      "Iteration 6365, loss = 569.06022842\n",
      "Iteration 6366, loss = 568.73688674\n",
      "Iteration 6367, loss = 568.41201985\n",
      "Iteration 6368, loss = 568.10673400\n",
      "Iteration 6369, loss = 567.82060731\n",
      "Iteration 6370, loss = 567.51084122\n",
      "Iteration 6371, loss = 567.28717558\n",
      "Iteration 6372, loss = 566.92960912\n",
      "Iteration 6373, loss = 566.60075683\n",
      "Iteration 6374, loss = 566.32005643\n",
      "Iteration 6375, loss = 566.04124562\n",
      "Iteration 6376, loss = 565.73129470\n",
      "Iteration 6377, loss = 565.43131981\n",
      "Iteration 6378, loss = 565.12071084\n",
      "Iteration 6379, loss = 564.84104158\n",
      "Iteration 6380, loss = 564.57975816\n",
      "Iteration 6381, loss = 564.24361471\n",
      "Iteration 6382, loss = 563.93732321\n",
      "Iteration 6383, loss = 563.65210217\n",
      "Iteration 6384, loss = 563.33565718\n",
      "Iteration 6385, loss = 563.01857642\n",
      "Iteration 6386, loss = 562.72514789\n",
      "Iteration 6387, loss = 562.43483054\n",
      "Iteration 6388, loss = 562.19176304\n",
      "Iteration 6389, loss = 561.86089626\n",
      "Iteration 6390, loss = 561.58754121\n",
      "Iteration 6391, loss = 561.23505371\n",
      "Iteration 6392, loss = 561.01162813\n",
      "Iteration 6393, loss = 560.74020467\n",
      "Iteration 6394, loss = 560.32749271\n",
      "Iteration 6395, loss = 560.02020429\n",
      "Iteration 6396, loss = 559.76476148\n",
      "Iteration 6397, loss = 559.41425382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6398, loss = 559.10788814\n",
      "Iteration 6399, loss = 558.82088766\n",
      "Iteration 6400, loss = 558.51025286\n",
      "Iteration 6401, loss = 558.21145444\n",
      "Iteration 6402, loss = 557.93247634\n",
      "Iteration 6403, loss = 557.55588851\n",
      "Iteration 6404, loss = 557.27054801\n",
      "Iteration 6405, loss = 556.93776984\n",
      "Iteration 6406, loss = 556.64290193\n",
      "Iteration 6407, loss = 556.33742568\n",
      "Iteration 6408, loss = 556.00341848\n",
      "Iteration 6409, loss = 555.71113189\n",
      "Iteration 6410, loss = 555.40114427\n",
      "Iteration 6411, loss = 555.06445774\n",
      "Iteration 6412, loss = 554.70602125\n",
      "Iteration 6413, loss = 554.34852620\n",
      "Iteration 6414, loss = 554.00572247\n",
      "Iteration 6415, loss = 553.65695019\n",
      "Iteration 6416, loss = 553.30762965\n",
      "Iteration 6417, loss = 552.95402185\n",
      "Iteration 6418, loss = 552.70433106\n",
      "Iteration 6419, loss = 552.20258506\n",
      "Iteration 6420, loss = 551.89300144\n",
      "Iteration 6421, loss = 551.47671203\n",
      "Iteration 6422, loss = 551.06850387\n",
      "Iteration 6423, loss = 550.64799077\n",
      "Iteration 6424, loss = 550.24211362\n",
      "Iteration 6425, loss = 549.81197920\n",
      "Iteration 6426, loss = 549.37152403\n",
      "Iteration 6427, loss = 548.91818778\n",
      "Iteration 6428, loss = 548.40440021\n",
      "Iteration 6429, loss = 547.89187217\n",
      "Iteration 6430, loss = 547.34803830\n",
      "Iteration 6431, loss = 546.77950283\n",
      "Iteration 6432, loss = 546.20577218\n",
      "Iteration 6433, loss = 545.61849412\n",
      "Iteration 6434, loss = 544.97315270\n",
      "Iteration 6435, loss = 544.33767893\n",
      "Iteration 6436, loss = 543.65436075\n",
      "Iteration 6437, loss = 543.00286487\n",
      "Iteration 6438, loss = 542.27165296\n",
      "Iteration 6439, loss = 541.55342847\n",
      "Iteration 6440, loss = 540.82690004\n",
      "Iteration 6441, loss = 540.07320012\n",
      "Iteration 6442, loss = 539.40770360\n",
      "Iteration 6443, loss = 538.59848394\n",
      "Iteration 6444, loss = 537.82907698\n",
      "Iteration 6445, loss = 537.07255105\n",
      "Iteration 6446, loss = 536.31682231\n",
      "Iteration 6447, loss = 535.58301423\n",
      "Iteration 6448, loss = 534.86979049\n",
      "Iteration 6449, loss = 534.11136023\n",
      "Iteration 6450, loss = 533.37127612\n",
      "Iteration 6451, loss = 532.67150284\n",
      "Iteration 6452, loss = 531.95487376\n",
      "Iteration 6453, loss = 531.21876800\n",
      "Iteration 6454, loss = 530.53264543\n",
      "Iteration 6455, loss = 529.84591046\n",
      "Iteration 6456, loss = 529.17690988\n",
      "Iteration 6457, loss = 528.48653255\n",
      "Iteration 6458, loss = 527.84339987\n",
      "Iteration 6459, loss = 527.20930781\n",
      "Iteration 6460, loss = 526.52709371\n",
      "Iteration 6461, loss = 525.95709247\n",
      "Iteration 6462, loss = 525.25823186\n",
      "Iteration 6463, loss = 524.61883056\n",
      "Iteration 6464, loss = 524.05400477\n",
      "Iteration 6465, loss = 523.39459489\n",
      "Iteration 6466, loss = 522.78021343\n",
      "Iteration 6467, loss = 522.17429588\n",
      "Iteration 6468, loss = 521.57440711\n",
      "Iteration 6469, loss = 520.99209292\n",
      "Iteration 6470, loss = 520.41548270\n",
      "Iteration 6471, loss = 519.82149930\n",
      "Iteration 6472, loss = 519.23197586\n",
      "Iteration 6473, loss = 518.65769214\n",
      "Iteration 6474, loss = 518.10040581\n",
      "Iteration 6475, loss = 517.55840093\n",
      "Iteration 6476, loss = 516.98329296\n",
      "Iteration 6477, loss = 516.42293322\n",
      "Iteration 6478, loss = 515.85369668\n",
      "Iteration 6479, loss = 515.31516540\n",
      "Iteration 6480, loss = 514.80187118\n",
      "Iteration 6481, loss = 514.22798909\n",
      "Iteration 6482, loss = 513.69679105\n",
      "Iteration 6483, loss = 513.17539842\n",
      "Iteration 6484, loss = 512.64182421\n",
      "Iteration 6485, loss = 512.12292288\n",
      "Iteration 6486, loss = 511.56868595\n",
      "Iteration 6487, loss = 511.06695752\n",
      "Iteration 6488, loss = 510.54885086\n",
      "Iteration 6489, loss = 510.03239633\n",
      "Iteration 6490, loss = 509.51632942\n",
      "Iteration 6491, loss = 509.02335576\n",
      "Iteration 6492, loss = 508.51961310\n",
      "Iteration 6493, loss = 507.99683720\n",
      "Iteration 6494, loss = 507.49581202\n",
      "Iteration 6495, loss = 507.00681831\n",
      "Iteration 6496, loss = 506.49653349\n",
      "Iteration 6497, loss = 505.99596209\n",
      "Iteration 6498, loss = 505.50307130\n",
      "Iteration 6499, loss = 505.01992226\n",
      "Iteration 6500, loss = 504.54455542\n",
      "Iteration 6501, loss = 504.04961009\n",
      "Iteration 6502, loss = 503.56315745\n",
      "Iteration 6503, loss = 503.09965440\n",
      "Iteration 6504, loss = 502.59984661\n",
      "Iteration 6505, loss = 502.12157288\n",
      "Iteration 6506, loss = 501.64351734\n",
      "Iteration 6507, loss = 501.17935617\n",
      "Iteration 6508, loss = 500.71188880\n",
      "Iteration 6509, loss = 500.24250160\n",
      "Iteration 6510, loss = 499.76535331\n",
      "Iteration 6511, loss = 499.30221676\n",
      "Iteration 6512, loss = 498.83413535\n",
      "Iteration 6513, loss = 498.38864574\n",
      "Iteration 6514, loss = 497.94816902\n",
      "Iteration 6515, loss = 497.47627952\n",
      "Iteration 6516, loss = 496.99841435\n",
      "Iteration 6517, loss = 496.54349982\n",
      "Iteration 6518, loss = 496.08779731\n",
      "Iteration 6519, loss = 495.64648609\n",
      "Iteration 6520, loss = 495.18259224\n",
      "Iteration 6521, loss = 494.73439051\n",
      "Iteration 6522, loss = 494.29421663\n",
      "Iteration 6523, loss = 493.85182949\n",
      "Iteration 6524, loss = 493.41376533\n",
      "Iteration 6525, loss = 492.98022075\n",
      "Iteration 6526, loss = 492.51324142\n",
      "Iteration 6527, loss = 492.07359697\n",
      "Iteration 6528, loss = 491.69380299\n",
      "Iteration 6529, loss = 491.20340428\n",
      "Iteration 6530, loss = 490.78231410\n",
      "Iteration 6531, loss = 490.32468371\n",
      "Iteration 6532, loss = 489.89850099\n",
      "Iteration 6533, loss = 489.52009220\n",
      "Iteration 6534, loss = 489.03453438\n",
      "Iteration 6535, loss = 488.60879663\n",
      "Iteration 6536, loss = 488.20651988\n",
      "Iteration 6537, loss = 487.77073281\n",
      "Iteration 6538, loss = 487.33732601\n",
      "Iteration 6539, loss = 486.98968895\n",
      "Iteration 6540, loss = 486.48558707\n",
      "Iteration 6541, loss = 486.05273075\n",
      "Iteration 6542, loss = 485.63952617\n",
      "Iteration 6543, loss = 485.24442160\n",
      "Iteration 6544, loss = 484.82279862\n",
      "Iteration 6545, loss = 484.40721989\n",
      "Iteration 6546, loss = 484.01035427\n",
      "Iteration 6547, loss = 483.57237521\n",
      "Iteration 6548, loss = 483.13673327\n",
      "Iteration 6549, loss = 482.73886877\n",
      "Iteration 6550, loss = 482.34075463\n",
      "Iteration 6551, loss = 481.92168877\n",
      "Iteration 6552, loss = 481.50709629\n",
      "Iteration 6553, loss = 481.13267867\n",
      "Iteration 6554, loss = 480.69752163\n",
      "Iteration 6555, loss = 480.27988308\n",
      "Iteration 6556, loss = 479.87379756\n",
      "Iteration 6557, loss = 479.46071450\n",
      "Iteration 6558, loss = 479.05988420\n",
      "Iteration 6559, loss = 478.67348603\n",
      "Iteration 6560, loss = 478.28079292\n",
      "Iteration 6561, loss = 477.85951448\n",
      "Iteration 6562, loss = 477.47765697\n",
      "Iteration 6563, loss = 477.05526435\n",
      "Iteration 6564, loss = 476.69067782\n",
      "Iteration 6565, loss = 476.26494392\n",
      "Iteration 6566, loss = 475.87517816\n",
      "Iteration 6567, loss = 475.48863657\n",
      "Iteration 6568, loss = 475.08514868\n",
      "Iteration 6569, loss = 474.68342711\n",
      "Iteration 6570, loss = 474.33449971\n",
      "Iteration 6571, loss = 473.92707303\n",
      "Iteration 6572, loss = 473.54116450\n",
      "Iteration 6573, loss = 473.12189371\n",
      "Iteration 6574, loss = 472.74782159\n",
      "Iteration 6575, loss = 472.35337721\n",
      "Iteration 6576, loss = 471.99137938\n",
      "Iteration 6577, loss = 471.56986312\n",
      "Iteration 6578, loss = 471.22169174\n",
      "Iteration 6579, loss = 470.85424147\n",
      "Iteration 6580, loss = 470.43384511\n",
      "Iteration 6581, loss = 470.04079398\n",
      "Iteration 6582, loss = 469.67113998\n",
      "Iteration 6583, loss = 469.28072216\n",
      "Iteration 6584, loss = 468.91059796\n",
      "Iteration 6585, loss = 468.52232687\n",
      "Iteration 6586, loss = 468.18979709\n",
      "Iteration 6587, loss = 467.75986219\n",
      "Iteration 6588, loss = 467.40806244\n",
      "Iteration 6589, loss = 467.04480961\n",
      "Iteration 6590, loss = 466.62416441\n",
      "Iteration 6591, loss = 466.27461832\n",
      "Iteration 6592, loss = 465.87051814\n",
      "Iteration 6593, loss = 465.54248158\n",
      "Iteration 6594, loss = 465.14932555\n",
      "Iteration 6595, loss = 464.75922756\n",
      "Iteration 6596, loss = 464.40236428\n",
      "Iteration 6597, loss = 464.03509562\n",
      "Iteration 6598, loss = 463.65231260\n",
      "Iteration 6599, loss = 463.31025212\n",
      "Iteration 6600, loss = 462.91883545\n",
      "Iteration 6601, loss = 462.55056420\n",
      "Iteration 6602, loss = 462.16795529\n",
      "Iteration 6603, loss = 461.81028765\n",
      "Iteration 6604, loss = 461.44998578\n",
      "Iteration 6605, loss = 461.08720199\n",
      "Iteration 6606, loss = 460.82919918\n",
      "Iteration 6607, loss = 460.37465946\n",
      "Iteration 6608, loss = 459.98371299\n",
      "Iteration 6609, loss = 459.65875527\n",
      "Iteration 6610, loss = 459.27622895\n",
      "Iteration 6611, loss = 458.89800722\n",
      "Iteration 6612, loss = 458.54853812\n",
      "Iteration 6613, loss = 458.17089390\n",
      "Iteration 6614, loss = 457.81563094\n",
      "Iteration 6615, loss = 457.49075848\n",
      "Iteration 6616, loss = 457.09885803\n",
      "Iteration 6617, loss = 456.77993099\n",
      "Iteration 6618, loss = 456.37243198\n",
      "Iteration 6619, loss = 456.02808429\n",
      "Iteration 6620, loss = 455.68358388\n",
      "Iteration 6621, loss = 455.32612485\n",
      "Iteration 6622, loss = 454.96827287\n",
      "Iteration 6623, loss = 454.62292583\n",
      "Iteration 6624, loss = 454.25491942\n",
      "Iteration 6625, loss = 453.88576507\n",
      "Iteration 6626, loss = 453.52960215\n",
      "Iteration 6627, loss = 453.17914350\n",
      "Iteration 6628, loss = 452.85003380\n",
      "Iteration 6629, loss = 452.50994412\n",
      "Iteration 6630, loss = 452.14679913\n",
      "Iteration 6631, loss = 451.78211497\n",
      "Iteration 6632, loss = 451.43608104\n",
      "Iteration 6633, loss = 451.07932219\n",
      "Iteration 6634, loss = 450.74312894\n",
      "Iteration 6635, loss = 450.40180842\n",
      "Iteration 6636, loss = 450.08119617\n",
      "Iteration 6637, loss = 449.67904764\n",
      "Iteration 6638, loss = 449.35411027\n",
      "Iteration 6639, loss = 449.00473173\n",
      "Iteration 6640, loss = 448.67000712\n",
      "Iteration 6641, loss = 448.30631525\n",
      "Iteration 6642, loss = 447.96593568\n",
      "Iteration 6643, loss = 447.68617606\n",
      "Iteration 6644, loss = 447.28686143\n",
      "Iteration 6645, loss = 446.94213501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6646, loss = 446.58558196\n",
      "Iteration 6647, loss = 446.23451493\n",
      "Iteration 6648, loss = 445.94465337\n",
      "Iteration 6649, loss = 445.55964311\n",
      "Iteration 6650, loss = 445.24705475\n",
      "Iteration 6651, loss = 444.87316585\n",
      "Iteration 6652, loss = 444.53091815\n",
      "Iteration 6653, loss = 444.24384999\n",
      "Iteration 6654, loss = 443.88453939\n",
      "Iteration 6655, loss = 443.55569003\n",
      "Iteration 6656, loss = 443.18327364\n",
      "Iteration 6657, loss = 442.85920458\n",
      "Iteration 6658, loss = 442.50025248\n",
      "Iteration 6659, loss = 442.27712017\n",
      "Iteration 6660, loss = 441.86811752\n",
      "Iteration 6661, loss = 441.50305251\n",
      "Iteration 6662, loss = 441.16252389\n",
      "Iteration 6663, loss = 440.86303291\n",
      "Iteration 6664, loss = 440.52057638\n",
      "Iteration 6665, loss = 440.17073653\n",
      "Iteration 6666, loss = 439.83089850\n",
      "Iteration 6667, loss = 439.49615851\n",
      "Iteration 6668, loss = 439.16885256\n",
      "Iteration 6669, loss = 438.82168618\n",
      "Iteration 6670, loss = 438.50357679\n",
      "Iteration 6671, loss = 438.18104094\n",
      "Iteration 6672, loss = 437.83419192\n",
      "Iteration 6673, loss = 437.51256180\n",
      "Iteration 6674, loss = 437.22941964\n",
      "Iteration 6675, loss = 436.87303159\n",
      "Iteration 6676, loss = 436.52718113\n",
      "Iteration 6677, loss = 436.21271517\n",
      "Iteration 6678, loss = 435.86033755\n",
      "Iteration 6679, loss = 435.51949640\n",
      "Iteration 6680, loss = 435.20730873\n",
      "Iteration 6681, loss = 434.87312063\n",
      "Iteration 6682, loss = 434.53750909\n",
      "Iteration 6683, loss = 434.24582102\n",
      "Iteration 6684, loss = 433.91475040\n",
      "Iteration 6685, loss = 433.57818788\n",
      "Iteration 6686, loss = 433.25307134\n",
      "Iteration 6687, loss = 432.94041295\n",
      "Iteration 6688, loss = 432.59489547\n",
      "Iteration 6689, loss = 432.27257625\n",
      "Iteration 6690, loss = 431.94472616\n",
      "Iteration 6691, loss = 431.64153973\n",
      "Iteration 6692, loss = 431.30983062\n",
      "Iteration 6693, loss = 431.00079941\n",
      "Iteration 6694, loss = 430.66434262\n",
      "Iteration 6695, loss = 430.34931074\n",
      "Iteration 6696, loss = 430.00965152\n",
      "Iteration 6697, loss = 429.73659639\n",
      "Iteration 6698, loss = 429.35757096\n",
      "Iteration 6699, loss = 429.08231001\n",
      "Iteration 6700, loss = 428.72428320\n",
      "Iteration 6701, loss = 428.43428042\n",
      "Iteration 6702, loss = 428.08643384\n",
      "Iteration 6703, loss = 427.75523175\n",
      "Iteration 6704, loss = 427.46455526\n",
      "Iteration 6705, loss = 427.12193876\n",
      "Iteration 6706, loss = 426.81734205\n",
      "Iteration 6707, loss = 426.51024116\n",
      "Iteration 6708, loss = 426.20359517\n",
      "Iteration 6709, loss = 425.86103948\n",
      "Iteration 6710, loss = 425.55673031\n",
      "Iteration 6711, loss = 425.24622842\n",
      "Iteration 6712, loss = 424.90973286\n",
      "Iteration 6713, loss = 424.59746705\n",
      "Iteration 6714, loss = 424.27289536\n",
      "Iteration 6715, loss = 423.99692381\n",
      "Iteration 6716, loss = 423.65693295\n",
      "Iteration 6717, loss = 423.34134447\n",
      "Iteration 6718, loss = 423.03841704\n",
      "Iteration 6719, loss = 422.72738534\n",
      "Iteration 6720, loss = 422.43070873\n",
      "Iteration 6721, loss = 422.08352496\n",
      "Iteration 6722, loss = 421.78798610\n",
      "Iteration 6723, loss = 421.46866907\n",
      "Iteration 6724, loss = 421.16514132\n",
      "Iteration 6725, loss = 420.86467752\n",
      "Iteration 6726, loss = 420.54432453\n",
      "Iteration 6727, loss = 420.22664629\n",
      "Iteration 6728, loss = 419.90365335\n",
      "Iteration 6729, loss = 419.62911798\n",
      "Iteration 6730, loss = 419.29778256\n",
      "Iteration 6731, loss = 419.02079275\n",
      "Iteration 6732, loss = 418.67478069\n",
      "Iteration 6733, loss = 418.38445196\n",
      "Iteration 6734, loss = 418.05758156\n",
      "Iteration 6735, loss = 417.75056592\n",
      "Iteration 6736, loss = 417.45120686\n",
      "Iteration 6737, loss = 417.12332305\n",
      "Iteration 6738, loss = 416.82243434\n",
      "Iteration 6739, loss = 416.50398909\n",
      "Iteration 6740, loss = 416.19175944\n",
      "Iteration 6741, loss = 415.90018502\n",
      "Iteration 6742, loss = 415.59058230\n",
      "Iteration 6743, loss = 415.29998083\n",
      "Iteration 6744, loss = 414.97394907\n",
      "Iteration 6745, loss = 414.66404385\n",
      "Iteration 6746, loss = 414.37143529\n",
      "Iteration 6747, loss = 414.06343000\n",
      "Iteration 6748, loss = 413.74820903\n",
      "Iteration 6749, loss = 413.44877774\n",
      "Iteration 6750, loss = 413.14633405\n",
      "Iteration 6751, loss = 412.84400259\n",
      "Iteration 6752, loss = 412.58697163\n",
      "Iteration 6753, loss = 412.23589695\n",
      "Iteration 6754, loss = 411.92693885\n",
      "Iteration 6755, loss = 411.63057981\n",
      "Iteration 6756, loss = 411.33109806\n",
      "Iteration 6757, loss = 411.04540218\n",
      "Iteration 6758, loss = 410.72175043\n",
      "Iteration 6759, loss = 410.44346516\n",
      "Iteration 6760, loss = 410.11969970\n",
      "Iteration 6761, loss = 409.82138346\n",
      "Iteration 6762, loss = 409.52730125\n",
      "Iteration 6763, loss = 409.21689080\n",
      "Iteration 6764, loss = 408.92232327\n",
      "Iteration 6765, loss = 408.61434519\n",
      "Iteration 6766, loss = 408.31749062\n",
      "Iteration 6767, loss = 408.02050328\n",
      "Iteration 6768, loss = 407.75465078\n",
      "Iteration 6769, loss = 407.43040878\n",
      "Iteration 6770, loss = 407.14673822\n",
      "Iteration 6771, loss = 406.83302959\n",
      "Iteration 6772, loss = 406.51799074\n",
      "Iteration 6773, loss = 406.23213177\n",
      "Iteration 6774, loss = 405.93538896\n",
      "Iteration 6775, loss = 405.64479553\n",
      "Iteration 6776, loss = 405.33776746\n",
      "Iteration 6777, loss = 405.04315172\n",
      "Iteration 6778, loss = 404.74060753\n",
      "Iteration 6779, loss = 404.46372105\n",
      "Iteration 6780, loss = 404.15996415\n",
      "Iteration 6781, loss = 403.88639399\n",
      "Iteration 6782, loss = 403.55113551\n",
      "Iteration 6783, loss = 403.25809478\n",
      "Iteration 6784, loss = 402.97515375\n",
      "Iteration 6785, loss = 402.67611666\n",
      "Iteration 6786, loss = 402.38529204\n",
      "Iteration 6787, loss = 402.12790519\n",
      "Iteration 6788, loss = 401.81351792\n",
      "Iteration 6789, loss = 401.49336115\n",
      "Iteration 6790, loss = 401.20996891\n",
      "Iteration 6791, loss = 400.92237420\n",
      "Iteration 6792, loss = 400.61499209\n",
      "Iteration 6793, loss = 400.33986214\n",
      "Iteration 6794, loss = 400.02889032\n",
      "Iteration 6795, loss = 399.74673674\n",
      "Iteration 6796, loss = 399.45619924\n",
      "Iteration 6797, loss = 399.17974177\n",
      "Iteration 6798, loss = 398.86501078\n",
      "Iteration 6799, loss = 398.62559245\n",
      "Iteration 6800, loss = 398.30040354\n",
      "Iteration 6801, loss = 398.04223560\n",
      "Iteration 6802, loss = 397.69650362\n",
      "Iteration 6803, loss = 397.41800229\n",
      "Iteration 6804, loss = 397.14878595\n",
      "Iteration 6805, loss = 396.82496523\n",
      "Iteration 6806, loss = 396.55072913\n",
      "Iteration 6807, loss = 396.28387348\n",
      "Iteration 6808, loss = 395.95044750\n",
      "Iteration 6809, loss = 395.71273212\n",
      "Iteration 6810, loss = 395.37853411\n",
      "Iteration 6811, loss = 395.12367720\n",
      "Iteration 6812, loss = 394.82568809\n",
      "Iteration 6813, loss = 394.53054088\n",
      "Iteration 6814, loss = 394.24118644\n",
      "Iteration 6815, loss = 393.97847415\n",
      "Iteration 6816, loss = 393.71278494\n",
      "Iteration 6817, loss = 393.38119884\n",
      "Iteration 6818, loss = 393.08665781\n",
      "Iteration 6819, loss = 392.80764176\n",
      "Iteration 6820, loss = 392.54105986\n",
      "Iteration 6821, loss = 392.23122996\n",
      "Iteration 6822, loss = 391.94390842\n",
      "Iteration 6823, loss = 391.67493330\n",
      "Iteration 6824, loss = 391.48661835\n",
      "Iteration 6825, loss = 391.17425225\n",
      "Iteration 6826, loss = 390.81155822\n",
      "Iteration 6827, loss = 390.51809968\n",
      "Iteration 6828, loss = 390.28599739\n",
      "Iteration 6829, loss = 389.93971912\n",
      "Iteration 6830, loss = 389.66541188\n",
      "Iteration 6831, loss = 389.39795252\n",
      "Iteration 6832, loss = 389.11488045\n",
      "Iteration 6833, loss = 388.84112503\n",
      "Iteration 6834, loss = 388.57928932\n",
      "Iteration 6835, loss = 388.25562841\n",
      "Iteration 6836, loss = 387.97282161\n",
      "Iteration 6837, loss = 387.70245294\n",
      "Iteration 6838, loss = 387.40597954\n",
      "Iteration 6839, loss = 387.14231736\n",
      "Iteration 6840, loss = 386.83813403\n",
      "Iteration 6841, loss = 386.56706632\n",
      "Iteration 6842, loss = 386.29204538\n",
      "Iteration 6843, loss = 385.99576851\n",
      "Iteration 6844, loss = 385.74542147\n",
      "Iteration 6845, loss = 385.44207611\n",
      "Iteration 6846, loss = 385.20153765\n",
      "Iteration 6847, loss = 384.91276645\n",
      "Iteration 6848, loss = 384.62328534\n",
      "Iteration 6849, loss = 384.32877168\n",
      "Iteration 6850, loss = 384.05654336\n",
      "Iteration 6851, loss = 383.78082810\n",
      "Iteration 6852, loss = 383.49197116\n",
      "Iteration 6853, loss = 383.20454698\n",
      "Iteration 6854, loss = 382.92482156\n",
      "Iteration 6855, loss = 382.64813023\n",
      "Iteration 6856, loss = 382.36422643\n",
      "Iteration 6857, loss = 382.09867060\n",
      "Iteration 6858, loss = 381.81169375\n",
      "Iteration 6859, loss = 381.55389379\n",
      "Iteration 6860, loss = 381.27097438\n",
      "Iteration 6861, loss = 380.99531640\n",
      "Iteration 6862, loss = 380.70413338\n",
      "Iteration 6863, loss = 380.43775527\n",
      "Iteration 6864, loss = 380.16642750\n",
      "Iteration 6865, loss = 379.90904691\n",
      "Iteration 6866, loss = 379.61541078\n",
      "Iteration 6867, loss = 379.33904873\n",
      "Iteration 6868, loss = 379.08330379\n",
      "Iteration 6869, loss = 378.77453889\n",
      "Iteration 6870, loss = 378.54285037\n",
      "Iteration 6871, loss = 378.24742302\n",
      "Iteration 6872, loss = 377.96125880\n",
      "Iteration 6873, loss = 377.68578753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6874, loss = 377.41233569\n",
      "Iteration 6875, loss = 377.13196818\n",
      "Iteration 6876, loss = 376.88057907\n",
      "Iteration 6877, loss = 376.59790380\n",
      "Iteration 6878, loss = 376.34680736\n",
      "Iteration 6879, loss = 376.02697992\n",
      "Iteration 6880, loss = 375.75779088\n",
      "Iteration 6881, loss = 375.49451098\n",
      "Iteration 6882, loss = 375.23698598\n",
      "Iteration 6883, loss = 374.94594443\n",
      "Iteration 6884, loss = 374.67492671\n",
      "Iteration 6885, loss = 374.40881450\n",
      "Iteration 6886, loss = 374.15754633\n",
      "Iteration 6887, loss = 373.87729697\n",
      "Iteration 6888, loss = 373.59676137\n",
      "Iteration 6889, loss = 373.34186086\n",
      "Iteration 6890, loss = 373.04465966\n",
      "Iteration 6891, loss = 372.79548710\n",
      "Iteration 6892, loss = 372.51868715\n",
      "Iteration 6893, loss = 372.27582568\n",
      "Iteration 6894, loss = 372.00622349\n",
      "Iteration 6895, loss = 371.70319447\n",
      "Iteration 6896, loss = 371.42477165\n",
      "Iteration 6897, loss = 371.15677160\n",
      "Iteration 6898, loss = 370.88455695\n",
      "Iteration 6899, loss = 370.60215016\n",
      "Iteration 6900, loss = 370.37785081\n",
      "Iteration 6901, loss = 370.07048006\n",
      "Iteration 6902, loss = 369.81505506\n",
      "Iteration 6903, loss = 369.55337292\n",
      "Iteration 6904, loss = 369.26324458\n",
      "Iteration 6905, loss = 369.01128876\n",
      "Iteration 6906, loss = 368.73905555\n",
      "Iteration 6907, loss = 368.46468001\n",
      "Iteration 6908, loss = 368.25169382\n",
      "Iteration 6909, loss = 367.93453968\n",
      "Iteration 6910, loss = 367.69236061\n",
      "Iteration 6911, loss = 367.41578056\n",
      "Iteration 6912, loss = 367.14277076\n",
      "Iteration 6913, loss = 366.90487931\n",
      "Iteration 6914, loss = 366.61761332\n",
      "Iteration 6915, loss = 366.33789532\n",
      "Iteration 6916, loss = 366.06533835\n",
      "Iteration 6917, loss = 365.80338679\n",
      "Iteration 6918, loss = 365.55119807\n",
      "Iteration 6919, loss = 365.26896085\n",
      "Iteration 6920, loss = 365.01797204\n",
      "Iteration 6921, loss = 364.77794266\n",
      "Iteration 6922, loss = 364.50662476\n",
      "Iteration 6923, loss = 364.21329807\n",
      "Iteration 6924, loss = 363.96893115\n",
      "Iteration 6925, loss = 363.68129313\n",
      "Iteration 6926, loss = 363.42672305\n",
      "Iteration 6927, loss = 363.16188915\n",
      "Iteration 6928, loss = 362.89554648\n",
      "Iteration 6929, loss = 362.64055167\n",
      "Iteration 6930, loss = 362.38032869\n",
      "Iteration 6931, loss = 362.10761811\n",
      "Iteration 6932, loss = 361.85298963\n",
      "Iteration 6933, loss = 361.61689629\n",
      "Iteration 6934, loss = 361.32064033\n",
      "Iteration 6935, loss = 361.06838782\n",
      "Iteration 6936, loss = 360.78698461\n",
      "Iteration 6937, loss = 360.53722632\n",
      "Iteration 6938, loss = 360.26714522\n",
      "Iteration 6939, loss = 360.00171959\n",
      "Iteration 6940, loss = 359.74906836\n",
      "Iteration 6941, loss = 359.48438617\n",
      "Iteration 6942, loss = 359.24849498\n",
      "Iteration 6943, loss = 358.96561894\n",
      "Iteration 6944, loss = 358.69892563\n",
      "Iteration 6945, loss = 358.46859914\n",
      "Iteration 6946, loss = 358.22382280\n",
      "Iteration 6947, loss = 357.93401664\n",
      "Iteration 6948, loss = 357.67054278\n",
      "Iteration 6949, loss = 357.39985179\n",
      "Iteration 6950, loss = 357.14146515\n",
      "Iteration 6951, loss = 356.93034505\n",
      "Iteration 6952, loss = 356.69616259\n",
      "Iteration 6953, loss = 356.36158611\n",
      "Iteration 6954, loss = 356.11967256\n",
      "Iteration 6955, loss = 355.84866751\n",
      "Iteration 6956, loss = 355.59404226\n",
      "Iteration 6957, loss = 355.34390267\n",
      "Iteration 6958, loss = 355.08796798\n",
      "Iteration 6959, loss = 354.81463273\n",
      "Iteration 6960, loss = 354.55759677\n",
      "Iteration 6961, loss = 354.30568191\n",
      "Iteration 6962, loss = 354.06032281\n",
      "Iteration 6963, loss = 353.79370092\n",
      "Iteration 6964, loss = 353.53980171\n",
      "Iteration 6965, loss = 353.28835729\n",
      "Iteration 6966, loss = 353.01833280\n",
      "Iteration 6967, loss = 352.79752542\n",
      "Iteration 6968, loss = 352.51995687\n",
      "Iteration 6969, loss = 352.27240109\n",
      "Iteration 6970, loss = 352.00014065\n",
      "Iteration 6971, loss = 351.78534468\n",
      "Iteration 6972, loss = 351.48504832\n",
      "Iteration 6973, loss = 351.25333916\n",
      "Iteration 6974, loss = 350.98093777\n",
      "Iteration 6975, loss = 350.71085189\n",
      "Iteration 6976, loss = 350.45963132\n",
      "Iteration 6977, loss = 350.22015255\n",
      "Iteration 6978, loss = 349.95596549\n",
      "Iteration 6979, loss = 349.76283362\n",
      "Iteration 6980, loss = 349.44728227\n",
      "Iteration 6981, loss = 349.22809491\n",
      "Iteration 6982, loss = 348.94080330\n",
      "Iteration 6983, loss = 348.67143630\n",
      "Iteration 6984, loss = 348.42647375\n",
      "Iteration 6985, loss = 348.17217309\n",
      "Iteration 6986, loss = 347.92498207\n",
      "Iteration 6987, loss = 347.66678437\n",
      "Iteration 6988, loss = 347.41569046\n",
      "Iteration 6989, loss = 347.16708322\n",
      "Iteration 6990, loss = 346.92835654\n",
      "Iteration 6991, loss = 346.68160322\n",
      "Iteration 6992, loss = 346.40250082\n",
      "Iteration 6993, loss = 346.16547227\n",
      "Iteration 6994, loss = 345.89953945\n",
      "Iteration 6995, loss = 345.67440411\n",
      "Iteration 6996, loss = 345.41059331\n",
      "Iteration 6997, loss = 345.15993055\n",
      "Iteration 6998, loss = 344.91455923\n",
      "Iteration 6999, loss = 344.64738357\n",
      "Iteration 7000, loss = 344.39770667\n",
      "Iteration 7001, loss = 344.14885588\n",
      "Iteration 7002, loss = 343.89605620\n",
      "Iteration 7003, loss = 343.65705718\n",
      "Iteration 7004, loss = 343.41664463\n",
      "Iteration 7005, loss = 343.14950074\n",
      "Iteration 7006, loss = 342.90895199\n",
      "Iteration 7007, loss = 342.65897845\n",
      "Iteration 7008, loss = 342.39721882\n",
      "Iteration 7009, loss = 342.15290848\n",
      "Iteration 7010, loss = 341.89704394\n",
      "Iteration 7011, loss = 341.65193890\n",
      "Iteration 7012, loss = 341.42118678\n",
      "Iteration 7013, loss = 341.18755068\n",
      "Iteration 7014, loss = 340.90337493\n",
      "Iteration 7015, loss = 340.66382883\n",
      "Iteration 7016, loss = 340.43589223\n",
      "Iteration 7017, loss = 340.23640424\n",
      "Iteration 7018, loss = 339.94522855\n",
      "Iteration 7019, loss = 339.66701883\n",
      "Iteration 7020, loss = 339.42013832\n",
      "Iteration 7021, loss = 339.19634244\n",
      "Iteration 7022, loss = 338.99572667\n",
      "Iteration 7023, loss = 338.67802117\n",
      "Iteration 7024, loss = 338.44178445\n",
      "Iteration 7025, loss = 338.19468415\n",
      "Iteration 7026, loss = 337.94342319\n",
      "Iteration 7027, loss = 337.70081096\n",
      "Iteration 7028, loss = 337.44752674\n",
      "Iteration 7029, loss = 337.20225964\n",
      "Iteration 7030, loss = 336.95456887\n",
      "Iteration 7031, loss = 336.71098677\n",
      "Iteration 7032, loss = 336.48388049\n",
      "Iteration 7033, loss = 336.21911578\n",
      "Iteration 7034, loss = 335.97232844\n",
      "Iteration 7035, loss = 335.73369023\n",
      "Iteration 7036, loss = 335.49629487\n",
      "Iteration 7037, loss = 335.26074708\n",
      "Iteration 7038, loss = 335.00782030\n",
      "Iteration 7039, loss = 334.74930672\n",
      "Iteration 7040, loss = 334.50318861\n",
      "Iteration 7041, loss = 334.27320552\n",
      "Iteration 7042, loss = 334.06688631\n",
      "Iteration 7043, loss = 333.80510896\n",
      "Iteration 7044, loss = 333.54051108\n",
      "Iteration 7045, loss = 333.29583277\n",
      "Iteration 7046, loss = 333.05546301\n",
      "Iteration 7047, loss = 332.79900807\n",
      "Iteration 7048, loss = 332.56689572\n",
      "Iteration 7049, loss = 332.35598314\n",
      "Iteration 7050, loss = 332.08138705\n",
      "Iteration 7051, loss = 331.83213093\n",
      "Iteration 7052, loss = 331.60549528\n",
      "Iteration 7053, loss = 331.34734391\n",
      "Iteration 7054, loss = 331.10626490\n",
      "Iteration 7055, loss = 330.87410051\n",
      "Iteration 7056, loss = 330.63391052\n",
      "Iteration 7057, loss = 330.46228219\n",
      "Iteration 7058, loss = 330.14194263\n",
      "Iteration 7059, loss = 329.91074723\n",
      "Iteration 7060, loss = 329.66006202\n",
      "Iteration 7061, loss = 329.43143074\n",
      "Iteration 7062, loss = 329.21925432\n",
      "Iteration 7063, loss = 328.93778709\n",
      "Iteration 7064, loss = 328.69822936\n",
      "Iteration 7065, loss = 328.45276812\n",
      "Iteration 7066, loss = 328.21779574\n",
      "Iteration 7067, loss = 327.98924941\n",
      "Iteration 7068, loss = 327.75270090\n",
      "Iteration 7069, loss = 327.53022764\n",
      "Iteration 7070, loss = 327.24784817\n",
      "Iteration 7071, loss = 327.03214230\n",
      "Iteration 7072, loss = 326.78158285\n",
      "Iteration 7073, loss = 326.54429004\n",
      "Iteration 7074, loss = 326.29585000\n",
      "Iteration 7075, loss = 326.05394340\n",
      "Iteration 7076, loss = 325.82076604\n",
      "Iteration 7077, loss = 325.57935784\n",
      "Iteration 7078, loss = 325.40127055\n",
      "Iteration 7079, loss = 325.11698318\n",
      "Iteration 7080, loss = 324.87508262\n",
      "Iteration 7081, loss = 324.64846096\n",
      "Iteration 7082, loss = 324.39822284\n",
      "Iteration 7083, loss = 324.20161184\n",
      "Iteration 7084, loss = 323.91661235\n",
      "Iteration 7085, loss = 323.70271280\n",
      "Iteration 7086, loss = 323.43917124\n",
      "Iteration 7087, loss = 323.19169456\n",
      "Iteration 7088, loss = 322.97359989\n",
      "Iteration 7089, loss = 322.74067845\n",
      "Iteration 7090, loss = 322.51603789\n",
      "Iteration 7091, loss = 322.26298635\n",
      "Iteration 7092, loss = 322.10561750\n",
      "Iteration 7093, loss = 321.84122848\n",
      "Iteration 7094, loss = 321.61641251\n",
      "Iteration 7095, loss = 321.35800083\n",
      "Iteration 7096, loss = 321.07881884\n",
      "Iteration 7097, loss = 320.83981890\n",
      "Iteration 7098, loss = 320.62225442\n",
      "Iteration 7099, loss = 320.43705197\n",
      "Iteration 7100, loss = 320.14211979\n",
      "Iteration 7101, loss = 319.90737155\n",
      "Iteration 7102, loss = 319.67786287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7103, loss = 319.45408547\n",
      "Iteration 7104, loss = 319.20286729\n",
      "Iteration 7105, loss = 318.96895086\n",
      "Iteration 7106, loss = 318.72345076\n",
      "Iteration 7107, loss = 318.49605668\n",
      "Iteration 7108, loss = 318.26324799\n",
      "Iteration 7109, loss = 318.06048190\n",
      "Iteration 7110, loss = 317.80717006\n",
      "Iteration 7111, loss = 317.61283997\n",
      "Iteration 7112, loss = 317.33271047\n",
      "Iteration 7113, loss = 317.10673094\n",
      "Iteration 7114, loss = 316.86699288\n",
      "Iteration 7115, loss = 316.63721944\n",
      "Iteration 7116, loss = 316.40030759\n",
      "Iteration 7117, loss = 316.16001996\n",
      "Iteration 7118, loss = 315.93162189\n",
      "Iteration 7119, loss = 315.72409365\n",
      "Iteration 7120, loss = 315.47278975\n",
      "Iteration 7121, loss = 315.24349055\n",
      "Iteration 7122, loss = 315.03657609\n",
      "Iteration 7123, loss = 314.80197523\n",
      "Iteration 7124, loss = 314.55782461\n",
      "Iteration 7125, loss = 314.31347136\n",
      "Iteration 7126, loss = 314.08642485\n",
      "Iteration 7127, loss = 313.84780220\n",
      "Iteration 7128, loss = 313.61781470\n",
      "Iteration 7129, loss = 313.38174746\n",
      "Iteration 7130, loss = 313.17331534\n",
      "Iteration 7131, loss = 312.92427603\n",
      "Iteration 7132, loss = 312.71282146\n",
      "Iteration 7133, loss = 312.46891606\n",
      "Iteration 7134, loss = 312.23879139\n",
      "Iteration 7135, loss = 312.00207002\n",
      "Iteration 7136, loss = 311.78438109\n",
      "Iteration 7137, loss = 311.56666994\n",
      "Iteration 7138, loss = 311.33561898\n",
      "Iteration 7139, loss = 311.09980220\n",
      "Iteration 7140, loss = 310.86526893\n",
      "Iteration 7141, loss = 310.62930997\n",
      "Iteration 7142, loss = 310.40919199\n",
      "Iteration 7143, loss = 310.16182343\n",
      "Iteration 7144, loss = 309.94738970\n",
      "Iteration 7145, loss = 309.74682436\n",
      "Iteration 7146, loss = 309.51758762\n",
      "Iteration 7147, loss = 309.25167477\n",
      "Iteration 7148, loss = 309.02367450\n",
      "Iteration 7149, loss = 308.79564658\n",
      "Iteration 7150, loss = 308.56999420\n",
      "Iteration 7151, loss = 308.34163291\n",
      "Iteration 7152, loss = 308.13223935\n",
      "Iteration 7153, loss = 307.89348112\n",
      "Iteration 7154, loss = 307.65425032\n",
      "Iteration 7155, loss = 307.43531803\n",
      "Iteration 7156, loss = 307.20913701\n",
      "Iteration 7157, loss = 306.97836096\n",
      "Iteration 7158, loss = 306.74882924\n",
      "Iteration 7159, loss = 306.52586581\n",
      "Iteration 7160, loss = 306.29985615\n",
      "Iteration 7161, loss = 306.06786238\n",
      "Iteration 7162, loss = 305.84091135\n",
      "Iteration 7163, loss = 305.63183146\n",
      "Iteration 7164, loss = 305.40921660\n",
      "Iteration 7165, loss = 305.17574062\n",
      "Iteration 7166, loss = 304.94131122\n",
      "Iteration 7167, loss = 304.72296971\n",
      "Iteration 7168, loss = 304.49672551\n",
      "Iteration 7169, loss = 304.28536348\n",
      "Iteration 7170, loss = 304.04671784\n",
      "Iteration 7171, loss = 303.88443491\n",
      "Iteration 7172, loss = 303.58369138\n",
      "Iteration 7173, loss = 303.37144757\n",
      "Iteration 7174, loss = 303.17980073\n",
      "Iteration 7175, loss = 302.92333594\n",
      "Iteration 7176, loss = 302.69464291\n",
      "Iteration 7177, loss = 302.49567378\n",
      "Iteration 7178, loss = 302.24683580\n",
      "Iteration 7179, loss = 302.03185142\n",
      "Iteration 7180, loss = 301.79393142\n",
      "Iteration 7181, loss = 301.57008736\n",
      "Iteration 7182, loss = 301.42769419\n",
      "Iteration 7183, loss = 301.17111087\n",
      "Iteration 7184, loss = 300.91217097\n",
      "Iteration 7185, loss = 300.68807409\n",
      "Iteration 7186, loss = 300.46067596\n",
      "Iteration 7187, loss = 300.22952231\n",
      "Iteration 7188, loss = 300.01778457\n",
      "Iteration 7189, loss = 299.82056621\n",
      "Iteration 7190, loss = 299.58474776\n",
      "Iteration 7191, loss = 299.34078708\n",
      "Iteration 7192, loss = 299.12109924\n",
      "Iteration 7193, loss = 298.90094990\n",
      "Iteration 7194, loss = 298.67330152\n",
      "Iteration 7195, loss = 298.45390940\n",
      "Iteration 7196, loss = 298.30028867\n",
      "Iteration 7197, loss = 298.09517974\n",
      "Iteration 7198, loss = 297.79962462\n",
      "Iteration 7199, loss = 297.59507913\n",
      "Iteration 7200, loss = 297.37756719\n",
      "Iteration 7201, loss = 297.14385168\n",
      "Iteration 7202, loss = 296.95228028\n",
      "Iteration 7203, loss = 296.71141838\n",
      "Iteration 7204, loss = 296.48950947\n",
      "Iteration 7205, loss = 296.28127984\n",
      "Iteration 7206, loss = 296.02264252\n",
      "Iteration 7207, loss = 295.80683855\n",
      "Iteration 7208, loss = 295.60171123\n",
      "Iteration 7209, loss = 295.38815855\n",
      "Iteration 7210, loss = 295.16274460\n",
      "Iteration 7211, loss = 294.95305028\n",
      "Iteration 7212, loss = 294.70596744\n",
      "Iteration 7213, loss = 294.55209307\n",
      "Iteration 7214, loss = 294.27676843\n",
      "Iteration 7215, loss = 294.05541568\n",
      "Iteration 7216, loss = 293.83418129\n",
      "Iteration 7217, loss = 293.65824431\n",
      "Iteration 7218, loss = 293.39917775\n",
      "Iteration 7219, loss = 293.17889147\n",
      "Iteration 7220, loss = 292.95547501\n",
      "Iteration 7221, loss = 292.74928915\n",
      "Iteration 7222, loss = 292.54990134\n",
      "Iteration 7223, loss = 292.33894662\n",
      "Iteration 7224, loss = 292.10819690\n",
      "Iteration 7225, loss = 291.88570788\n",
      "Iteration 7226, loss = 291.66633029\n",
      "Iteration 7227, loss = 291.44806900\n",
      "Iteration 7228, loss = 291.23628568\n",
      "Iteration 7229, loss = 291.00560480\n",
      "Iteration 7230, loss = 290.80759152\n",
      "Iteration 7231, loss = 290.59081803\n",
      "Iteration 7232, loss = 290.35224052\n",
      "Iteration 7233, loss = 290.13831713\n",
      "Iteration 7234, loss = 289.93005288\n",
      "Iteration 7235, loss = 289.70173949\n",
      "Iteration 7236, loss = 289.49454764\n",
      "Iteration 7237, loss = 289.27592106\n",
      "Iteration 7238, loss = 289.06301887\n",
      "Iteration 7239, loss = 288.84038693\n",
      "Iteration 7240, loss = 288.68072576\n",
      "Iteration 7241, loss = 288.40848631\n",
      "Iteration 7242, loss = 288.22435184\n",
      "Iteration 7243, loss = 287.98679041\n",
      "Iteration 7244, loss = 287.77162569\n",
      "Iteration 7245, loss = 287.59283908\n",
      "Iteration 7246, loss = 287.35138133\n",
      "Iteration 7247, loss = 287.14390636\n",
      "Iteration 7248, loss = 286.92088372\n",
      "Iteration 7249, loss = 286.72704284\n",
      "Iteration 7250, loss = 286.48493582\n",
      "Iteration 7251, loss = 286.28362274\n",
      "Iteration 7252, loss = 286.05394635\n",
      "Iteration 7253, loss = 285.88585525\n",
      "Iteration 7254, loss = 285.61111108\n",
      "Iteration 7255, loss = 285.42719215\n",
      "Iteration 7256, loss = 285.20415615\n",
      "Iteration 7257, loss = 284.98569483\n",
      "Iteration 7258, loss = 284.77777485\n",
      "Iteration 7259, loss = 284.58384921\n",
      "Iteration 7260, loss = 284.34360308\n",
      "Iteration 7261, loss = 284.17481976\n",
      "Iteration 7262, loss = 283.91561277\n",
      "Iteration 7263, loss = 283.70490519\n",
      "Iteration 7264, loss = 283.57902905\n",
      "Iteration 7265, loss = 283.28004072\n",
      "Iteration 7266, loss = 283.07795773\n",
      "Iteration 7267, loss = 282.87019641\n",
      "Iteration 7268, loss = 282.72827423\n",
      "Iteration 7269, loss = 282.42548138\n",
      "Iteration 7270, loss = 282.22848611\n",
      "Iteration 7271, loss = 282.01305826\n",
      "Iteration 7272, loss = 281.81643378\n",
      "Iteration 7273, loss = 281.60267770\n",
      "Iteration 7274, loss = 281.36828196\n",
      "Iteration 7275, loss = 281.16805151\n",
      "Iteration 7276, loss = 280.95607770\n",
      "Iteration 7277, loss = 280.76764504\n",
      "Iteration 7278, loss = 280.53006278\n",
      "Iteration 7279, loss = 280.33560931\n",
      "Iteration 7280, loss = 280.11603847\n",
      "Iteration 7281, loss = 279.92191439\n",
      "Iteration 7282, loss = 279.69079443\n",
      "Iteration 7283, loss = 279.47947642\n",
      "Iteration 7284, loss = 279.27380464\n",
      "Iteration 7285, loss = 279.07276293\n",
      "Iteration 7286, loss = 278.87275198\n",
      "Iteration 7287, loss = 278.64885491\n",
      "Iteration 7288, loss = 278.43461430\n",
      "Iteration 7289, loss = 278.25214996\n",
      "Iteration 7290, loss = 278.02549523\n",
      "Iteration 7291, loss = 277.83673173\n",
      "Iteration 7292, loss = 277.59674493\n",
      "Iteration 7293, loss = 277.38731056\n",
      "Iteration 7294, loss = 277.17903366\n",
      "Iteration 7295, loss = 276.97261441\n",
      "Iteration 7296, loss = 276.77631940\n",
      "Iteration 7297, loss = 276.57169194\n",
      "Iteration 7298, loss = 276.36645001\n",
      "Iteration 7299, loss = 276.15606496\n",
      "Iteration 7300, loss = 275.93784872\n",
      "Iteration 7301, loss = 275.71853608\n",
      "Iteration 7302, loss = 275.53835445\n",
      "Iteration 7303, loss = 275.32655733\n",
      "Iteration 7304, loss = 275.11782030\n",
      "Iteration 7305, loss = 274.90358271\n",
      "Iteration 7306, loss = 274.69264700\n",
      "Iteration 7307, loss = 274.49042739\n",
      "Iteration 7308, loss = 274.31263635\n",
      "Iteration 7309, loss = 274.06909184\n",
      "Iteration 7310, loss = 273.86933687\n",
      "Iteration 7311, loss = 273.68373969\n",
      "Iteration 7312, loss = 273.46623955\n",
      "Iteration 7313, loss = 273.24197909\n",
      "Iteration 7314, loss = 273.08351991\n",
      "Iteration 7315, loss = 272.84945646\n",
      "Iteration 7316, loss = 272.63580226\n",
      "Iteration 7317, loss = 272.47174165\n",
      "Iteration 7318, loss = 272.21571977\n",
      "Iteration 7319, loss = 272.05956992\n",
      "Iteration 7320, loss = 271.80294143\n",
      "Iteration 7321, loss = 271.59929272\n",
      "Iteration 7322, loss = 271.42176388\n",
      "Iteration 7323, loss = 271.18923928\n",
      "Iteration 7324, loss = 271.01986539\n",
      "Iteration 7325, loss = 270.79489878\n",
      "Iteration 7326, loss = 270.60169821\n",
      "Iteration 7327, loss = 270.37233519\n",
      "Iteration 7328, loss = 270.16252653\n",
      "Iteration 7329, loss = 270.00504260\n",
      "Iteration 7330, loss = 269.80052311\n",
      "Iteration 7331, loss = 269.62557682\n",
      "Iteration 7332, loss = 269.37305426\n",
      "Iteration 7333, loss = 269.16324639\n",
      "Iteration 7334, loss = 268.94746647\n",
      "Iteration 7335, loss = 268.75304563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7336, loss = 268.54135186\n",
      "Iteration 7337, loss = 268.41508338\n",
      "Iteration 7338, loss = 268.14548405\n",
      "Iteration 7339, loss = 267.94967114\n",
      "Iteration 7340, loss = 267.75172643\n",
      "Iteration 7341, loss = 267.52527550\n",
      "Iteration 7342, loss = 267.31537074\n",
      "Iteration 7343, loss = 267.11943164\n",
      "Iteration 7344, loss = 266.95929933\n",
      "Iteration 7345, loss = 266.72294295\n",
      "Iteration 7346, loss = 266.51663961\n",
      "Iteration 7347, loss = 266.31706572\n",
      "Iteration 7348, loss = 266.12963194\n",
      "Iteration 7349, loss = 265.90867742\n",
      "Iteration 7350, loss = 265.72416177\n",
      "Iteration 7351, loss = 265.53643763\n",
      "Iteration 7352, loss = 265.32111622\n",
      "Iteration 7353, loss = 265.10412817\n",
      "Iteration 7354, loss = 264.90147476\n",
      "Iteration 7355, loss = 264.70587190\n",
      "Iteration 7356, loss = 264.50749512\n",
      "Iteration 7357, loss = 264.30283192\n",
      "Iteration 7358, loss = 264.10939331\n",
      "Iteration 7359, loss = 263.92885344\n",
      "Iteration 7360, loss = 263.74159673\n",
      "Iteration 7361, loss = 263.50441287\n",
      "Iteration 7362, loss = 263.34540998\n",
      "Iteration 7363, loss = 263.10573617\n",
      "Iteration 7364, loss = 262.92173388\n",
      "Iteration 7365, loss = 262.71119020\n",
      "Iteration 7366, loss = 262.50078884\n",
      "Iteration 7367, loss = 262.33373227\n",
      "Iteration 7368, loss = 262.11189795\n",
      "Iteration 7369, loss = 261.90418338\n",
      "Iteration 7370, loss = 261.73107114\n",
      "Iteration 7371, loss = 261.56121600\n",
      "Iteration 7372, loss = 261.30368022\n",
      "Iteration 7373, loss = 261.11573428\n",
      "Iteration 7374, loss = 260.94045854\n",
      "Iteration 7375, loss = 260.70931745\n",
      "Iteration 7376, loss = 260.52021655\n",
      "Iteration 7377, loss = 260.34830925\n",
      "Iteration 7378, loss = 260.14893789\n",
      "Iteration 7379, loss = 259.92195071\n",
      "Iteration 7380, loss = 259.71333445\n",
      "Iteration 7381, loss = 259.53747931\n",
      "Iteration 7382, loss = 259.32764270\n",
      "Iteration 7383, loss = 259.12774460\n",
      "Iteration 7384, loss = 258.92874345\n",
      "Iteration 7385, loss = 258.73293076\n",
      "Iteration 7386, loss = 258.54587577\n",
      "Iteration 7387, loss = 258.38099610\n",
      "Iteration 7388, loss = 258.15777017\n",
      "Iteration 7389, loss = 257.95541205\n",
      "Iteration 7390, loss = 257.74110046\n",
      "Iteration 7391, loss = 257.57171202\n",
      "Iteration 7392, loss = 257.36943340\n",
      "Iteration 7393, loss = 257.15897116\n",
      "Iteration 7394, loss = 256.98746770\n",
      "Iteration 7395, loss = 256.76487452\n",
      "Iteration 7396, loss = 256.58236194\n",
      "Iteration 7397, loss = 256.40218510\n",
      "Iteration 7398, loss = 256.17903809\n",
      "Iteration 7399, loss = 255.98469162\n",
      "Iteration 7400, loss = 255.80545560\n",
      "Iteration 7401, loss = 255.60505791\n",
      "Iteration 7402, loss = 255.44248558\n",
      "Iteration 7403, loss = 255.26298296\n",
      "Iteration 7404, loss = 255.00349109\n",
      "Iteration 7405, loss = 254.81241167\n",
      "Iteration 7406, loss = 254.63182986\n",
      "Iteration 7407, loss = 254.41769209\n",
      "Iteration 7408, loss = 254.23369020\n",
      "Iteration 7409, loss = 254.02949720\n",
      "Iteration 7410, loss = 253.85305559\n",
      "Iteration 7411, loss = 253.66159107\n",
      "Iteration 7412, loss = 253.46860097\n",
      "Iteration 7413, loss = 253.26067870\n",
      "Iteration 7414, loss = 253.06997212\n",
      "Iteration 7415, loss = 252.87497592\n",
      "Iteration 7416, loss = 252.68601399\n",
      "Iteration 7417, loss = 252.48398403\n",
      "Iteration 7418, loss = 252.29076600\n",
      "Iteration 7419, loss = 252.09301430\n",
      "Iteration 7420, loss = 251.90177443\n",
      "Iteration 7421, loss = 251.70559564\n",
      "Iteration 7422, loss = 251.52154754\n",
      "Iteration 7423, loss = 251.34582774\n",
      "Iteration 7424, loss = 251.15541158\n",
      "Iteration 7425, loss = 250.94683266\n",
      "Iteration 7426, loss = 250.74768663\n",
      "Iteration 7427, loss = 250.56359413\n",
      "Iteration 7428, loss = 250.37906233\n",
      "Iteration 7429, loss = 250.17125631\n",
      "Iteration 7430, loss = 249.98574728\n",
      "Iteration 7431, loss = 249.78516577\n",
      "Iteration 7432, loss = 249.61859101\n",
      "Iteration 7433, loss = 249.39951040\n",
      "Iteration 7434, loss = 249.20961565\n",
      "Iteration 7435, loss = 249.03801588\n",
      "Iteration 7436, loss = 248.83278278\n",
      "Iteration 7437, loss = 248.64677552\n",
      "Iteration 7438, loss = 248.44236946\n",
      "Iteration 7439, loss = 248.27287507\n",
      "Iteration 7440, loss = 248.06556632\n",
      "Iteration 7441, loss = 247.87040143\n",
      "Iteration 7442, loss = 247.72585792\n",
      "Iteration 7443, loss = 247.48036135\n",
      "Iteration 7444, loss = 247.29658780\n",
      "Iteration 7445, loss = 247.11857264\n",
      "Iteration 7446, loss = 246.92214287\n",
      "Iteration 7447, loss = 246.72890514\n",
      "Iteration 7448, loss = 246.53451012\n",
      "Iteration 7449, loss = 246.35830946\n",
      "Iteration 7450, loss = 246.15375687\n",
      "Iteration 7451, loss = 245.98766884\n",
      "Iteration 7452, loss = 245.79178057\n",
      "Iteration 7453, loss = 245.58356588\n",
      "Iteration 7454, loss = 245.41965687\n",
      "Iteration 7455, loss = 245.22663768\n",
      "Iteration 7456, loss = 245.03648528\n",
      "Iteration 7457, loss = 244.83310539\n",
      "Iteration 7458, loss = 244.65697798\n",
      "Iteration 7459, loss = 244.45257385\n",
      "Iteration 7460, loss = 244.28215559\n",
      "Iteration 7461, loss = 244.10268345\n",
      "Iteration 7462, loss = 243.89220742\n",
      "Iteration 7463, loss = 243.69566936\n",
      "Iteration 7464, loss = 243.51777175\n",
      "Iteration 7465, loss = 243.32295319\n",
      "Iteration 7466, loss = 243.15434313\n",
      "Iteration 7467, loss = 242.94484316\n",
      "Iteration 7468, loss = 242.78400307\n",
      "Iteration 7469, loss = 242.56613139\n",
      "Iteration 7470, loss = 242.40667175\n",
      "Iteration 7471, loss = 242.20569090\n",
      "Iteration 7472, loss = 242.01362083\n",
      "Iteration 7473, loss = 241.81924279\n",
      "Iteration 7474, loss = 241.66841448\n",
      "Iteration 7475, loss = 241.46405901\n",
      "Iteration 7476, loss = 241.26067477\n",
      "Iteration 7477, loss = 241.07811984\n",
      "Iteration 7478, loss = 240.91010374\n",
      "Iteration 7479, loss = 240.69363765\n",
      "Iteration 7480, loss = 240.52226858\n",
      "Iteration 7481, loss = 240.33740768\n",
      "Iteration 7482, loss = 240.16978745\n",
      "Iteration 7483, loss = 239.98437410\n",
      "Iteration 7484, loss = 239.80508808\n",
      "Iteration 7485, loss = 239.58344101\n",
      "Iteration 7486, loss = 239.41100560\n",
      "Iteration 7487, loss = 239.23743379\n",
      "Iteration 7488, loss = 239.06151976\n",
      "Iteration 7489, loss = 238.87790767\n",
      "Iteration 7490, loss = 238.67498177\n",
      "Iteration 7491, loss = 238.48552904\n",
      "Iteration 7492, loss = 238.27716445\n",
      "Iteration 7493, loss = 238.09403789\n",
      "Iteration 7494, loss = 237.91857716\n",
      "Iteration 7495, loss = 237.73617165\n",
      "Iteration 7496, loss = 237.55075353\n",
      "Iteration 7497, loss = 237.34758585\n",
      "Iteration 7498, loss = 237.17717318\n",
      "Iteration 7499, loss = 237.00007299\n",
      "Iteration 7500, loss = 236.82496212\n",
      "Iteration 7501, loss = 236.64081720\n",
      "Iteration 7502, loss = 236.44703676\n",
      "Iteration 7503, loss = 236.24509579\n",
      "Iteration 7504, loss = 236.06439482\n",
      "Iteration 7505, loss = 235.87302609\n",
      "Iteration 7506, loss = 235.70117914\n",
      "Iteration 7507, loss = 235.52203799\n",
      "Iteration 7508, loss = 235.33608931\n",
      "Iteration 7509, loss = 235.14566370\n",
      "Iteration 7510, loss = 234.95319854\n",
      "Iteration 7511, loss = 234.78471746\n",
      "Iteration 7512, loss = 234.58469450\n",
      "Iteration 7513, loss = 234.42575745\n",
      "Iteration 7514, loss = 234.22125012\n",
      "Iteration 7515, loss = 234.06030351\n",
      "Iteration 7516, loss = 233.84587806\n",
      "Iteration 7517, loss = 233.71420061\n",
      "Iteration 7518, loss = 233.49987386\n",
      "Iteration 7519, loss = 233.31169430\n",
      "Iteration 7520, loss = 233.12250780\n",
      "Iteration 7521, loss = 232.93090100\n",
      "Iteration 7522, loss = 232.78323976\n",
      "Iteration 7523, loss = 232.56509537\n",
      "Iteration 7524, loss = 232.38151468\n",
      "Iteration 7525, loss = 232.23509694\n",
      "Iteration 7526, loss = 232.01822657\n",
      "Iteration 7527, loss = 231.83934790\n",
      "Iteration 7528, loss = 231.67026169\n",
      "Iteration 7529, loss = 231.53142855\n",
      "Iteration 7530, loss = 231.29428177\n",
      "Iteration 7531, loss = 231.12292476\n",
      "Iteration 7532, loss = 230.91624249\n",
      "Iteration 7533, loss = 230.72518230\n",
      "Iteration 7534, loss = 230.55508484\n",
      "Iteration 7535, loss = 230.37350067\n",
      "Iteration 7536, loss = 230.19269076\n",
      "Iteration 7537, loss = 230.00935585\n",
      "Iteration 7538, loss = 229.80805545\n",
      "Iteration 7539, loss = 229.62092095\n",
      "Iteration 7540, loss = 229.46589338\n",
      "Iteration 7541, loss = 229.24930819\n",
      "Iteration 7542, loss = 229.07520658\n",
      "Iteration 7543, loss = 228.87303232\n",
      "Iteration 7544, loss = 228.74272111\n",
      "Iteration 7545, loss = 228.49851738\n",
      "Iteration 7546, loss = 228.30295265\n",
      "Iteration 7547, loss = 228.12839409\n",
      "Iteration 7548, loss = 227.93132499\n",
      "Iteration 7549, loss = 227.74160851\n",
      "Iteration 7550, loss = 227.53995974\n",
      "Iteration 7551, loss = 227.36527607\n",
      "Iteration 7552, loss = 227.17294217\n",
      "Iteration 7553, loss = 226.96154729\n",
      "Iteration 7554, loss = 226.77292440\n",
      "Iteration 7555, loss = 226.56515864\n",
      "Iteration 7556, loss = 226.39095787\n",
      "Iteration 7557, loss = 226.14845765\n",
      "Iteration 7558, loss = 225.95783924\n",
      "Iteration 7559, loss = 225.72228273\n",
      "Iteration 7560, loss = 225.52093473\n",
      "Iteration 7561, loss = 225.32007826\n",
      "Iteration 7562, loss = 225.06835204\n",
      "Iteration 7563, loss = 224.83618150\n",
      "Iteration 7564, loss = 224.59945403\n",
      "Iteration 7565, loss = 224.34081547\n",
      "Iteration 7566, loss = 224.09919907\n",
      "Iteration 7567, loss = 223.85219079\n",
      "Iteration 7568, loss = 223.58908663\n",
      "Iteration 7569, loss = 223.29255199\n",
      "Iteration 7570, loss = 223.01034452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7571, loss = 222.70044349\n",
      "Iteration 7572, loss = 222.38932427\n",
      "Iteration 7573, loss = 222.08341000\n",
      "Iteration 7574, loss = 221.72878920\n",
      "Iteration 7575, loss = 221.45625001\n",
      "Iteration 7576, loss = 221.02766484\n",
      "Iteration 7577, loss = 220.65721475\n",
      "Iteration 7578, loss = 220.29668386\n",
      "Iteration 7579, loss = 219.91453195\n",
      "Iteration 7580, loss = 219.50312674\n",
      "Iteration 7581, loss = 219.11034318\n",
      "Iteration 7582, loss = 218.72927066\n",
      "Iteration 7583, loss = 218.34509236\n",
      "Iteration 7584, loss = 217.89269429\n",
      "Iteration 7585, loss = 217.49408998\n",
      "Iteration 7586, loss = 217.07908086\n",
      "Iteration 7587, loss = 216.68675484\n",
      "Iteration 7588, loss = 216.26646369\n",
      "Iteration 7589, loss = 215.86481904\n",
      "Iteration 7590, loss = 215.47537121\n",
      "Iteration 7591, loss = 215.06376199\n",
      "Iteration 7592, loss = 214.66796862\n",
      "Iteration 7593, loss = 214.26681017\n",
      "Iteration 7594, loss = 213.88323836\n",
      "Iteration 7595, loss = 213.54159297\n",
      "Iteration 7596, loss = 213.13271381\n",
      "Iteration 7597, loss = 212.76260324\n",
      "Iteration 7598, loss = 212.43945788\n",
      "Iteration 7599, loss = 212.03704778\n",
      "Iteration 7600, loss = 211.63775618\n",
      "Iteration 7601, loss = 211.28654786\n",
      "Iteration 7602, loss = 210.93784320\n",
      "Iteration 7603, loss = 210.59065857\n",
      "Iteration 7604, loss = 210.24332329\n",
      "Iteration 7605, loss = 209.88750026\n",
      "Iteration 7606, loss = 209.55005706\n",
      "Iteration 7607, loss = 209.20395945\n",
      "Iteration 7608, loss = 208.87514520\n",
      "Iteration 7609, loss = 208.54166778\n",
      "Iteration 7610, loss = 208.20048606\n",
      "Iteration 7611, loss = 207.88879150\n",
      "Iteration 7612, loss = 207.57259176\n",
      "Iteration 7613, loss = 207.24351202\n",
      "Iteration 7614, loss = 206.90873531\n",
      "Iteration 7615, loss = 206.60435122\n",
      "Iteration 7616, loss = 206.28338146\n",
      "Iteration 7617, loss = 205.96814211\n",
      "Iteration 7618, loss = 205.66451898\n",
      "Iteration 7619, loss = 205.35889821\n",
      "Iteration 7620, loss = 205.05964733\n",
      "Iteration 7621, loss = 204.74413670\n",
      "Iteration 7622, loss = 204.45276271\n",
      "Iteration 7623, loss = 204.15042305\n",
      "Iteration 7624, loss = 203.84882112\n",
      "Iteration 7625, loss = 203.55370753\n",
      "Iteration 7626, loss = 203.27810271\n",
      "Iteration 7627, loss = 202.96302513\n",
      "Iteration 7628, loss = 202.68058502\n",
      "Iteration 7629, loss = 202.38307099\n",
      "Iteration 7630, loss = 202.09475648\n",
      "Iteration 7631, loss = 201.82550800\n",
      "Iteration 7632, loss = 201.52682096\n",
      "Iteration 7633, loss = 201.25202377\n",
      "Iteration 7634, loss = 200.96025775\n",
      "Iteration 7635, loss = 200.68837753\n",
      "Iteration 7636, loss = 200.41425258\n",
      "Iteration 7637, loss = 200.11987616\n",
      "Iteration 7638, loss = 199.86181385\n",
      "Iteration 7639, loss = 199.56734038\n",
      "Iteration 7640, loss = 199.30233551\n",
      "Iteration 7641, loss = 199.03161211\n",
      "Iteration 7642, loss = 198.76699976\n",
      "Iteration 7643, loss = 198.48041604\n",
      "Iteration 7644, loss = 198.20750912\n",
      "Iteration 7645, loss = 197.94578442\n",
      "Iteration 7646, loss = 197.66786675\n",
      "Iteration 7647, loss = 197.40632340\n",
      "Iteration 7648, loss = 197.14183309\n",
      "Iteration 7649, loss = 196.92773288\n",
      "Iteration 7650, loss = 196.63681577\n",
      "Iteration 7651, loss = 196.35904905\n",
      "Iteration 7652, loss = 196.09365793\n",
      "Iteration 7653, loss = 195.82329555\n",
      "Iteration 7654, loss = 195.56825565\n",
      "Iteration 7655, loss = 195.30777230\n",
      "Iteration 7656, loss = 195.04672653\n",
      "Iteration 7657, loss = 194.78930620\n",
      "Iteration 7658, loss = 194.53652801\n",
      "Iteration 7659, loss = 194.28961867\n",
      "Iteration 7660, loss = 194.03397014\n",
      "Iteration 7661, loss = 193.75534248\n",
      "Iteration 7662, loss = 193.51398750\n",
      "Iteration 7663, loss = 193.25842823\n",
      "Iteration 7664, loss = 193.02346371\n",
      "Iteration 7665, loss = 192.73033124\n",
      "Iteration 7666, loss = 192.48071706\n",
      "Iteration 7667, loss = 192.24342195\n",
      "Iteration 7668, loss = 191.97957511\n",
      "Iteration 7669, loss = 191.72119900\n",
      "Iteration 7670, loss = 191.46563578\n",
      "Iteration 7671, loss = 191.20021957\n",
      "Iteration 7672, loss = 190.94526454\n",
      "Iteration 7673, loss = 190.69620641\n",
      "Iteration 7674, loss = 190.48239337\n",
      "Iteration 7675, loss = 190.17989903\n",
      "Iteration 7676, loss = 189.93036055\n",
      "Iteration 7677, loss = 189.66442999\n",
      "Iteration 7678, loss = 189.42529912\n",
      "Iteration 7679, loss = 189.13362788\n",
      "Iteration 7680, loss = 188.88127502\n",
      "Iteration 7681, loss = 188.60912321\n",
      "Iteration 7682, loss = 188.35170068\n",
      "Iteration 7683, loss = 188.06685792\n",
      "Iteration 7684, loss = 187.79613682\n",
      "Iteration 7685, loss = 187.52762802\n",
      "Iteration 7686, loss = 187.25884221\n",
      "Iteration 7687, loss = 186.94295602\n",
      "Iteration 7688, loss = 186.64849860\n",
      "Iteration 7689, loss = 186.35428137\n",
      "Iteration 7690, loss = 186.06973002\n",
      "Iteration 7691, loss = 185.76696009\n",
      "Iteration 7692, loss = 185.42316513\n",
      "Iteration 7693, loss = 185.09553353\n",
      "Iteration 7694, loss = 184.78100755\n",
      "Iteration 7695, loss = 184.44710856\n",
      "Iteration 7696, loss = 184.11111282\n",
      "Iteration 7697, loss = 183.74535541\n",
      "Iteration 7698, loss = 183.38451661\n",
      "Iteration 7699, loss = 183.05972794\n",
      "Iteration 7700, loss = 182.63142722\n",
      "Iteration 7701, loss = 182.25363058\n",
      "Iteration 7702, loss = 181.87294356\n",
      "Iteration 7703, loss = 181.46449542\n",
      "Iteration 7704, loss = 181.06897421\n",
      "Iteration 7705, loss = 180.66508636\n",
      "Iteration 7706, loss = 180.26383572\n",
      "Iteration 7707, loss = 179.84926575\n",
      "Iteration 7708, loss = 179.45247055\n",
      "Iteration 7709, loss = 179.03766760\n",
      "Iteration 7710, loss = 178.61903639\n",
      "Iteration 7711, loss = 178.21572145\n",
      "Iteration 7712, loss = 177.80850610\n",
      "Iteration 7713, loss = 177.39046165\n",
      "Iteration 7714, loss = 176.98467790\n",
      "Iteration 7715, loss = 176.57943608\n",
      "Iteration 7716, loss = 176.17752146\n",
      "Iteration 7717, loss = 175.77470810\n",
      "Iteration 7718, loss = 175.39757274\n",
      "Iteration 7719, loss = 174.99941723\n",
      "Iteration 7720, loss = 174.61163075\n",
      "Iteration 7721, loss = 174.21768407\n",
      "Iteration 7722, loss = 173.83784774\n",
      "Iteration 7723, loss = 173.46640757\n",
      "Iteration 7724, loss = 173.10535101\n",
      "Iteration 7725, loss = 172.71853976\n",
      "Iteration 7726, loss = 172.36025535\n",
      "Iteration 7727, loss = 171.99494760\n",
      "Iteration 7728, loss = 171.63814378\n",
      "Iteration 7729, loss = 171.27772893\n",
      "Iteration 7730, loss = 170.92181591\n",
      "Iteration 7731, loss = 170.57036003\n",
      "Iteration 7732, loss = 170.22137285\n",
      "Iteration 7733, loss = 169.88094196\n",
      "Iteration 7734, loss = 169.54577702\n",
      "Iteration 7735, loss = 169.19636713\n",
      "Iteration 7736, loss = 168.85510061\n",
      "Iteration 7737, loss = 168.52663432\n",
      "Iteration 7738, loss = 168.20131481\n",
      "Iteration 7739, loss = 167.86645936\n",
      "Iteration 7740, loss = 167.55474392\n",
      "Iteration 7741, loss = 167.23817029\n",
      "Iteration 7742, loss = 166.93625094\n",
      "Iteration 7743, loss = 166.58280257\n",
      "Iteration 7744, loss = 166.27299726\n",
      "Iteration 7745, loss = 165.95607466\n",
      "Iteration 7746, loss = 165.65212273\n",
      "Iteration 7747, loss = 165.34940149\n",
      "Iteration 7748, loss = 165.03972091\n",
      "Iteration 7749, loss = 164.74494052\n",
      "Iteration 7750, loss = 164.42453504\n",
      "Iteration 7751, loss = 164.12182873\n",
      "Iteration 7752, loss = 163.82506291\n",
      "Iteration 7753, loss = 163.52225299\n",
      "Iteration 7754, loss = 163.22469176\n",
      "Iteration 7755, loss = 162.94217715\n",
      "Iteration 7756, loss = 162.63434146\n",
      "Iteration 7757, loss = 162.35134321\n",
      "Iteration 7758, loss = 162.05523640\n",
      "Iteration 7759, loss = 161.78850714\n",
      "Iteration 7760, loss = 161.47766447\n",
      "Iteration 7761, loss = 161.20098373\n",
      "Iteration 7762, loss = 160.91553876\n",
      "Iteration 7763, loss = 160.63775274\n",
      "Iteration 7764, loss = 160.35197729\n",
      "Iteration 7765, loss = 160.07018372\n",
      "Iteration 7766, loss = 159.80597909\n",
      "Iteration 7767, loss = 159.53330156\n",
      "Iteration 7768, loss = 159.24762186\n",
      "Iteration 7769, loss = 158.98147426\n",
      "Iteration 7770, loss = 158.69871206\n",
      "Iteration 7771, loss = 158.42590271\n",
      "Iteration 7772, loss = 158.17397061\n",
      "Iteration 7773, loss = 157.89463967\n",
      "Iteration 7774, loss = 157.62939259\n",
      "Iteration 7775, loss = 157.35988308\n",
      "Iteration 7776, loss = 157.09265259\n",
      "Iteration 7777, loss = 156.84543685\n",
      "Iteration 7778, loss = 156.56309396\n",
      "Iteration 7779, loss = 156.31373578\n",
      "Iteration 7780, loss = 156.04629467\n",
      "Iteration 7781, loss = 155.79207902\n",
      "Iteration 7782, loss = 155.54061234\n",
      "Iteration 7783, loss = 155.27196352\n",
      "Iteration 7784, loss = 155.01781922\n",
      "Iteration 7785, loss = 154.77071333\n",
      "Iteration 7786, loss = 154.50504931\n",
      "Iteration 7787, loss = 154.25232827\n",
      "Iteration 7788, loss = 154.00594644\n",
      "Iteration 7789, loss = 153.76526414\n",
      "Iteration 7790, loss = 153.50600135\n",
      "Iteration 7791, loss = 153.24989006\n",
      "Iteration 7792, loss = 153.00624278\n",
      "Iteration 7793, loss = 152.76247392\n",
      "Iteration 7794, loss = 152.50900591\n",
      "Iteration 7795, loss = 152.26890543\n",
      "Iteration 7796, loss = 152.01669876\n",
      "Iteration 7797, loss = 151.77614148\n",
      "Iteration 7798, loss = 151.53873233\n",
      "Iteration 7799, loss = 151.28849608\n",
      "Iteration 7800, loss = 151.05442024\n",
      "Iteration 7801, loss = 150.82371420\n",
      "Iteration 7802, loss = 150.57273041\n",
      "Iteration 7803, loss = 150.34441070\n",
      "Iteration 7804, loss = 150.10242014\n",
      "Iteration 7805, loss = 149.86020849\n",
      "Iteration 7806, loss = 149.62964422\n",
      "Iteration 7807, loss = 149.39156724\n",
      "Iteration 7808, loss = 149.17032599\n",
      "Iteration 7809, loss = 148.93965216\n",
      "Iteration 7810, loss = 148.69551813\n",
      "Iteration 7811, loss = 148.44958600\n",
      "Iteration 7812, loss = 148.22245119\n",
      "Iteration 7813, loss = 147.99402605\n",
      "Iteration 7814, loss = 147.77375356\n",
      "Iteration 7815, loss = 147.53938377\n",
      "Iteration 7816, loss = 147.30632205\n",
      "Iteration 7817, loss = 147.07536988\n",
      "Iteration 7818, loss = 146.85079325\n",
      "Iteration 7819, loss = 146.63041871\n",
      "Iteration 7820, loss = 146.39743284\n",
      "Iteration 7821, loss = 146.16636602\n",
      "Iteration 7822, loss = 145.94966052\n",
      "Iteration 7823, loss = 145.72626014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7824, loss = 145.50109821\n",
      "Iteration 7825, loss = 145.28155264\n",
      "Iteration 7826, loss = 145.05840291\n",
      "Iteration 7827, loss = 144.83748926\n",
      "Iteration 7828, loss = 144.61209585\n",
      "Iteration 7829, loss = 144.40073577\n",
      "Iteration 7830, loss = 144.17939216\n",
      "Iteration 7831, loss = 143.96288508\n",
      "Iteration 7832, loss = 143.75000047\n",
      "Iteration 7833, loss = 143.52060334\n",
      "Iteration 7834, loss = 143.30982284\n",
      "Iteration 7835, loss = 143.09345467\n",
      "Iteration 7836, loss = 142.87530004\n",
      "Iteration 7837, loss = 142.65284166\n",
      "Iteration 7838, loss = 142.45357545\n",
      "Iteration 7839, loss = 142.23420494\n",
      "Iteration 7840, loss = 142.02205811\n",
      "Iteration 7841, loss = 141.80694382\n",
      "Iteration 7842, loss = 141.59209428\n",
      "Iteration 7843, loss = 141.37803489\n",
      "Iteration 7844, loss = 141.17290904\n",
      "Iteration 7845, loss = 140.96713894\n",
      "Iteration 7846, loss = 140.75026244\n",
      "Iteration 7847, loss = 140.55372527\n",
      "Iteration 7848, loss = 140.33407106\n",
      "Iteration 7849, loss = 140.11877783\n",
      "Iteration 7850, loss = 139.92051816\n",
      "Iteration 7851, loss = 139.73354542\n",
      "Iteration 7852, loss = 139.49717456\n",
      "Iteration 7853, loss = 139.30221098\n",
      "Iteration 7854, loss = 139.08975358\n",
      "Iteration 7855, loss = 138.88272665\n",
      "Iteration 7856, loss = 138.68236833\n",
      "Iteration 7857, loss = 138.47736677\n",
      "Iteration 7858, loss = 138.27594888\n",
      "Iteration 7859, loss = 138.06450605\n",
      "Iteration 7860, loss = 137.87750489\n",
      "Iteration 7861, loss = 137.66933097\n",
      "Iteration 7862, loss = 137.46386459\n",
      "Iteration 7863, loss = 137.27065936\n",
      "Iteration 7864, loss = 137.05732078\n",
      "Iteration 7865, loss = 136.87034906\n",
      "Iteration 7866, loss = 136.66377195\n",
      "Iteration 7867, loss = 136.46068281\n",
      "Iteration 7868, loss = 136.26989075\n",
      "Iteration 7869, loss = 136.07199427\n",
      "Iteration 7870, loss = 135.86893628\n",
      "Iteration 7871, loss = 135.67272925\n",
      "Iteration 7872, loss = 135.48147915\n",
      "Iteration 7873, loss = 135.27274734\n",
      "Iteration 7874, loss = 135.08333134\n",
      "Iteration 7875, loss = 134.90136979\n",
      "Iteration 7876, loss = 134.69168346\n",
      "Iteration 7877, loss = 134.49500406\n",
      "Iteration 7878, loss = 134.29607697\n",
      "Iteration 7879, loss = 134.11207043\n",
      "Iteration 7880, loss = 133.90924114\n",
      "Iteration 7881, loss = 133.73056986\n",
      "Iteration 7882, loss = 133.52242623\n",
      "Iteration 7883, loss = 133.34225493\n",
      "Iteration 7884, loss = 133.14135104\n",
      "Iteration 7885, loss = 132.94636415\n",
      "Iteration 7886, loss = 132.76237591\n",
      "Iteration 7887, loss = 132.56456173\n",
      "Iteration 7888, loss = 132.38187762\n",
      "Iteration 7889, loss = 132.20389030\n",
      "Iteration 7890, loss = 132.00556987\n",
      "Iteration 7891, loss = 131.80833682\n",
      "Iteration 7892, loss = 131.62859297\n",
      "Iteration 7893, loss = 131.44045231\n",
      "Iteration 7894, loss = 131.24428031\n",
      "Iteration 7895, loss = 131.07169160\n",
      "Iteration 7896, loss = 130.87090236\n",
      "Iteration 7897, loss = 130.68936864\n",
      "Iteration 7898, loss = 130.50152281\n",
      "Iteration 7899, loss = 130.31256739\n",
      "Iteration 7900, loss = 130.12730860\n",
      "Iteration 7901, loss = 129.94827152\n",
      "Iteration 7902, loss = 129.75680996\n",
      "Iteration 7903, loss = 129.58709887\n",
      "Iteration 7904, loss = 129.39118735\n",
      "Iteration 7905, loss = 129.20944483\n",
      "Iteration 7906, loss = 129.01493503\n",
      "Iteration 7907, loss = 128.84080294\n",
      "Iteration 7908, loss = 128.65778485\n",
      "Iteration 7909, loss = 128.47676181\n",
      "Iteration 7910, loss = 128.29752177\n",
      "Iteration 7911, loss = 128.11015364\n",
      "Iteration 7912, loss = 127.92901845\n",
      "Iteration 7913, loss = 127.76581084\n",
      "Iteration 7914, loss = 127.56836802\n",
      "Iteration 7915, loss = 127.39134757\n",
      "Iteration 7916, loss = 127.21088961\n",
      "Iteration 7917, loss = 127.03265516\n",
      "Iteration 7918, loss = 126.85604608\n",
      "Iteration 7919, loss = 126.68126297\n",
      "Iteration 7920, loss = 126.49804116\n",
      "Iteration 7921, loss = 126.33237909\n",
      "Iteration 7922, loss = 126.14065237\n",
      "Iteration 7923, loss = 125.96748896\n",
      "Iteration 7924, loss = 125.78722476\n",
      "Iteration 7925, loss = 125.61092529\n",
      "Iteration 7926, loss = 125.44699156\n",
      "Iteration 7927, loss = 125.26317959\n",
      "Iteration 7928, loss = 125.07412267\n",
      "Iteration 7929, loss = 124.90751013\n",
      "Iteration 7930, loss = 124.73544585\n",
      "Iteration 7931, loss = 124.56209041\n",
      "Iteration 7932, loss = 124.38260449\n",
      "Iteration 7933, loss = 124.20309377\n",
      "Iteration 7934, loss = 124.03284465\n",
      "Iteration 7935, loss = 123.86523933\n",
      "Iteration 7936, loss = 123.68812942\n",
      "Iteration 7937, loss = 123.51115967\n",
      "Iteration 7938, loss = 123.34338221\n",
      "Iteration 7939, loss = 123.17320877\n",
      "Iteration 7940, loss = 123.00241468\n",
      "Iteration 7941, loss = 122.82296405\n",
      "Iteration 7942, loss = 122.65941214\n",
      "Iteration 7943, loss = 122.48224209\n",
      "Iteration 7944, loss = 122.30650345\n",
      "Iteration 7945, loss = 122.13979257\n",
      "Iteration 7946, loss = 121.96553202\n",
      "Iteration 7947, loss = 121.80654718\n",
      "Iteration 7948, loss = 121.63960847\n",
      "Iteration 7949, loss = 121.46409282\n",
      "Iteration 7950, loss = 121.30882894\n",
      "Iteration 7951, loss = 121.12678614\n",
      "Iteration 7952, loss = 120.96183989\n",
      "Iteration 7953, loss = 120.79241242\n",
      "Iteration 7954, loss = 120.62400980\n",
      "Iteration 7955, loss = 120.46337491\n",
      "Iteration 7956, loss = 120.28664951\n",
      "Iteration 7957, loss = 120.13047439\n",
      "Iteration 7958, loss = 119.95004051\n",
      "Iteration 7959, loss = 119.78579468\n",
      "Iteration 7960, loss = 119.63260702\n",
      "Iteration 7961, loss = 119.45935170\n",
      "Iteration 7962, loss = 119.29567413\n",
      "Iteration 7963, loss = 119.14183146\n",
      "Iteration 7964, loss = 118.96315147\n",
      "Iteration 7965, loss = 118.79968308\n",
      "Iteration 7966, loss = 118.62548631\n",
      "Iteration 7967, loss = 118.46781401\n",
      "Iteration 7968, loss = 118.29992143\n",
      "Iteration 7969, loss = 118.14483036\n",
      "Iteration 7970, loss = 117.97760706\n",
      "Iteration 7971, loss = 117.80978061\n",
      "Iteration 7972, loss = 117.65065086\n",
      "Iteration 7973, loss = 117.50011544\n",
      "Iteration 7974, loss = 117.32905147\n",
      "Iteration 7975, loss = 117.15601485\n",
      "Iteration 7976, loss = 117.01501113\n",
      "Iteration 7977, loss = 116.84300225\n",
      "Iteration 7978, loss = 116.67907150\n",
      "Iteration 7979, loss = 116.51783990\n",
      "Iteration 7980, loss = 116.35355478\n",
      "Iteration 7981, loss = 116.20626954\n",
      "Iteration 7982, loss = 116.03434450\n",
      "Iteration 7983, loss = 115.88688998\n",
      "Iteration 7984, loss = 115.71923154\n",
      "Iteration 7985, loss = 115.56572139\n",
      "Iteration 7986, loss = 115.40551375\n",
      "Iteration 7987, loss = 115.23773869\n",
      "Iteration 7988, loss = 115.08664126\n",
      "Iteration 7989, loss = 114.92523495\n",
      "Iteration 7990, loss = 114.76379815\n",
      "Iteration 7991, loss = 114.60826526\n",
      "Iteration 7992, loss = 114.44945793\n",
      "Iteration 7993, loss = 114.29324830\n",
      "Iteration 7994, loss = 114.14988818\n",
      "Iteration 7995, loss = 113.97824074\n",
      "Iteration 7996, loss = 113.81667853\n",
      "Iteration 7997, loss = 113.66125824\n",
      "Iteration 7998, loss = 113.53069107\n",
      "Iteration 7999, loss = 113.35451308\n",
      "Iteration 8000, loss = 113.20597645\n",
      "Iteration 8001, loss = 113.03979969\n",
      "Iteration 8002, loss = 112.89906331\n",
      "Iteration 8003, loss = 112.74390987\n",
      "Iteration 8004, loss = 112.58420419\n",
      "Iteration 8005, loss = 112.42689387\n",
      "Iteration 8006, loss = 112.28165258\n",
      "Iteration 8007, loss = 112.11443946\n",
      "Iteration 8008, loss = 111.97011320\n",
      "Iteration 8009, loss = 111.81837436\n",
      "Iteration 8010, loss = 111.66118761\n",
      "Iteration 8011, loss = 111.51277071\n",
      "Iteration 8012, loss = 111.37383619\n",
      "Iteration 8013, loss = 111.19871308\n",
      "Iteration 8014, loss = 111.04471589\n",
      "Iteration 8015, loss = 110.89423059\n",
      "Iteration 8016, loss = 110.74536043\n",
      "Iteration 8017, loss = 110.58585935\n",
      "Iteration 8018, loss = 110.43835163\n",
      "Iteration 8019, loss = 110.29775729\n",
      "Iteration 8020, loss = 110.14001309\n",
      "Iteration 8021, loss = 109.98953346\n",
      "Iteration 8022, loss = 109.83324195\n",
      "Iteration 8023, loss = 109.68341958\n",
      "Iteration 8024, loss = 109.53416868\n",
      "Iteration 8025, loss = 109.38821800\n",
      "Iteration 8026, loss = 109.23563582\n",
      "Iteration 8027, loss = 109.09447486\n",
      "Iteration 8028, loss = 108.94436370\n",
      "Iteration 8029, loss = 108.78838048\n",
      "Iteration 8030, loss = 108.66262525\n",
      "Iteration 8031, loss = 108.49362327\n",
      "Iteration 8032, loss = 108.35430787\n",
      "Iteration 8033, loss = 108.19482264\n",
      "Iteration 8034, loss = 108.05664332\n",
      "Iteration 8035, loss = 107.90199170\n",
      "Iteration 8036, loss = 107.75690640\n",
      "Iteration 8037, loss = 107.61825930\n",
      "Iteration 8038, loss = 107.45459686\n",
      "Iteration 8039, loss = 107.31681801\n",
      "Iteration 8040, loss = 107.16458526\n",
      "Iteration 8041, loss = 107.02276237\n",
      "Iteration 8042, loss = 106.87101709\n",
      "Iteration 8043, loss = 106.73298222\n",
      "Iteration 8044, loss = 106.58922789\n",
      "Iteration 8045, loss = 106.43727347\n",
      "Iteration 8046, loss = 106.29532383\n",
      "Iteration 8047, loss = 106.14592394\n",
      "Iteration 8048, loss = 106.00064334\n",
      "Iteration 8049, loss = 105.85317587\n",
      "Iteration 8050, loss = 105.72155248\n",
      "Iteration 8051, loss = 105.56761414\n",
      "Iteration 8052, loss = 105.42737802\n",
      "Iteration 8053, loss = 105.28270133\n",
      "Iteration 8054, loss = 105.15533411\n",
      "Iteration 8055, loss = 104.99848708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8056, loss = 104.85962351\n",
      "Iteration 8057, loss = 104.70708120\n",
      "Iteration 8058, loss = 104.57076031\n",
      "Iteration 8059, loss = 104.42036599\n",
      "Iteration 8060, loss = 104.27940573\n",
      "Iteration 8061, loss = 104.14633669\n",
      "Iteration 8062, loss = 103.99959936\n",
      "Iteration 8063, loss = 103.85930206\n",
      "Iteration 8064, loss = 103.71509878\n",
      "Iteration 8065, loss = 103.57650294\n",
      "Iteration 8066, loss = 103.43162892\n",
      "Iteration 8067, loss = 103.29214256\n",
      "Iteration 8068, loss = 103.14778048\n",
      "Iteration 8069, loss = 103.01855894\n",
      "Iteration 8070, loss = 102.88021597\n",
      "Iteration 8071, loss = 102.73129378\n",
      "Iteration 8072, loss = 102.61252554\n",
      "Iteration 8073, loss = 102.46919534\n",
      "Iteration 8074, loss = 102.32639537\n",
      "Iteration 8075, loss = 102.17183452\n",
      "Iteration 8076, loss = 102.03620141\n",
      "Iteration 8077, loss = 101.89004052\n",
      "Iteration 8078, loss = 101.76006823\n",
      "Iteration 8079, loss = 101.62054317\n",
      "Iteration 8080, loss = 101.48578784\n",
      "Iteration 8081, loss = 101.34341099\n",
      "Iteration 8082, loss = 101.21083683\n",
      "Iteration 8083, loss = 101.07477298\n",
      "Iteration 8084, loss = 100.94444324\n",
      "Iteration 8085, loss = 100.82254603\n",
      "Iteration 8086, loss = 100.66573183\n",
      "Iteration 8087, loss = 100.53255844\n",
      "Iteration 8088, loss = 100.38732299\n",
      "Iteration 8089, loss = 100.24554028\n",
      "Iteration 8090, loss = 100.10862478\n",
      "Iteration 8091, loss = 99.97304300\n",
      "Iteration 8092, loss = 99.84094279\n",
      "Iteration 8093, loss = 99.69953131\n",
      "Iteration 8094, loss = 99.57017478\n",
      "Iteration 8095, loss = 99.43160310\n",
      "Iteration 8096, loss = 99.29947444\n",
      "Iteration 8097, loss = 99.18474050\n",
      "Iteration 8098, loss = 99.05519807\n",
      "Iteration 8099, loss = 98.89515316\n",
      "Iteration 8100, loss = 98.75395197\n",
      "Iteration 8101, loss = 98.62795858\n",
      "Iteration 8102, loss = 98.49113864\n",
      "Iteration 8103, loss = 98.35474231\n",
      "Iteration 8104, loss = 98.21968592\n",
      "Iteration 8105, loss = 98.08810947\n",
      "Iteration 8106, loss = 97.96181042\n",
      "Iteration 8107, loss = 97.82296498\n",
      "Iteration 8108, loss = 97.70464011\n",
      "Iteration 8109, loss = 97.55287416\n",
      "Iteration 8110, loss = 97.42194310\n",
      "Iteration 8111, loss = 97.28924927\n",
      "Iteration 8112, loss = 97.15766842\n",
      "Iteration 8113, loss = 97.03148030\n",
      "Iteration 8114, loss = 96.89624078\n",
      "Iteration 8115, loss = 96.75992154\n",
      "Iteration 8116, loss = 96.63087196\n",
      "Iteration 8117, loss = 96.49808776\n",
      "Iteration 8118, loss = 96.38047823\n",
      "Iteration 8119, loss = 96.23553530\n",
      "Iteration 8120, loss = 96.10847991\n",
      "Iteration 8121, loss = 95.97430200\n",
      "Iteration 8122, loss = 95.84659502\n",
      "Iteration 8123, loss = 95.72976498\n",
      "Iteration 8124, loss = 95.59976405\n",
      "Iteration 8125, loss = 95.46908414\n",
      "Iteration 8126, loss = 95.33157865\n",
      "Iteration 8127, loss = 95.19244057\n",
      "Iteration 8128, loss = 95.07591108\n",
      "Iteration 8129, loss = 94.96200680\n",
      "Iteration 8130, loss = 94.80819558\n",
      "Iteration 8131, loss = 94.69079661\n",
      "Iteration 8132, loss = 94.54600165\n",
      "Iteration 8133, loss = 94.42439868\n",
      "Iteration 8134, loss = 94.29082065\n",
      "Iteration 8135, loss = 94.17740838\n",
      "Iteration 8136, loss = 94.05777435\n",
      "Iteration 8137, loss = 93.91702322\n",
      "Iteration 8138, loss = 93.79212841\n",
      "Iteration 8139, loss = 93.66058574\n",
      "Iteration 8140, loss = 93.53715372\n",
      "Iteration 8141, loss = 93.42069020\n",
      "Iteration 8142, loss = 93.28958163\n",
      "Iteration 8143, loss = 93.15050009\n",
      "Iteration 8144, loss = 93.01880235\n",
      "Iteration 8145, loss = 92.89748996\n",
      "Iteration 8146, loss = 92.76724038\n",
      "Iteration 8147, loss = 92.64225879\n",
      "Iteration 8148, loss = 92.51130246\n",
      "Iteration 8149, loss = 92.39238481\n",
      "Iteration 8150, loss = 92.26493108\n",
      "Iteration 8151, loss = 92.15189296\n",
      "Iteration 8152, loss = 92.01570291\n",
      "Iteration 8153, loss = 91.92872979\n",
      "Iteration 8154, loss = 91.76720136\n",
      "Iteration 8155, loss = 91.63695872\n",
      "Iteration 8156, loss = 91.51781936\n",
      "Iteration 8157, loss = 91.40011882\n",
      "Iteration 8158, loss = 91.27462098\n",
      "Iteration 8159, loss = 91.14091933\n",
      "Iteration 8160, loss = 91.01724533\n",
      "Iteration 8161, loss = 90.89784115\n",
      "Iteration 8162, loss = 90.77209949\n",
      "Iteration 8163, loss = 90.65584892\n",
      "Iteration 8164, loss = 90.53166003\n",
      "Iteration 8165, loss = 90.41288272\n",
      "Iteration 8166, loss = 90.28152798\n",
      "Iteration 8167, loss = 90.17197747\n",
      "Iteration 8168, loss = 90.03697914\n",
      "Iteration 8169, loss = 89.91110888\n",
      "Iteration 8170, loss = 89.78998683\n",
      "Iteration 8171, loss = 89.66603000\n",
      "Iteration 8172, loss = 89.54032385\n",
      "Iteration 8173, loss = 89.42845798\n",
      "Iteration 8174, loss = 89.30007671\n",
      "Iteration 8175, loss = 89.18641772\n",
      "Iteration 8176, loss = 89.06517106\n",
      "Iteration 8177, loss = 88.93910671\n",
      "Iteration 8178, loss = 88.81641448\n",
      "Iteration 8179, loss = 88.69604627\n",
      "Iteration 8180, loss = 88.57974205\n",
      "Iteration 8181, loss = 88.45271124\n",
      "Iteration 8182, loss = 88.33243280\n",
      "Iteration 8183, loss = 88.21348903\n",
      "Iteration 8184, loss = 88.09535974\n",
      "Iteration 8185, loss = 87.98832028\n",
      "Iteration 8186, loss = 87.86385429\n",
      "Iteration 8187, loss = 87.73609362\n",
      "Iteration 8188, loss = 87.61554997\n",
      "Iteration 8189, loss = 87.49900598\n",
      "Iteration 8190, loss = 87.37777939\n",
      "Iteration 8191, loss = 87.25857441\n",
      "Iteration 8192, loss = 87.14134656\n",
      "Iteration 8193, loss = 87.01872707\n",
      "Iteration 8194, loss = 86.90617059\n",
      "Iteration 8195, loss = 86.78822955\n",
      "Iteration 8196, loss = 86.66895382\n",
      "Iteration 8197, loss = 86.58340394\n",
      "Iteration 8198, loss = 86.42962122\n",
      "Iteration 8199, loss = 86.32112648\n",
      "Iteration 8200, loss = 86.20780882\n",
      "Iteration 8201, loss = 86.07550049\n",
      "Iteration 8202, loss = 85.97017960\n",
      "Iteration 8203, loss = 85.85565558\n",
      "Iteration 8204, loss = 85.72722392\n",
      "Iteration 8205, loss = 85.61203720\n",
      "Iteration 8206, loss = 85.49972850\n",
      "Iteration 8207, loss = 85.38036419\n",
      "Iteration 8208, loss = 85.25536927\n",
      "Iteration 8209, loss = 85.14644438\n",
      "Iteration 8210, loss = 85.04315802\n",
      "Iteration 8211, loss = 84.91289261\n",
      "Iteration 8212, loss = 84.80004971\n",
      "Iteration 8213, loss = 84.68054895\n",
      "Iteration 8214, loss = 84.56031600\n",
      "Iteration 8215, loss = 84.44955594\n",
      "Iteration 8216, loss = 84.33091183\n",
      "Iteration 8217, loss = 84.21709258\n",
      "Iteration 8218, loss = 84.10309690\n",
      "Iteration 8219, loss = 83.99236738\n",
      "Iteration 8220, loss = 83.87750586\n",
      "Iteration 8221, loss = 83.75861548\n",
      "Iteration 8222, loss = 83.64662102\n",
      "Iteration 8223, loss = 83.52678408\n",
      "Iteration 8224, loss = 83.43546334\n",
      "Iteration 8225, loss = 83.29437796\n",
      "Iteration 8226, loss = 83.18525636\n",
      "Iteration 8227, loss = 83.07397836\n",
      "Iteration 8228, loss = 82.96017253\n",
      "Iteration 8229, loss = 82.85402762\n",
      "Iteration 8230, loss = 82.72884010\n",
      "Iteration 8231, loss = 82.62405097\n",
      "Iteration 8232, loss = 82.51391945\n",
      "Iteration 8233, loss = 82.39222654\n",
      "Iteration 8234, loss = 82.28372479\n",
      "Iteration 8235, loss = 82.17154615\n",
      "Iteration 8236, loss = 82.05130328\n",
      "Iteration 8237, loss = 81.93541589\n",
      "Iteration 8238, loss = 81.82479680\n",
      "Iteration 8239, loss = 81.72234735\n",
      "Iteration 8240, loss = 81.60185316\n",
      "Iteration 8241, loss = 81.49712230\n",
      "Iteration 8242, loss = 81.37660661\n",
      "Iteration 8243, loss = 81.27342888\n",
      "Iteration 8244, loss = 81.16438886\n",
      "Iteration 8245, loss = 81.04236256\n",
      "Iteration 8246, loss = 80.93282928\n",
      "Iteration 8247, loss = 80.82535098\n",
      "Iteration 8248, loss = 80.71295223\n",
      "Iteration 8249, loss = 80.61130351\n",
      "Iteration 8250, loss = 80.48116385\n",
      "Iteration 8251, loss = 80.38162404\n",
      "Iteration 8252, loss = 80.26528621\n",
      "Iteration 8253, loss = 80.14396676\n",
      "Iteration 8254, loss = 80.04319843\n",
      "Iteration 8255, loss = 79.92862664\n",
      "Iteration 8256, loss = 79.82037623\n",
      "Iteration 8257, loss = 79.72030979\n",
      "Iteration 8258, loss = 79.60007139\n",
      "Iteration 8259, loss = 79.49266634\n",
      "Iteration 8260, loss = 79.37955558\n",
      "Iteration 8261, loss = 79.26704696\n",
      "Iteration 8262, loss = 79.16345993\n",
      "Iteration 8263, loss = 79.04571342\n",
      "Iteration 8264, loss = 78.93641766\n",
      "Iteration 8265, loss = 78.82752943\n",
      "Iteration 8266, loss = 78.71067535\n",
      "Iteration 8267, loss = 78.60165052\n",
      "Iteration 8268, loss = 78.48911419\n",
      "Iteration 8269, loss = 78.38741844\n",
      "Iteration 8270, loss = 78.27003170\n",
      "Iteration 8271, loss = 78.16367811\n",
      "Iteration 8272, loss = 78.04879920\n",
      "Iteration 8273, loss = 77.94653024\n",
      "Iteration 8274, loss = 77.82880667\n",
      "Iteration 8275, loss = 77.72462127\n",
      "Iteration 8276, loss = 77.61726359\n",
      "Iteration 8277, loss = 77.50205148\n",
      "Iteration 8278, loss = 77.40455386\n",
      "Iteration 8279, loss = 77.28137666\n",
      "Iteration 8280, loss = 77.18254119\n",
      "Iteration 8281, loss = 77.06685894\n",
      "Iteration 8282, loss = 76.95603487\n",
      "Iteration 8283, loss = 76.84502673\n",
      "Iteration 8284, loss = 76.74998665\n",
      "Iteration 8285, loss = 76.63006266\n",
      "Iteration 8286, loss = 76.53369516\n",
      "Iteration 8287, loss = 76.40762698\n",
      "Iteration 8288, loss = 76.30581476\n",
      "Iteration 8289, loss = 76.20165945\n",
      "Iteration 8290, loss = 76.08572057\n",
      "Iteration 8291, loss = 75.97884279\n",
      "Iteration 8292, loss = 75.86186504\n",
      "Iteration 8293, loss = 75.76106171\n",
      "Iteration 8294, loss = 75.64584620\n",
      "Iteration 8295, loss = 75.54169988\n",
      "Iteration 8296, loss = 75.43829243\n",
      "Iteration 8297, loss = 75.32812360\n",
      "Iteration 8298, loss = 75.22471053\n",
      "Iteration 8299, loss = 75.10571006\n",
      "Iteration 8300, loss = 75.00115732\n",
      "Iteration 8301, loss = 74.90082638\n",
      "Iteration 8302, loss = 74.79597933\n",
      "Iteration 8303, loss = 74.69230297\n",
      "Iteration 8304, loss = 74.57203251\n",
      "Iteration 8305, loss = 74.46855362\n",
      "Iteration 8306, loss = 74.37219754\n",
      "Iteration 8307, loss = 74.24978686\n",
      "Iteration 8308, loss = 74.15533806\n",
      "Iteration 8309, loss = 74.05101711\n",
      "Iteration 8310, loss = 73.93451000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8311, loss = 73.83318121\n",
      "Iteration 8312, loss = 73.72037620\n",
      "Iteration 8313, loss = 73.61155271\n",
      "Iteration 8314, loss = 73.51776222\n",
      "Iteration 8315, loss = 73.41123221\n",
      "Iteration 8316, loss = 73.30690785\n",
      "Iteration 8317, loss = 73.19009281\n",
      "Iteration 8318, loss = 73.08618174\n",
      "Iteration 8319, loss = 72.98091139\n",
      "Iteration 8320, loss = 72.86730236\n",
      "Iteration 8321, loss = 72.76454633\n",
      "Iteration 8322, loss = 72.67189986\n",
      "Iteration 8323, loss = 72.55749554\n",
      "Iteration 8324, loss = 72.45790082\n",
      "Iteration 8325, loss = 72.35170174\n",
      "Iteration 8326, loss = 72.24257700\n",
      "Iteration 8327, loss = 72.14591947\n",
      "Iteration 8328, loss = 72.03680085\n",
      "Iteration 8329, loss = 71.92636277\n",
      "Iteration 8330, loss = 71.82568461\n",
      "Iteration 8331, loss = 71.72472890\n",
      "Iteration 8332, loss = 71.61495060\n",
      "Iteration 8333, loss = 71.51114451\n",
      "Iteration 8334, loss = 71.40523856\n",
      "Iteration 8335, loss = 71.30781201\n",
      "Iteration 8336, loss = 71.20123905\n",
      "Iteration 8337, loss = 71.09660053\n",
      "Iteration 8338, loss = 70.99875891\n",
      "Iteration 8339, loss = 70.89881290\n",
      "Iteration 8340, loss = 70.79163834\n",
      "Iteration 8341, loss = 70.69367336\n",
      "Iteration 8342, loss = 70.58309495\n",
      "Iteration 8343, loss = 70.47963139\n",
      "Iteration 8344, loss = 70.38185463\n",
      "Iteration 8345, loss = 70.27119725\n",
      "Iteration 8346, loss = 70.17684940\n",
      "Iteration 8347, loss = 70.06620726\n",
      "Iteration 8348, loss = 69.96879079\n",
      "Iteration 8349, loss = 69.86683735\n",
      "Iteration 8350, loss = 69.75835819\n",
      "Iteration 8351, loss = 69.66537254\n",
      "Iteration 8352, loss = 69.56480552\n",
      "Iteration 8353, loss = 69.45180853\n",
      "Iteration 8354, loss = 69.35177941\n",
      "Iteration 8355, loss = 69.24899720\n",
      "Iteration 8356, loss = 69.15895160\n",
      "Iteration 8357, loss = 69.06050884\n",
      "Iteration 8358, loss = 68.94713568\n",
      "Iteration 8359, loss = 68.85018988\n",
      "Iteration 8360, loss = 68.74417846\n",
      "Iteration 8361, loss = 68.64090846\n",
      "Iteration 8362, loss = 68.55771933\n",
      "Iteration 8363, loss = 68.44518719\n",
      "Iteration 8364, loss = 68.33762586\n",
      "Iteration 8365, loss = 68.24334001\n",
      "Iteration 8366, loss = 68.13625371\n",
      "Iteration 8367, loss = 68.04366763\n",
      "Iteration 8368, loss = 67.94481872\n",
      "Iteration 8369, loss = 67.83493980\n",
      "Iteration 8370, loss = 67.73335423\n",
      "Iteration 8371, loss = 67.63999843\n",
      "Iteration 8372, loss = 67.53937536\n",
      "Iteration 8373, loss = 67.43885047\n",
      "Iteration 8374, loss = 67.33480619\n",
      "Iteration 8375, loss = 67.23523635\n",
      "Iteration 8376, loss = 67.15191001\n",
      "Iteration 8377, loss = 67.04722396\n",
      "Iteration 8378, loss = 66.93831137\n",
      "Iteration 8379, loss = 66.83915327\n",
      "Iteration 8380, loss = 66.73945637\n",
      "Iteration 8381, loss = 66.64109872\n",
      "Iteration 8382, loss = 66.54117796\n",
      "Iteration 8383, loss = 66.44465487\n",
      "Iteration 8384, loss = 66.37011428\n",
      "Iteration 8385, loss = 66.25286851\n",
      "Iteration 8386, loss = 66.15116453\n",
      "Iteration 8387, loss = 66.05633212\n",
      "Iteration 8388, loss = 65.94262044\n",
      "Iteration 8389, loss = 65.84837227\n",
      "Iteration 8390, loss = 65.75477169\n",
      "Iteration 8391, loss = 65.65253635\n",
      "Iteration 8392, loss = 65.56143263\n",
      "Iteration 8393, loss = 65.46142174\n",
      "Iteration 8394, loss = 65.35681415\n",
      "Iteration 8395, loss = 65.25792748\n",
      "Iteration 8396, loss = 65.17071162\n",
      "Iteration 8397, loss = 65.06766842\n",
      "Iteration 8398, loss = 64.97064160\n",
      "Iteration 8399, loss = 64.86287345\n",
      "Iteration 8400, loss = 64.77081962\n",
      "Iteration 8401, loss = 64.66691346\n",
      "Iteration 8402, loss = 64.56934350\n",
      "Iteration 8403, loss = 64.47040404\n",
      "Iteration 8404, loss = 64.37291440\n",
      "Iteration 8405, loss = 64.28052691\n",
      "Iteration 8406, loss = 64.17395144\n",
      "Iteration 8407, loss = 64.08712288\n",
      "Iteration 8408, loss = 63.99052234\n",
      "Iteration 8409, loss = 63.87743589\n",
      "Iteration 8410, loss = 63.78927064\n",
      "Iteration 8411, loss = 63.68048341\n",
      "Iteration 8412, loss = 63.57755021\n",
      "Iteration 8413, loss = 63.47735271\n",
      "Iteration 8414, loss = 63.38870125\n",
      "Iteration 8415, loss = 63.28273273\n",
      "Iteration 8416, loss = 63.18000592\n",
      "Iteration 8417, loss = 63.07559892\n",
      "Iteration 8418, loss = 62.97427653\n",
      "Iteration 8419, loss = 62.87101695\n",
      "Iteration 8420, loss = 62.77853729\n",
      "Iteration 8421, loss = 62.66679620\n",
      "Iteration 8422, loss = 62.56872297\n",
      "Iteration 8423, loss = 62.46514768\n",
      "Iteration 8424, loss = 62.36533413\n",
      "Iteration 8425, loss = 62.26676413\n",
      "Iteration 8426, loss = 62.14090668\n",
      "Iteration 8427, loss = 62.04343898\n",
      "Iteration 8428, loss = 61.93159123\n",
      "Iteration 8429, loss = 61.83369204\n",
      "Iteration 8430, loss = 61.70621000\n",
      "Iteration 8431, loss = 61.60087217\n",
      "Iteration 8432, loss = 61.48926282\n",
      "Iteration 8433, loss = 61.37385050\n",
      "Iteration 8434, loss = 61.24985401\n",
      "Iteration 8435, loss = 61.13250958\n",
      "Iteration 8436, loss = 61.01020754\n",
      "Iteration 8437, loss = 60.89808234\n",
      "Iteration 8438, loss = 60.77322041\n",
      "Iteration 8439, loss = 60.64020904\n",
      "Iteration 8440, loss = 60.51481763\n",
      "Iteration 8441, loss = 60.38222522\n",
      "Iteration 8442, loss = 60.24918966\n",
      "Iteration 8443, loss = 60.11111721\n",
      "Iteration 8444, loss = 59.96714954\n",
      "Iteration 8445, loss = 59.83036113\n",
      "Iteration 8446, loss = 59.68898720\n",
      "Iteration 8447, loss = 59.53974867\n",
      "Iteration 8448, loss = 59.39128733\n",
      "Iteration 8449, loss = 59.24012061\n",
      "Iteration 8450, loss = 59.08772947\n",
      "Iteration 8451, loss = 58.93056941\n",
      "Iteration 8452, loss = 58.77585427\n",
      "Iteration 8453, loss = 58.61709773\n",
      "Iteration 8454, loss = 58.46275400\n",
      "Iteration 8455, loss = 58.30329754\n",
      "Iteration 8456, loss = 58.14825889\n",
      "Iteration 8457, loss = 57.97906894\n",
      "Iteration 8458, loss = 57.82224823\n",
      "Iteration 8459, loss = 57.66047275\n",
      "Iteration 8460, loss = 57.50810391\n",
      "Iteration 8461, loss = 57.34448972\n",
      "Iteration 8462, loss = 57.18346453\n",
      "Iteration 8463, loss = 57.02633506\n",
      "Iteration 8464, loss = 56.87815284\n",
      "Iteration 8465, loss = 56.71477067\n",
      "Iteration 8466, loss = 56.55668174\n",
      "Iteration 8467, loss = 56.39913465\n",
      "Iteration 8468, loss = 56.24263988\n",
      "Iteration 8469, loss = 56.08763063\n",
      "Iteration 8470, loss = 55.95163381\n",
      "Iteration 8471, loss = 55.79245956\n",
      "Iteration 8472, loss = 55.63801079\n",
      "Iteration 8473, loss = 55.48420391\n",
      "Iteration 8474, loss = 55.34395255\n",
      "Iteration 8475, loss = 55.20564621\n",
      "Iteration 8476, loss = 55.04389916\n",
      "Iteration 8477, loss = 54.91592857\n",
      "Iteration 8478, loss = 54.75779975\n",
      "Iteration 8479, loss = 54.61739529\n",
      "Iteration 8480, loss = 54.47428942\n",
      "Iteration 8481, loss = 54.33150434\n",
      "Iteration 8482, loss = 54.19639808\n",
      "Iteration 8483, loss = 54.05689432\n",
      "Iteration 8484, loss = 53.91713867\n",
      "Iteration 8485, loss = 53.78244901\n",
      "Iteration 8486, loss = 53.64377896\n",
      "Iteration 8487, loss = 53.50957551\n",
      "Iteration 8488, loss = 53.37777625\n",
      "Iteration 8489, loss = 53.24506461\n",
      "Iteration 8490, loss = 53.11275746\n",
      "Iteration 8491, loss = 52.98086002\n",
      "Iteration 8492, loss = 52.84924928\n",
      "Iteration 8493, loss = 52.72432454\n",
      "Iteration 8494, loss = 52.59604655\n",
      "Iteration 8495, loss = 52.46817211\n",
      "Iteration 8496, loss = 52.34520655\n",
      "Iteration 8497, loss = 52.22544157\n",
      "Iteration 8498, loss = 52.08654701\n",
      "Iteration 8499, loss = 51.96028603\n",
      "Iteration 8500, loss = 51.83694156\n",
      "Iteration 8501, loss = 51.72035934\n",
      "Iteration 8502, loss = 51.59159640\n",
      "Iteration 8503, loss = 51.46649156\n",
      "Iteration 8504, loss = 51.34954506\n",
      "Iteration 8505, loss = 51.23135180\n",
      "Iteration 8506, loss = 51.10809587\n",
      "Iteration 8507, loss = 51.00158469\n",
      "Iteration 8508, loss = 50.87211391\n",
      "Iteration 8509, loss = 50.75333389\n",
      "Iteration 8510, loss = 50.63690601\n",
      "Iteration 8511, loss = 50.51669672\n",
      "Iteration 8512, loss = 50.40354223\n",
      "Iteration 8513, loss = 50.29383663\n",
      "Iteration 8514, loss = 50.16591265\n",
      "Iteration 8515, loss = 50.06124953\n",
      "Iteration 8516, loss = 49.93911243\n",
      "Iteration 8517, loss = 49.83117562\n",
      "Iteration 8518, loss = 49.71894008\n",
      "Iteration 8519, loss = 49.59832306\n",
      "Iteration 8520, loss = 49.48969827\n",
      "Iteration 8521, loss = 49.37312294\n",
      "Iteration 8522, loss = 49.26704663\n",
      "Iteration 8523, loss = 49.15306353\n",
      "Iteration 8524, loss = 49.04955642\n",
      "Iteration 8525, loss = 48.93951067\n",
      "Iteration 8526, loss = 48.84048736\n",
      "Iteration 8527, loss = 48.71727470\n",
      "Iteration 8528, loss = 48.60857815\n",
      "Iteration 8529, loss = 48.50809241\n",
      "Iteration 8530, loss = 48.39802182\n",
      "Iteration 8531, loss = 48.28904615\n",
      "Iteration 8532, loss = 48.17907536\n",
      "Iteration 8533, loss = 48.07429604\n",
      "Iteration 8534, loss = 47.96505561\n",
      "Iteration 8535, loss = 47.86454279\n",
      "Iteration 8536, loss = 47.76362817\n",
      "Iteration 8537, loss = 47.65910495\n",
      "Iteration 8538, loss = 47.55369299\n",
      "Iteration 8539, loss = 47.44759171\n",
      "Iteration 8540, loss = 47.34597372\n",
      "Iteration 8541, loss = 47.23697054\n",
      "Iteration 8542, loss = 47.14597522\n",
      "Iteration 8543, loss = 47.03560486\n",
      "Iteration 8544, loss = 46.93328943\n",
      "Iteration 8545, loss = 46.83294088\n",
      "Iteration 8546, loss = 46.73337620\n",
      "Iteration 8547, loss = 46.62793107\n",
      "Iteration 8548, loss = 46.52991994\n",
      "Iteration 8549, loss = 46.42902232\n",
      "Iteration 8550, loss = 46.33102081\n",
      "Iteration 8551, loss = 46.22950768\n",
      "Iteration 8552, loss = 46.13046653\n",
      "Iteration 8553, loss = 46.02817967\n",
      "Iteration 8554, loss = 45.93465565\n",
      "Iteration 8555, loss = 45.83712744\n",
      "Iteration 8556, loss = 45.73539480\n",
      "Iteration 8557, loss = 45.63969354\n",
      "Iteration 8558, loss = 45.54771439\n",
      "Iteration 8559, loss = 45.44635917\n",
      "Iteration 8560, loss = 45.35207196\n",
      "Iteration 8561, loss = 45.25992253\n",
      "Iteration 8562, loss = 45.15926418\n",
      "Iteration 8563, loss = 45.06414413\n",
      "Iteration 8564, loss = 44.96870946\n",
      "Iteration 8565, loss = 44.87151229\n",
      "Iteration 8566, loss = 44.78526827\n",
      "Iteration 8567, loss = 44.68178827\n",
      "Iteration 8568, loss = 44.58748225\n",
      "Iteration 8569, loss = 44.49310680\n",
      "Iteration 8570, loss = 44.40102964\n",
      "Iteration 8571, loss = 44.31079723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8572, loss = 44.21960027\n",
      "Iteration 8573, loss = 44.11988515\n",
      "Iteration 8574, loss = 44.02810865\n",
      "Iteration 8575, loss = 43.94056318\n",
      "Iteration 8576, loss = 43.84452784\n",
      "Iteration 8577, loss = 43.75700235\n",
      "Iteration 8578, loss = 43.66332197\n",
      "Iteration 8579, loss = 43.56736076\n",
      "Iteration 8580, loss = 43.47995707\n",
      "Iteration 8581, loss = 43.39743790\n",
      "Iteration 8582, loss = 43.29801717\n",
      "Iteration 8583, loss = 43.21912815\n",
      "Iteration 8584, loss = 43.11812126\n",
      "Iteration 8585, loss = 43.03230143\n",
      "Iteration 8586, loss = 42.93927861\n",
      "Iteration 8587, loss = 42.85105743\n",
      "Iteration 8588, loss = 42.76011845\n",
      "Iteration 8589, loss = 42.67804922\n",
      "Iteration 8590, loss = 42.58583192\n",
      "Iteration 8591, loss = 42.50014108\n",
      "Iteration 8592, loss = 42.40753812\n",
      "Iteration 8593, loss = 42.31978975\n",
      "Iteration 8594, loss = 42.23524646\n",
      "Iteration 8595, loss = 42.15538171\n",
      "Iteration 8596, loss = 42.06442857\n",
      "Iteration 8597, loss = 41.96895915\n",
      "Iteration 8598, loss = 41.89216954\n",
      "Iteration 8599, loss = 41.79742031\n",
      "Iteration 8600, loss = 41.71516376\n",
      "Iteration 8601, loss = 41.64976511\n",
      "Iteration 8602, loss = 41.53766782\n",
      "Iteration 8603, loss = 41.45321988\n",
      "Iteration 8604, loss = 41.37007240\n",
      "Iteration 8605, loss = 41.27705023\n",
      "Iteration 8606, loss = 41.19123290\n",
      "Iteration 8607, loss = 41.11031570\n",
      "Iteration 8608, loss = 41.03549213\n",
      "Iteration 8609, loss = 40.94598214\n",
      "Iteration 8610, loss = 40.86014379\n",
      "Iteration 8611, loss = 40.76722784\n",
      "Iteration 8612, loss = 40.68868294\n",
      "Iteration 8613, loss = 40.60394377\n",
      "Iteration 8614, loss = 40.52240097\n",
      "Iteration 8615, loss = 40.44440844\n",
      "Iteration 8616, loss = 40.35381792\n",
      "Iteration 8617, loss = 40.26892774\n",
      "Iteration 8618, loss = 40.17790313\n",
      "Iteration 8619, loss = 40.09850724\n",
      "Iteration 8620, loss = 40.01297375\n",
      "Iteration 8621, loss = 39.93215335\n",
      "Iteration 8622, loss = 39.85783776\n",
      "Iteration 8623, loss = 39.76054141\n",
      "Iteration 8624, loss = 39.68501895\n",
      "Iteration 8625, loss = 39.59824714\n",
      "Iteration 8626, loss = 39.51421297\n",
      "Iteration 8627, loss = 39.44600560\n",
      "Iteration 8628, loss = 39.36154312\n",
      "Iteration 8629, loss = 39.26656351\n",
      "Iteration 8630, loss = 39.18361425\n",
      "Iteration 8631, loss = 39.10439245\n",
      "Iteration 8632, loss = 39.01596003\n",
      "Iteration 8633, loss = 38.93602242\n",
      "Iteration 8634, loss = 38.85008273\n",
      "Iteration 8635, loss = 38.76779853\n",
      "Iteration 8636, loss = 38.69376070\n",
      "Iteration 8637, loss = 38.60087756\n",
      "Iteration 8638, loss = 38.51990415\n",
      "Iteration 8639, loss = 38.43950265\n",
      "Iteration 8640, loss = 38.35370850\n",
      "Iteration 8641, loss = 38.26416879\n",
      "Iteration 8642, loss = 38.18486770\n",
      "Iteration 8643, loss = 38.09999142\n",
      "Iteration 8644, loss = 38.01397403\n",
      "Iteration 8645, loss = 37.92926231\n",
      "Iteration 8646, loss = 37.84340425\n",
      "Iteration 8647, loss = 37.75921895\n",
      "Iteration 8648, loss = 37.67618960\n",
      "Iteration 8649, loss = 37.58747864\n",
      "Iteration 8650, loss = 37.50310638\n",
      "Iteration 8651, loss = 37.41137003\n",
      "Iteration 8652, loss = 37.32620914\n",
      "Iteration 8653, loss = 37.23796623\n",
      "Iteration 8654, loss = 37.14821641\n",
      "Iteration 8655, loss = 37.05888948\n",
      "Iteration 8656, loss = 36.97053011\n",
      "Iteration 8657, loss = 36.87741809\n",
      "Iteration 8658, loss = 36.78781216\n",
      "Iteration 8659, loss = 36.69381605\n",
      "Iteration 8660, loss = 36.59926966\n",
      "Iteration 8661, loss = 36.50317207\n",
      "Iteration 8662, loss = 36.40631215\n",
      "Iteration 8663, loss = 36.31507562\n",
      "Iteration 8664, loss = 36.21678941\n",
      "Iteration 8665, loss = 36.11927985\n",
      "Iteration 8666, loss = 36.02680003\n",
      "Iteration 8667, loss = 35.92336545\n",
      "Iteration 8668, loss = 35.82133213\n",
      "Iteration 8669, loss = 35.71790443\n",
      "Iteration 8670, loss = 35.61422792\n",
      "Iteration 8671, loss = 35.50769819\n",
      "Iteration 8672, loss = 35.40823827\n",
      "Iteration 8673, loss = 35.29901376\n",
      "Iteration 8674, loss = 35.19870639\n",
      "Iteration 8675, loss = 35.08680156\n",
      "Iteration 8676, loss = 34.98795920\n",
      "Iteration 8677, loss = 34.87617345\n",
      "Iteration 8678, loss = 34.76846331\n",
      "Iteration 8679, loss = 34.66200837\n",
      "Iteration 8680, loss = 34.55512247\n",
      "Iteration 8681, loss = 34.44496968\n",
      "Iteration 8682, loss = 34.33599420\n",
      "Iteration 8683, loss = 34.22587624\n",
      "Iteration 8684, loss = 34.12349937\n",
      "Iteration 8685, loss = 34.01143070\n",
      "Iteration 8686, loss = 33.90735592\n",
      "Iteration 8687, loss = 33.79541726\n",
      "Iteration 8688, loss = 33.68469086\n",
      "Iteration 8689, loss = 33.57779462\n",
      "Iteration 8690, loss = 33.47719253\n",
      "Iteration 8691, loss = 33.36290447\n",
      "Iteration 8692, loss = 33.25888987\n",
      "Iteration 8693, loss = 33.15487982\n",
      "Iteration 8694, loss = 33.05035450\n",
      "Iteration 8695, loss = 32.94385673\n",
      "Iteration 8696, loss = 32.84586507\n",
      "Iteration 8697, loss = 32.74002072\n",
      "Iteration 8698, loss = 32.63627484\n",
      "Iteration 8699, loss = 32.53540516\n",
      "Iteration 8700, loss = 32.42675974\n",
      "Iteration 8701, loss = 32.33294255\n",
      "Iteration 8702, loss = 32.22581726\n",
      "Iteration 8703, loss = 32.13191288\n",
      "Iteration 8704, loss = 32.02818552\n",
      "Iteration 8705, loss = 31.92925136\n",
      "Iteration 8706, loss = 31.83401869\n",
      "Iteration 8707, loss = 31.73828651\n",
      "Iteration 8708, loss = 31.63842254\n",
      "Iteration 8709, loss = 31.55306725\n",
      "Iteration 8710, loss = 31.44113701\n",
      "Iteration 8711, loss = 31.35084085\n",
      "Iteration 8712, loss = 31.25866024\n",
      "Iteration 8713, loss = 31.16291120\n",
      "Iteration 8714, loss = 31.07201430\n",
      "Iteration 8715, loss = 30.97696067\n",
      "Iteration 8716, loss = 30.88110274\n",
      "Iteration 8717, loss = 30.79759518\n",
      "Iteration 8718, loss = 30.70497039\n",
      "Iteration 8719, loss = 30.61332316\n",
      "Iteration 8720, loss = 30.52352179\n",
      "Iteration 8721, loss = 30.43170197\n",
      "Iteration 8722, loss = 30.34299462\n",
      "Iteration 8723, loss = 30.25090227\n",
      "Iteration 8724, loss = 30.16140145\n",
      "Iteration 8725, loss = 30.08441630\n",
      "Iteration 8726, loss = 29.98774209\n",
      "Iteration 8727, loss = 29.89920856\n",
      "Iteration 8728, loss = 29.81428997\n",
      "Iteration 8729, loss = 29.73395097\n",
      "Iteration 8730, loss = 29.64591344\n",
      "Iteration 8731, loss = 29.56971084\n",
      "Iteration 8732, loss = 29.47605939\n",
      "Iteration 8733, loss = 29.39010349\n",
      "Iteration 8734, loss = 29.30893807\n",
      "Iteration 8735, loss = 29.23358267\n",
      "Iteration 8736, loss = 29.14265793\n",
      "Iteration 8737, loss = 29.05975394\n",
      "Iteration 8738, loss = 28.97648526\n",
      "Iteration 8739, loss = 28.90198086\n",
      "Iteration 8740, loss = 28.81456642\n",
      "Iteration 8741, loss = 28.73931633\n",
      "Iteration 8742, loss = 28.65440076\n",
      "Iteration 8743, loss = 28.57717708\n",
      "Iteration 8744, loss = 28.50241000\n",
      "Iteration 8745, loss = 28.41577992\n",
      "Iteration 8746, loss = 28.33949032\n",
      "Iteration 8747, loss = 28.25791338\n",
      "Iteration 8748, loss = 28.18902791\n",
      "Iteration 8749, loss = 28.10563040\n",
      "Iteration 8750, loss = 28.02861505\n",
      "Iteration 8751, loss = 27.95589092\n",
      "Iteration 8752, loss = 27.87861813\n",
      "Iteration 8753, loss = 27.80238889\n",
      "Iteration 8754, loss = 27.72598529\n",
      "Iteration 8755, loss = 27.65530687\n",
      "Iteration 8756, loss = 27.57368451\n",
      "Iteration 8757, loss = 27.50113805\n",
      "Iteration 8758, loss = 27.42776612\n",
      "Iteration 8759, loss = 27.36716104\n",
      "Iteration 8760, loss = 27.29886835\n",
      "Iteration 8761, loss = 27.21103438\n",
      "Iteration 8762, loss = 27.13361667\n",
      "Iteration 8763, loss = 27.06332008\n",
      "Iteration 8764, loss = 26.98832858\n",
      "Iteration 8765, loss = 26.92011183\n",
      "Iteration 8766, loss = 26.84735553\n",
      "Iteration 8767, loss = 26.77775400\n",
      "Iteration 8768, loss = 26.70686057\n",
      "Iteration 8769, loss = 26.63623322\n",
      "Iteration 8770, loss = 26.56518821\n",
      "Iteration 8771, loss = 26.49281222\n",
      "Iteration 8772, loss = 26.42372341\n",
      "Iteration 8773, loss = 26.35141871\n",
      "Iteration 8774, loss = 26.28602604\n",
      "Iteration 8775, loss = 26.21510493\n",
      "Iteration 8776, loss = 26.14500664\n",
      "Iteration 8777, loss = 26.07781076\n",
      "Iteration 8778, loss = 26.00947434\n",
      "Iteration 8779, loss = 25.94271594\n",
      "Iteration 8780, loss = 25.87649822\n",
      "Iteration 8781, loss = 25.81180459\n",
      "Iteration 8782, loss = 25.73870932\n",
      "Iteration 8783, loss = 25.67427477\n",
      "Iteration 8784, loss = 25.60318189\n",
      "Iteration 8785, loss = 25.54294136\n",
      "Iteration 8786, loss = 25.47561220\n",
      "Iteration 8787, loss = 25.41354637\n",
      "Iteration 8788, loss = 25.34436967\n",
      "Iteration 8789, loss = 25.28396320\n",
      "Iteration 8790, loss = 25.21204007\n",
      "Iteration 8791, loss = 25.15729469\n",
      "Iteration 8792, loss = 25.08480502\n",
      "Iteration 8793, loss = 25.03000817\n",
      "Iteration 8794, loss = 24.96124759\n",
      "Iteration 8795, loss = 24.89418793\n",
      "Iteration 8796, loss = 24.83040285\n",
      "Iteration 8797, loss = 24.76341808\n",
      "Iteration 8798, loss = 24.70716183\n",
      "Iteration 8799, loss = 24.63888089\n",
      "Iteration 8800, loss = 24.57627290\n",
      "Iteration 8801, loss = 24.51826354\n",
      "Iteration 8802, loss = 24.45730577\n",
      "Iteration 8803, loss = 24.39668977\n",
      "Iteration 8804, loss = 24.33924556\n",
      "Iteration 8805, loss = 24.27042342\n",
      "Iteration 8806, loss = 24.21230287\n",
      "Iteration 8807, loss = 24.14587581\n",
      "Iteration 8808, loss = 24.08439091\n",
      "Iteration 8809, loss = 24.03229405\n",
      "Iteration 8810, loss = 23.97132979\n",
      "Iteration 8811, loss = 23.91608530\n",
      "Iteration 8812, loss = 23.85091639\n",
      "Iteration 8813, loss = 23.78838907\n",
      "Iteration 8814, loss = 23.72653747\n",
      "Iteration 8815, loss = 23.67295430\n",
      "Iteration 8816, loss = 23.61113276\n",
      "Iteration 8817, loss = 23.55429188\n",
      "Iteration 8818, loss = 23.49295871\n",
      "Iteration 8819, loss = 23.43897508\n",
      "Iteration 8820, loss = 23.37458962\n",
      "Iteration 8821, loss = 23.32671413\n",
      "Iteration 8822, loss = 23.26386920\n",
      "Iteration 8823, loss = 23.21195219\n",
      "Iteration 8824, loss = 23.14518746\n",
      "Iteration 8825, loss = 23.09043367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8826, loss = 23.03549641\n",
      "Iteration 8827, loss = 22.97717699\n",
      "Iteration 8828, loss = 22.92804901\n",
      "Iteration 8829, loss = 22.86648081\n",
      "Iteration 8830, loss = 22.81089099\n",
      "Iteration 8831, loss = 22.75322703\n",
      "Iteration 8832, loss = 22.69328384\n",
      "Iteration 8833, loss = 22.63941757\n",
      "Iteration 8834, loss = 22.58754323\n",
      "Iteration 8835, loss = 22.53114786\n",
      "Iteration 8836, loss = 22.47837964\n",
      "Iteration 8837, loss = 22.41959642\n",
      "Iteration 8838, loss = 22.36746397\n",
      "Iteration 8839, loss = 22.31531694\n",
      "Iteration 8840, loss = 22.25908010\n",
      "Iteration 8841, loss = 22.20802940\n",
      "Iteration 8842, loss = 22.15495491\n",
      "Iteration 8843, loss = 22.09683566\n",
      "Iteration 8844, loss = 22.04545394\n",
      "Iteration 8845, loss = 21.99270898\n",
      "Iteration 8846, loss = 21.94097844\n",
      "Iteration 8847, loss = 21.88511016\n",
      "Iteration 8848, loss = 21.83386932\n",
      "Iteration 8849, loss = 21.78121142\n",
      "Iteration 8850, loss = 21.73148407\n",
      "Iteration 8851, loss = 21.67759581\n",
      "Iteration 8852, loss = 21.62352620\n",
      "Iteration 8853, loss = 21.57455623\n",
      "Iteration 8854, loss = 21.52100026\n",
      "Iteration 8855, loss = 21.46822012\n",
      "Iteration 8856, loss = 21.41817639\n",
      "Iteration 8857, loss = 21.37352362\n",
      "Iteration 8858, loss = 21.31657066\n",
      "Iteration 8859, loss = 21.26443316\n",
      "Iteration 8860, loss = 21.21663485\n",
      "Iteration 8861, loss = 21.16832751\n",
      "Iteration 8862, loss = 21.11217021\n",
      "Iteration 8863, loss = 21.06183510\n",
      "Iteration 8864, loss = 21.01419451\n",
      "Iteration 8865, loss = 20.96025157\n",
      "Iteration 8866, loss = 20.91465484\n",
      "Iteration 8867, loss = 20.86953027\n",
      "Iteration 8868, loss = 20.82560633\n",
      "Iteration 8869, loss = 20.76831584\n",
      "Iteration 8870, loss = 20.71769790\n",
      "Iteration 8871, loss = 20.66875576\n",
      "Iteration 8872, loss = 20.61764049\n",
      "Iteration 8873, loss = 20.57359849\n",
      "Iteration 8874, loss = 20.52314076\n",
      "Iteration 8875, loss = 20.47194450\n",
      "Iteration 8876, loss = 20.42542945\n",
      "Iteration 8877, loss = 20.37720458\n",
      "Iteration 8878, loss = 20.33219393\n",
      "Iteration 8879, loss = 20.31563364\n",
      "Iteration 8880, loss = 20.24444529\n",
      "Iteration 8881, loss = 20.19314389\n",
      "Iteration 8882, loss = 20.13368799\n",
      "Iteration 8883, loss = 20.09900271\n",
      "Iteration 8884, loss = 20.04380327\n",
      "Iteration 8885, loss = 20.00556790\n",
      "Iteration 8886, loss = 19.94953968\n",
      "Iteration 8887, loss = 19.90990123\n",
      "Iteration 8888, loss = 19.85955484\n",
      "Iteration 8889, loss = 19.81005401\n",
      "Iteration 8890, loss = 19.77043554\n",
      "Iteration 8891, loss = 19.72120055\n",
      "Iteration 8892, loss = 19.67746019\n",
      "Iteration 8893, loss = 19.62644052\n",
      "Iteration 8894, loss = 19.58364730\n",
      "Iteration 8895, loss = 19.54182239\n",
      "Iteration 8896, loss = 19.49315147\n",
      "Iteration 8897, loss = 19.44960746\n",
      "Iteration 8898, loss = 19.40436604\n",
      "Iteration 8899, loss = 19.36723564\n",
      "Iteration 8900, loss = 19.31526085\n",
      "Iteration 8901, loss = 19.26609879\n",
      "Iteration 8902, loss = 19.22310151\n",
      "Iteration 8903, loss = 19.17562526\n",
      "Iteration 8904, loss = 19.13536360\n",
      "Iteration 8905, loss = 19.09096258\n",
      "Iteration 8906, loss = 19.05161693\n",
      "Iteration 8907, loss = 19.00123095\n",
      "Iteration 8908, loss = 18.95932785\n",
      "Iteration 8909, loss = 18.91243899\n",
      "Iteration 8910, loss = 18.88020809\n",
      "Iteration 8911, loss = 18.83326555\n",
      "Iteration 8912, loss = 18.78776412\n",
      "Iteration 8913, loss = 18.74010282\n",
      "Iteration 8914, loss = 18.69783053\n",
      "Iteration 8915, loss = 18.65080873\n",
      "Iteration 8916, loss = 18.61295837\n",
      "Iteration 8917, loss = 18.57462333\n",
      "Iteration 8918, loss = 18.52646019\n",
      "Iteration 8919, loss = 18.48341005\n",
      "Iteration 8920, loss = 18.44108191\n",
      "Iteration 8921, loss = 18.40365124\n",
      "Iteration 8922, loss = 18.36065894\n",
      "Iteration 8923, loss = 18.31872196\n",
      "Iteration 8924, loss = 18.26932031\n",
      "Iteration 8925, loss = 18.22702519\n",
      "Iteration 8926, loss = 18.18629963\n",
      "Iteration 8927, loss = 18.14503525\n",
      "Iteration 8928, loss = 18.10843061\n",
      "Iteration 8929, loss = 18.06157706\n",
      "Iteration 8930, loss = 18.01705313\n",
      "Iteration 8931, loss = 17.98049778\n",
      "Iteration 8932, loss = 17.93829788\n",
      "Iteration 8933, loss = 17.89941282\n",
      "Iteration 8934, loss = 17.85420949\n",
      "Iteration 8935, loss = 17.81306443\n",
      "Iteration 8936, loss = 17.77749385\n",
      "Iteration 8937, loss = 17.73518179\n",
      "Iteration 8938, loss = 17.68945331\n",
      "Iteration 8939, loss = 17.65168829\n",
      "Iteration 8940, loss = 17.60930140\n",
      "Iteration 8941, loss = 17.56778817\n",
      "Iteration 8942, loss = 17.52855193\n",
      "Iteration 8943, loss = 17.48929000\n",
      "Iteration 8944, loss = 17.44675631\n",
      "Iteration 8945, loss = 17.40976762\n",
      "Iteration 8946, loss = 17.36580115\n",
      "Iteration 8947, loss = 17.33013761\n",
      "Iteration 8948, loss = 17.29325517\n",
      "Iteration 8949, loss = 17.25222078\n",
      "Iteration 8950, loss = 17.20670460\n",
      "Iteration 8951, loss = 17.16560957\n",
      "Iteration 8952, loss = 17.12894870\n",
      "Iteration 8953, loss = 17.09151092\n",
      "Iteration 8954, loss = 17.05065929\n",
      "Iteration 8955, loss = 17.01773266\n",
      "Iteration 8956, loss = 16.97424873\n",
      "Iteration 8957, loss = 16.94414228\n",
      "Iteration 8958, loss = 16.88995222\n",
      "Iteration 8959, loss = 16.85421022\n",
      "Iteration 8960, loss = 16.81703773\n",
      "Iteration 8961, loss = 16.77490543\n",
      "Iteration 8962, loss = 16.73630213\n",
      "Iteration 8963, loss = 16.70128300\n",
      "Iteration 8964, loss = 16.66115973\n",
      "Iteration 8965, loss = 16.62477682\n",
      "Iteration 8966, loss = 16.58694508\n",
      "Iteration 8967, loss = 16.54266536\n",
      "Iteration 8968, loss = 16.50201431\n",
      "Iteration 8969, loss = 16.46597225\n",
      "Iteration 8970, loss = 16.42935816\n",
      "Iteration 8971, loss = 16.39021746\n",
      "Iteration 8972, loss = 16.35193308\n",
      "Iteration 8973, loss = 16.31567680\n",
      "Iteration 8974, loss = 16.27440781\n",
      "Iteration 8975, loss = 16.23678532\n",
      "Iteration 8976, loss = 16.20511587\n",
      "Iteration 8977, loss = 16.16434835\n",
      "Iteration 8978, loss = 16.12857298\n",
      "Iteration 8979, loss = 16.08585976\n",
      "Iteration 8980, loss = 16.05179690\n",
      "Iteration 8981, loss = 16.01659663\n",
      "Iteration 8982, loss = 15.98013344\n",
      "Iteration 8983, loss = 15.94231254\n",
      "Iteration 8984, loss = 15.90780987\n",
      "Iteration 8985, loss = 15.86848071\n",
      "Iteration 8986, loss = 15.83844159\n",
      "Iteration 8987, loss = 15.80183603\n",
      "Iteration 8988, loss = 15.75964601\n",
      "Iteration 8989, loss = 15.71782807\n",
      "Iteration 8990, loss = 15.68293702\n",
      "Iteration 8991, loss = 15.64545400\n",
      "Iteration 8992, loss = 15.60750711\n",
      "Iteration 8993, loss = 15.57385176\n",
      "Iteration 8994, loss = 15.53915638\n",
      "Iteration 8995, loss = 15.50338607\n",
      "Iteration 8996, loss = 15.47444584\n",
      "Iteration 8997, loss = 15.42772589\n",
      "Iteration 8998, loss = 15.40049651\n",
      "Iteration 8999, loss = 15.35903492\n",
      "Iteration 9000, loss = 15.32267329\n",
      "Iteration 9001, loss = 15.29198352\n",
      "Iteration 9002, loss = 15.25315614\n",
      "Iteration 9003, loss = 15.21809156\n",
      "Iteration 9004, loss = 15.18294276\n",
      "Iteration 9005, loss = 15.15120356\n",
      "Iteration 9006, loss = 15.12645289\n",
      "Iteration 9007, loss = 15.08384804\n",
      "Iteration 9008, loss = 15.04194925\n",
      "Iteration 9009, loss = 15.01474014\n",
      "Iteration 9010, loss = 14.97424952\n",
      "Iteration 9011, loss = 14.94208039\n",
      "Iteration 9012, loss = 14.90478946\n",
      "Iteration 9013, loss = 14.87074569\n",
      "Iteration 9014, loss = 14.84237611\n",
      "Iteration 9015, loss = 14.80635153\n",
      "Iteration 9016, loss = 14.77990791\n",
      "Iteration 9017, loss = 14.75087832\n",
      "Iteration 9018, loss = 14.70313921\n",
      "Iteration 9019, loss = 14.66699979\n",
      "Iteration 9020, loss = 14.63251960\n",
      "Iteration 9021, loss = 14.59994529\n",
      "Iteration 9022, loss = 14.56694611\n",
      "Iteration 9023, loss = 14.53048573\n",
      "Iteration 9024, loss = 14.50781915\n",
      "Iteration 9025, loss = 14.46565256\n",
      "Iteration 9026, loss = 14.43829701\n",
      "Iteration 9027, loss = 14.40459580\n",
      "Iteration 9028, loss = 14.36356337\n",
      "Iteration 9029, loss = 14.33537648\n",
      "Iteration 9030, loss = 14.30032062\n",
      "Iteration 9031, loss = 14.27148469\n",
      "Iteration 9032, loss = 14.23614924\n",
      "Iteration 9033, loss = 14.20883567\n",
      "Iteration 9034, loss = 14.17008319\n",
      "Iteration 9035, loss = 14.13922376\n",
      "Iteration 9036, loss = 14.10680769\n",
      "Iteration 9037, loss = 14.08664711\n",
      "Iteration 9038, loss = 14.04419257\n",
      "Iteration 9039, loss = 14.01213921\n",
      "Iteration 9040, loss = 13.98865318\n",
      "Iteration 9041, loss = 13.94364616\n",
      "Iteration 9042, loss = 13.91727485\n",
      "Iteration 9043, loss = 13.88628866\n",
      "Iteration 9044, loss = 13.85235639\n",
      "Iteration 9045, loss = 13.81718839\n",
      "Iteration 9046, loss = 13.78403048\n",
      "Iteration 9047, loss = 13.75465242\n",
      "Iteration 9048, loss = 13.72206708\n",
      "Iteration 9049, loss = 13.69204335\n",
      "Iteration 9050, loss = 13.65870192\n",
      "Iteration 9051, loss = 13.63450413\n",
      "Iteration 9052, loss = 13.59607273\n",
      "Iteration 9053, loss = 13.56965022\n",
      "Iteration 9054, loss = 13.53730315\n",
      "Iteration 9055, loss = 13.50422886\n",
      "Iteration 9056, loss = 13.47505394\n",
      "Iteration 9057, loss = 13.44239622\n",
      "Iteration 9058, loss = 13.41450087\n",
      "Iteration 9059, loss = 13.38928143\n",
      "Iteration 9060, loss = 13.35082090\n",
      "Iteration 9061, loss = 13.32582278\n",
      "Iteration 9062, loss = 13.30126161\n",
      "Iteration 9063, loss = 13.26215045\n",
      "Iteration 9064, loss = 13.24277041\n",
      "Iteration 9065, loss = 13.20079550\n",
      "Iteration 9066, loss = 13.18237282\n",
      "Iteration 9067, loss = 13.14131630\n",
      "Iteration 9068, loss = 13.11527767\n",
      "Iteration 9069, loss = 13.08834481\n",
      "Iteration 9070, loss = 13.04854377\n",
      "Iteration 9071, loss = 13.02248491\n",
      "Iteration 9072, loss = 12.99682603\n",
      "Iteration 9073, loss = 12.96217434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9074, loss = 12.93853209\n",
      "Iteration 9075, loss = 12.91229244\n",
      "Iteration 9076, loss = 12.87743750\n",
      "Iteration 9077, loss = 12.84828751\n",
      "Iteration 9078, loss = 12.81849465\n",
      "Iteration 9079, loss = 12.78706205\n",
      "Iteration 9080, loss = 12.76174583\n",
      "Iteration 9081, loss = 12.72765794\n",
      "Iteration 9082, loss = 12.70277396\n",
      "Iteration 9083, loss = 12.67078966\n",
      "Iteration 9084, loss = 12.64194486\n",
      "Iteration 9085, loss = 12.61273779\n",
      "Iteration 9086, loss = 12.58582046\n",
      "Iteration 9087, loss = 12.55716607\n",
      "Iteration 9088, loss = 12.53180764\n",
      "Iteration 9089, loss = 12.49890990\n",
      "Iteration 9090, loss = 12.47198485\n",
      "Iteration 9091, loss = 12.44565088\n",
      "Iteration 9092, loss = 12.41725401\n",
      "Iteration 9093, loss = 12.39375863\n",
      "Iteration 9094, loss = 12.35894596\n",
      "Iteration 9095, loss = 12.33532994\n",
      "Iteration 9096, loss = 12.30391560\n",
      "Iteration 9097, loss = 12.27505898\n",
      "Iteration 9098, loss = 12.24849807\n",
      "Iteration 9099, loss = 12.22940426\n",
      "Iteration 9100, loss = 12.19847265\n",
      "Iteration 9101, loss = 12.16559406\n",
      "Iteration 9102, loss = 12.13673629\n",
      "Iteration 9103, loss = 12.11152345\n",
      "Iteration 9104, loss = 12.08368987\n",
      "Iteration 9105, loss = 12.06330084\n",
      "Iteration 9106, loss = 12.03864955\n",
      "Iteration 9107, loss = 12.00724435\n",
      "Iteration 9108, loss = 11.97838049\n",
      "Iteration 9109, loss = 11.94868983\n",
      "Iteration 9110, loss = 11.92053609\n",
      "Iteration 9111, loss = 11.89380164\n",
      "Iteration 9112, loss = 11.86834127\n",
      "Iteration 9113, loss = 11.83978475\n",
      "Iteration 9114, loss = 11.81808863\n",
      "Iteration 9115, loss = 11.79365622\n",
      "Iteration 9116, loss = 11.76203311\n",
      "Iteration 9117, loss = 11.73401577\n",
      "Iteration 9118, loss = 11.71234933\n",
      "Iteration 9119, loss = 11.68477101\n",
      "Iteration 9120, loss = 11.65514307\n",
      "Iteration 9121, loss = 11.63138276\n",
      "Iteration 9122, loss = 11.61154423\n",
      "Iteration 9123, loss = 11.57994923\n",
      "Iteration 9124, loss = 11.55601892\n",
      "Iteration 9125, loss = 11.53109194\n",
      "Iteration 9126, loss = 11.49988108\n",
      "Iteration 9127, loss = 11.47663877\n",
      "Iteration 9128, loss = 11.45418870\n",
      "Iteration 9129, loss = 11.42568748\n",
      "Iteration 9130, loss = 11.40095588\n",
      "Iteration 9131, loss = 11.37245872\n",
      "Iteration 9132, loss = 11.34700660\n",
      "Iteration 9133, loss = 11.32043164\n",
      "Iteration 9134, loss = 11.29661775\n",
      "Iteration 9135, loss = 11.27274976\n",
      "Iteration 9136, loss = 11.24801522\n",
      "Iteration 9137, loss = 11.22988503\n",
      "Iteration 9138, loss = 11.19545790\n",
      "Iteration 9139, loss = 11.18367073\n",
      "Iteration 9140, loss = 11.15504455\n",
      "Iteration 9141, loss = 11.12733368\n",
      "Iteration 9142, loss = 11.09806692\n",
      "Iteration 9143, loss = 11.07147703\n",
      "Iteration 9144, loss = 11.05015689\n",
      "Iteration 9145, loss = 11.02303943\n",
      "Iteration 9146, loss = 11.00199174\n",
      "Iteration 9147, loss = 10.97592429\n",
      "Iteration 9148, loss = 10.95266295\n",
      "Iteration 9149, loss = 10.92841134\n",
      "Iteration 9150, loss = 10.90300813\n",
      "Iteration 9151, loss = 10.88480192\n",
      "Iteration 9152, loss = 10.85982366\n",
      "Iteration 9153, loss = 10.83255855\n",
      "Iteration 9154, loss = 10.80583397\n",
      "Iteration 9155, loss = 10.78369774\n",
      "Iteration 9156, loss = 10.76166964\n",
      "Iteration 9157, loss = 10.74117948\n",
      "Iteration 9158, loss = 10.71346236\n",
      "Iteration 9159, loss = 10.68822315\n",
      "Iteration 9160, loss = 10.66626565\n",
      "Iteration 9161, loss = 10.64128317\n",
      "Iteration 9162, loss = 10.62114247\n",
      "Iteration 9163, loss = 10.59502851\n",
      "Iteration 9164, loss = 10.57663315\n",
      "Iteration 9165, loss = 10.54863074\n",
      "Iteration 9166, loss = 10.52466156\n",
      "Iteration 9167, loss = 10.50280121\n",
      "Iteration 9168, loss = 10.47779602\n",
      "Iteration 9169, loss = 10.45436605\n",
      "Iteration 9170, loss = 10.43277916\n",
      "Iteration 9171, loss = 10.41896797\n",
      "Iteration 9172, loss = 10.38654924\n",
      "Iteration 9173, loss = 10.37001185\n",
      "Iteration 9174, loss = 10.34036006\n",
      "Iteration 9175, loss = 10.31804482\n",
      "Iteration 9176, loss = 10.30283288\n",
      "Iteration 9177, loss = 10.27987955\n",
      "Iteration 9178, loss = 10.25019905\n",
      "Iteration 9179, loss = 10.22914893\n",
      "Iteration 9180, loss = 10.20855962\n",
      "Iteration 9181, loss = 10.19618563\n",
      "Iteration 9182, loss = 10.16164411\n",
      "Iteration 9183, loss = 10.13891552\n",
      "Iteration 9184, loss = 10.11822642\n",
      "Iteration 9185, loss = 10.10660369\n",
      "Iteration 9186, loss = 10.07555604\n",
      "Iteration 9187, loss = 10.05203906\n",
      "Iteration 9188, loss = 10.03630631\n",
      "Iteration 9189, loss = 10.01355701\n",
      "Iteration 9190, loss = 9.99058392\n",
      "Iteration 9191, loss = 9.96458689\n",
      "Iteration 9192, loss = 9.94192333\n",
      "Iteration 9193, loss = 9.92136352\n",
      "Iteration 9194, loss = 9.91599424\n",
      "Iteration 9195, loss = 9.89050751\n",
      "Iteration 9196, loss = 9.86683602\n",
      "Iteration 9197, loss = 9.83818256\n",
      "Iteration 9198, loss = 9.81863602\n",
      "Iteration 9199, loss = 9.80270233\n",
      "Iteration 9200, loss = 9.77748125\n",
      "Iteration 9201, loss = 9.75072044\n",
      "Iteration 9202, loss = 9.73913386\n",
      "Iteration 9203, loss = 9.71099930\n",
      "Iteration 9204, loss = 9.69928846\n",
      "Iteration 9205, loss = 9.66938108\n",
      "Iteration 9206, loss = 9.65271540\n",
      "Iteration 9207, loss = 9.63524100\n",
      "Iteration 9208, loss = 9.60773825\n",
      "Iteration 9209, loss = 9.58699175\n",
      "Iteration 9210, loss = 9.56856166\n",
      "Iteration 9211, loss = 9.54914604\n",
      "Iteration 9212, loss = 9.52349725\n",
      "Iteration 9213, loss = 9.50548903\n",
      "Iteration 9214, loss = 9.48279514\n",
      "Iteration 9215, loss = 9.46452362\n",
      "Iteration 9216, loss = 9.44679144\n",
      "Iteration 9217, loss = 9.42656380\n",
      "Iteration 9218, loss = 9.40208549\n",
      "Iteration 9219, loss = 9.38672025\n",
      "Iteration 9220, loss = 9.36183012\n",
      "Iteration 9221, loss = 9.34241617\n",
      "Iteration 9222, loss = 9.32377658\n",
      "Iteration 9223, loss = 9.30289955\n",
      "Iteration 9224, loss = 9.28607790\n",
      "Iteration 9225, loss = 9.26727514\n",
      "Iteration 9226, loss = 9.24191235\n",
      "Iteration 9227, loss = 9.22972488\n",
      "Iteration 9228, loss = 9.20591341\n",
      "Iteration 9229, loss = 9.18781677\n",
      "Iteration 9230, loss = 9.16327422\n",
      "Iteration 9231, loss = 9.14668926\n",
      "Iteration 9232, loss = 9.12580025\n",
      "Iteration 9233, loss = 9.10763851\n",
      "Iteration 9234, loss = 9.08346743\n",
      "Iteration 9235, loss = 9.06620090\n",
      "Iteration 9236, loss = 9.05883375\n",
      "Iteration 9237, loss = 9.02880708\n",
      "Iteration 9238, loss = 9.01577638\n",
      "Iteration 9239, loss = 8.99250099\n",
      "Iteration 9240, loss = 8.97702269\n",
      "Iteration 9241, loss = 8.95220057\n",
      "Iteration 9242, loss = 8.93606382\n",
      "Iteration 9243, loss = 8.91606371\n",
      "Iteration 9244, loss = 8.89704000\n",
      "Iteration 9245, loss = 8.87622069\n",
      "Iteration 9246, loss = 8.86303450\n",
      "Iteration 9247, loss = 8.84187025\n",
      "Iteration 9248, loss = 8.82569579\n",
      "Iteration 9249, loss = 8.80310090\n",
      "Iteration 9250, loss = 8.79429320\n",
      "Iteration 9251, loss = 8.77093508\n",
      "Iteration 9252, loss = 8.74973213\n",
      "Iteration 9253, loss = 8.73125014\n",
      "Iteration 9254, loss = 8.71406889\n",
      "Iteration 9255, loss = 8.69356126\n",
      "Iteration 9256, loss = 8.68073976\n",
      "Iteration 9257, loss = 8.65938049\n",
      "Iteration 9258, loss = 8.64201073\n",
      "Iteration 9259, loss = 8.62072773\n",
      "Iteration 9260, loss = 8.60999997\n",
      "Iteration 9261, loss = 8.58778890\n",
      "Iteration 9262, loss = 8.56941217\n",
      "Iteration 9263, loss = 8.55300079\n",
      "Iteration 9264, loss = 8.53540087\n",
      "Iteration 9265, loss = 8.51591600\n",
      "Iteration 9266, loss = 8.49566433\n",
      "Iteration 9267, loss = 8.47884896\n",
      "Iteration 9268, loss = 8.46037632\n",
      "Iteration 9269, loss = 8.44716519\n",
      "Iteration 9270, loss = 8.42987453\n",
      "Iteration 9271, loss = 8.41134650\n",
      "Iteration 9272, loss = 8.39948508\n",
      "Iteration 9273, loss = 8.37810964\n",
      "Iteration 9274, loss = 8.35683421\n",
      "Iteration 9275, loss = 8.33795489\n",
      "Iteration 9276, loss = 8.32001749\n",
      "Iteration 9277, loss = 8.30976035\n",
      "Iteration 9278, loss = 8.29419929\n",
      "Iteration 9279, loss = 8.26921679\n",
      "Iteration 9280, loss = 8.25470720\n",
      "Iteration 9281, loss = 8.23764548\n",
      "Iteration 9282, loss = 8.22035748\n",
      "Iteration 9283, loss = 8.20679390\n",
      "Iteration 9284, loss = 8.18707223\n",
      "Iteration 9285, loss = 8.18108132\n",
      "Iteration 9286, loss = 8.15719325\n",
      "Iteration 9287, loss = 8.13668746\n",
      "Iteration 9288, loss = 8.11836530\n",
      "Iteration 9289, loss = 8.10330884\n",
      "Iteration 9290, loss = 8.08287746\n",
      "Iteration 9291, loss = 8.07733945\n",
      "Iteration 9292, loss = 8.05669655\n",
      "Iteration 9293, loss = 8.03497385\n",
      "Iteration 9294, loss = 8.02065722\n",
      "Iteration 9295, loss = 8.00578852\n",
      "Iteration 9296, loss = 7.98926305\n",
      "Iteration 9297, loss = 7.97726005\n",
      "Iteration 9298, loss = 7.95465091\n",
      "Iteration 9299, loss = 7.94152843\n",
      "Iteration 9300, loss = 7.92142598\n",
      "Iteration 9301, loss = 7.90564109\n",
      "Iteration 9302, loss = 7.89363430\n",
      "Iteration 9303, loss = 7.87940857\n",
      "Iteration 9304, loss = 7.86291714\n",
      "Iteration 9305, loss = 7.84317624\n",
      "Iteration 9306, loss = 7.82643705\n",
      "Iteration 9307, loss = 7.81781714\n",
      "Iteration 9308, loss = 7.79745255\n",
      "Iteration 9309, loss = 7.77696544\n",
      "Iteration 9310, loss = 7.76556483\n",
      "Iteration 9311, loss = 7.74813501\n",
      "Iteration 9312, loss = 7.73415416\n",
      "Iteration 9313, loss = 7.72584159\n",
      "Iteration 9314, loss = 7.70330030\n",
      "Iteration 9315, loss = 7.68714306\n",
      "Iteration 9316, loss = 7.67764271\n",
      "Iteration 9317, loss = 7.65333693\n",
      "Iteration 9318, loss = 7.64191748\n",
      "Iteration 9319, loss = 7.62564094\n",
      "Iteration 9320, loss = 7.60838244\n",
      "Iteration 9321, loss = 7.59852155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9322, loss = 7.58475280\n",
      "Iteration 9323, loss = 7.56515332\n",
      "Iteration 9324, loss = 7.55565697\n",
      "Iteration 9325, loss = 7.53472153\n",
      "Iteration 9326, loss = 7.52038037\n",
      "Iteration 9327, loss = 7.50851907\n",
      "Iteration 9328, loss = 7.49197029\n",
      "Iteration 9329, loss = 7.47416401\n",
      "Iteration 9330, loss = 7.46081811\n",
      "Iteration 9331, loss = 7.44220206\n",
      "Iteration 9332, loss = 7.42803497\n",
      "Iteration 9333, loss = 7.41558990\n",
      "Iteration 9334, loss = 7.40567935\n",
      "Iteration 9335, loss = 7.38309102\n",
      "Iteration 9336, loss = 7.37795396\n",
      "Iteration 9337, loss = 7.35782022\n",
      "Iteration 9338, loss = 7.34186155\n",
      "Iteration 9339, loss = 7.32739172\n",
      "Iteration 9340, loss = 7.31398702\n",
      "Iteration 9341, loss = 7.30086897\n",
      "Iteration 9342, loss = 7.28303239\n",
      "Iteration 9343, loss = 7.27936850\n",
      "Iteration 9344, loss = 7.25922425\n",
      "Iteration 9345, loss = 7.24231825\n",
      "Iteration 9346, loss = 7.23370558\n",
      "Iteration 9347, loss = 7.21769048\n",
      "Iteration 9348, loss = 7.20264848\n",
      "Iteration 9349, loss = 7.19101960\n",
      "Iteration 9350, loss = 7.18254614\n",
      "Iteration 9351, loss = 7.16158168\n",
      "Iteration 9352, loss = 7.14668018\n",
      "Iteration 9353, loss = 7.13633213\n",
      "Iteration 9354, loss = 7.11660237\n",
      "Iteration 9355, loss = 7.09933634\n",
      "Iteration 9356, loss = 7.08875365\n",
      "Iteration 9357, loss = 7.07680239\n",
      "Iteration 9358, loss = 7.05925576\n",
      "Iteration 9359, loss = 7.04630764\n",
      "Iteration 9360, loss = 7.03266895\n",
      "Iteration 9361, loss = 7.02073101\n",
      "Iteration 9362, loss = 7.00629083\n",
      "Iteration 9363, loss = 6.99955006\n",
      "Iteration 9364, loss = 6.98551063\n",
      "Iteration 9365, loss = 6.96723522\n",
      "Iteration 9366, loss = 6.95661359\n",
      "Iteration 9367, loss = 6.93776429\n",
      "Iteration 9368, loss = 6.92584263\n",
      "Iteration 9369, loss = 6.91971907\n",
      "Iteration 9370, loss = 6.90302037\n",
      "Iteration 9371, loss = 6.88674341\n",
      "Iteration 9372, loss = 6.87787986\n",
      "Iteration 9373, loss = 6.86499551\n",
      "Iteration 9374, loss = 6.84954217\n",
      "Iteration 9375, loss = 6.83528175\n",
      "Iteration 9376, loss = 6.82424892\n",
      "Iteration 9377, loss = 6.80343742\n",
      "Iteration 9378, loss = 6.79608376\n",
      "Iteration 9379, loss = 6.78090967\n",
      "Iteration 9380, loss = 6.77143825\n",
      "Iteration 9381, loss = 6.75931377\n",
      "Iteration 9382, loss = 6.74193561\n",
      "Iteration 9383, loss = 6.74369075\n",
      "Iteration 9384, loss = 6.71296948\n",
      "Iteration 9385, loss = 6.70868195\n",
      "Iteration 9386, loss = 6.69731417\n",
      "Iteration 9387, loss = 6.68321605\n",
      "Iteration 9388, loss = 6.67060088\n",
      "Iteration 9389, loss = 6.65430014\n",
      "Iteration 9390, loss = 6.64169961\n",
      "Iteration 9391, loss = 6.63434744\n",
      "Iteration 9392, loss = 6.61749239\n",
      "Iteration 9393, loss = 6.60624585\n",
      "Iteration 9394, loss = 6.59342425\n",
      "Iteration 9395, loss = 6.58295226\n",
      "Iteration 9396, loss = 6.56637013\n",
      "Iteration 9397, loss = 6.55841264\n",
      "Iteration 9398, loss = 6.54344861\n",
      "Iteration 9399, loss = 6.53375802\n",
      "Iteration 9400, loss = 6.52445855\n",
      "Iteration 9401, loss = 6.50700252\n",
      "Iteration 9402, loss = 6.49377462\n",
      "Iteration 9403, loss = 6.48159041\n",
      "Iteration 9404, loss = 6.46863830\n",
      "Iteration 9405, loss = 6.46056155\n",
      "Iteration 9406, loss = 6.45142812\n",
      "Iteration 9407, loss = 6.43362961\n",
      "Iteration 9408, loss = 6.42342057\n",
      "Iteration 9409, loss = 6.41348154\n",
      "Iteration 9410, loss = 6.39658008\n",
      "Iteration 9411, loss = 6.38528990\n",
      "Iteration 9412, loss = 6.37178792\n",
      "Iteration 9413, loss = 6.36631659\n",
      "Iteration 9414, loss = 6.35340452\n",
      "Iteration 9415, loss = 6.33933982\n",
      "Iteration 9416, loss = 6.32924911\n",
      "Iteration 9417, loss = 6.31870057\n",
      "Iteration 9418, loss = 6.30960174\n",
      "Iteration 9419, loss = 6.29999407\n",
      "Iteration 9420, loss = 6.28204962\n",
      "Iteration 9421, loss = 6.27394937\n",
      "Iteration 9422, loss = 6.25644979\n",
      "Iteration 9423, loss = 6.25311379\n",
      "Iteration 9424, loss = 6.24116833\n",
      "Iteration 9425, loss = 6.23034265\n",
      "Iteration 9426, loss = 6.21018782\n",
      "Iteration 9427, loss = 6.19928377\n",
      "Iteration 9428, loss = 6.19429742\n",
      "Iteration 9429, loss = 6.18283368\n",
      "Iteration 9430, loss = 6.17151898\n",
      "Iteration 9431, loss = 6.15703463\n",
      "Iteration 9432, loss = 6.15249634\n",
      "Iteration 9433, loss = 6.13292337\n",
      "Iteration 9434, loss = 6.12239981\n",
      "Iteration 9435, loss = 6.12175738\n",
      "Iteration 9436, loss = 6.10587858\n",
      "Iteration 9437, loss = 6.09286890\n",
      "Iteration 9438, loss = 6.07877338\n",
      "Iteration 9439, loss = 6.07039732\n",
      "Iteration 9440, loss = 6.06100702\n",
      "Iteration 9441, loss = 6.04956343\n",
      "Iteration 9442, loss = 6.03805491\n",
      "Iteration 9443, loss = 6.03243962\n",
      "Iteration 9444, loss = 6.01966120\n",
      "Iteration 9445, loss = 6.00813132\n",
      "Iteration 9446, loss = 5.99643736\n",
      "Iteration 9447, loss = 5.98792181\n",
      "Iteration 9448, loss = 5.97365287\n",
      "Iteration 9449, loss = 5.96716225\n",
      "Iteration 9450, loss = 5.95699953\n",
      "Iteration 9451, loss = 5.93893866\n",
      "Iteration 9452, loss = 5.93194506\n",
      "Iteration 9453, loss = 5.92405203\n",
      "Iteration 9454, loss = 5.91052192\n",
      "Iteration 9455, loss = 5.90362448\n",
      "Iteration 9456, loss = 5.89051472\n",
      "Iteration 9457, loss = 5.88057658\n",
      "Iteration 9458, loss = 5.86797369\n",
      "Iteration 9459, loss = 5.85955273\n",
      "Iteration 9460, loss = 5.85260249\n",
      "Iteration 9461, loss = 5.83972214\n",
      "Iteration 9462, loss = 5.82719386\n",
      "Iteration 9463, loss = 5.81893303\n",
      "Iteration 9464, loss = 5.80900262\n",
      "Iteration 9465, loss = 5.80386060\n",
      "Iteration 9466, loss = 5.79591082\n",
      "Iteration 9467, loss = 5.77601232\n",
      "Iteration 9468, loss = 5.76898798\n",
      "Iteration 9469, loss = 5.75871791\n",
      "Iteration 9470, loss = 5.74648188\n",
      "Iteration 9471, loss = 5.73606930\n",
      "Iteration 9472, loss = 5.72595544\n",
      "Iteration 9473, loss = 5.71849887\n",
      "Iteration 9474, loss = 5.70495066\n",
      "Iteration 9475, loss = 5.70033424\n",
      "Iteration 9476, loss = 5.69064888\n",
      "Iteration 9477, loss = 5.68064598\n",
      "Iteration 9478, loss = 5.67095789\n",
      "Iteration 9479, loss = 5.66331185\n",
      "Iteration 9480, loss = 5.64812078\n",
      "Iteration 9481, loss = 5.64530648\n",
      "Iteration 9482, loss = 5.63644470\n",
      "Iteration 9483, loss = 5.62042953\n",
      "Iteration 9484, loss = 5.60944294\n",
      "Iteration 9485, loss = 5.59865954\n",
      "Iteration 9486, loss = 5.59170583\n",
      "Iteration 9487, loss = 5.58482585\n",
      "Iteration 9488, loss = 5.57477639\n",
      "Iteration 9489, loss = 5.56281995\n",
      "Iteration 9490, loss = 5.55787233\n",
      "Iteration 9491, loss = 5.54490415\n",
      "Iteration 9492, loss = 5.53739230\n",
      "Iteration 9493, loss = 5.53085716\n",
      "Iteration 9494, loss = 5.52077665\n",
      "Iteration 9495, loss = 5.51475461\n",
      "Iteration 9496, loss = 5.50024673\n",
      "Iteration 9497, loss = 5.49393217\n",
      "Iteration 9498, loss = 5.48884441\n",
      "Iteration 9499, loss = 5.47160407\n",
      "Iteration 9500, loss = 5.46109621\n",
      "Iteration 9501, loss = 5.45338704\n",
      "Iteration 9502, loss = 5.44289618\n",
      "Iteration 9503, loss = 5.43771390\n",
      "Iteration 9504, loss = 5.42890979\n",
      "Iteration 9505, loss = 5.41558286\n",
      "Iteration 9506, loss = 5.41404648\n",
      "Iteration 9507, loss = 5.40912229\n",
      "Iteration 9508, loss = 5.39808449\n",
      "Iteration 9509, loss = 5.38854717\n",
      "Iteration 9510, loss = 5.37527921\n",
      "Iteration 9511, loss = 5.36426772\n",
      "Iteration 9512, loss = 5.35711808\n",
      "Iteration 9513, loss = 5.34764811\n",
      "Iteration 9514, loss = 5.33607348\n",
      "Iteration 9515, loss = 5.32961129\n",
      "Iteration 9516, loss = 5.32041672\n",
      "Iteration 9517, loss = 5.30928400\n",
      "Iteration 9518, loss = 5.30257082\n",
      "Iteration 9519, loss = 5.29371499\n",
      "Iteration 9520, loss = 5.29214085\n",
      "Iteration 9521, loss = 5.28755961\n",
      "Iteration 9522, loss = 5.27601325\n",
      "Iteration 9523, loss = 5.27107268\n",
      "Iteration 9524, loss = 5.25582041\n",
      "Iteration 9525, loss = 5.24593424\n",
      "Iteration 9526, loss = 5.23268353\n",
      "Iteration 9527, loss = 5.22561206\n",
      "Iteration 9528, loss = 5.22522688\n",
      "Iteration 9529, loss = 5.21609205\n",
      "Iteration 9530, loss = 5.20096678\n",
      "Iteration 9531, loss = 5.19727538\n",
      "Iteration 9532, loss = 5.18518351\n",
      "Iteration 9533, loss = 5.17856681\n",
      "Iteration 9534, loss = 5.17059248\n",
      "Iteration 9535, loss = 5.15756655\n",
      "Iteration 9536, loss = 5.15358780\n",
      "Iteration 9537, loss = 5.14363850\n",
      "Iteration 9538, loss = 5.14057522\n",
      "Iteration 9539, loss = 5.12750194\n",
      "Iteration 9540, loss = 5.12721625\n",
      "Iteration 9541, loss = 5.11231333\n",
      "Iteration 9542, loss = 5.10439597\n",
      "Iteration 9543, loss = 5.09658696\n",
      "Iteration 9544, loss = 5.09398969\n",
      "Iteration 9545, loss = 5.07798354\n",
      "Iteration 9546, loss = 5.08006059\n",
      "Iteration 9547, loss = 5.06458500\n",
      "Iteration 9548, loss = 5.05475429\n",
      "Iteration 9549, loss = 5.05555220\n",
      "Iteration 9550, loss = 5.04451675\n",
      "Iteration 9551, loss = 5.03618575\n",
      "Iteration 9552, loss = 5.02532594\n",
      "Iteration 9553, loss = 5.02057577\n",
      "Iteration 9554, loss = 5.01329577\n",
      "Iteration 9555, loss = 5.00075899\n",
      "Iteration 9556, loss = 4.99945890\n",
      "Iteration 9557, loss = 4.98590823\n",
      "Iteration 9558, loss = 4.98275648\n",
      "Iteration 9559, loss = 4.97895416\n",
      "Iteration 9560, loss = 4.96617428\n",
      "Iteration 9561, loss = 4.96861213\n",
      "Iteration 9562, loss = 4.95041866\n",
      "Iteration 9563, loss = 4.94176209\n",
      "Iteration 9564, loss = 4.93264051\n",
      "Iteration 9565, loss = 4.92746592\n",
      "Iteration 9566, loss = 4.91774243\n",
      "Iteration 9567, loss = 4.91196614\n",
      "Iteration 9568, loss = 4.90270472\n",
      "Iteration 9569, loss = 4.89551178\n",
      "Iteration 9570, loss = 4.88825192\n",
      "Iteration 9571, loss = 4.88540279\n",
      "Iteration 9572, loss = 4.87550181\n",
      "Iteration 9573, loss = 4.86895684\n",
      "Iteration 9574, loss = 4.86108928\n",
      "Iteration 9575, loss = 4.85464276\n",
      "Iteration 9576, loss = 4.84852587\n",
      "Iteration 9577, loss = 4.83939938\n",
      "Iteration 9578, loss = 4.82709119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9579, loss = 4.82210094\n",
      "Iteration 9580, loss = 4.82709555\n",
      "Iteration 9581, loss = 4.80760215\n",
      "Iteration 9582, loss = 4.80154423\n",
      "Iteration 9583, loss = 4.79292962\n",
      "Iteration 9584, loss = 4.78810365\n",
      "Iteration 9585, loss = 4.77809188\n",
      "Iteration 9586, loss = 4.77948303\n",
      "Iteration 9587, loss = 4.76732018\n",
      "Iteration 9588, loss = 4.76025482\n",
      "Iteration 9589, loss = 4.75152059\n",
      "Iteration 9590, loss = 4.74407492\n",
      "Iteration 9591, loss = 4.74161060\n",
      "Iteration 9592, loss = 4.73556319\n",
      "Iteration 9593, loss = 4.72149621\n",
      "Iteration 9594, loss = 4.71910753\n",
      "Iteration 9595, loss = 4.71000257\n",
      "Iteration 9596, loss = 4.70381981\n",
      "Iteration 9597, loss = 4.69613090\n",
      "Iteration 9598, loss = 4.69406793\n",
      "Iteration 9599, loss = 4.68803303\n",
      "Iteration 9600, loss = 4.69743774\n",
      "Iteration 9601, loss = 4.66930683\n",
      "Iteration 9602, loss = 4.66622181\n",
      "Iteration 9603, loss = 4.65503075\n",
      "Iteration 9604, loss = 4.65814640\n",
      "Iteration 9605, loss = 4.64247403\n",
      "Iteration 9606, loss = 4.63931725\n",
      "Iteration 9607, loss = 4.63233774\n",
      "Iteration 9608, loss = 4.62534130\n",
      "Iteration 9609, loss = 4.61716575\n",
      "Iteration 9610, loss = 4.61368889\n",
      "Iteration 9611, loss = 4.61095148\n",
      "Iteration 9612, loss = 4.59698544\n",
      "Iteration 9613, loss = 4.59222157\n",
      "Iteration 9614, loss = 4.58422612\n",
      "Iteration 9615, loss = 4.57778175\n",
      "Iteration 9616, loss = 4.57189719\n",
      "Iteration 9617, loss = 4.56611598\n",
      "Iteration 9618, loss = 4.55903608\n",
      "Iteration 9619, loss = 4.55768650\n",
      "Iteration 9620, loss = 4.54237729\n",
      "Iteration 9621, loss = 4.54390754\n",
      "Iteration 9622, loss = 4.53764219\n",
      "Iteration 9623, loss = 4.52974177\n",
      "Iteration 9624, loss = 4.52186933\n",
      "Iteration 9625, loss = 4.51846802\n",
      "Iteration 9626, loss = 4.50929812\n",
      "Iteration 9627, loss = 4.50428469\n",
      "Iteration 9628, loss = 4.49838318\n",
      "Iteration 9629, loss = 4.48746448\n",
      "Iteration 9630, loss = 4.48670693\n",
      "Iteration 9631, loss = 4.48162270\n",
      "Iteration 9632, loss = 4.47696943\n",
      "Iteration 9633, loss = 4.46967772\n",
      "Iteration 9634, loss = 4.45846818\n",
      "Iteration 9635, loss = 4.45852405\n",
      "Iteration 9636, loss = 4.44895948\n",
      "Iteration 9637, loss = 4.44382375\n",
      "Iteration 9638, loss = 4.43619287\n",
      "Iteration 9639, loss = 4.42806765\n",
      "Iteration 9640, loss = 4.42552899\n",
      "Iteration 9641, loss = 4.42069415\n",
      "Iteration 9642, loss = 4.41591562\n",
      "Iteration 9643, loss = 4.40993898\n",
      "Iteration 9644, loss = 4.40264218\n",
      "Iteration 9645, loss = 4.40201198\n",
      "Iteration 9646, loss = 4.38901915\n",
      "Iteration 9647, loss = 4.38727951\n",
      "Iteration 9648, loss = 4.38075872\n",
      "Iteration 9649, loss = 4.37266418\n",
      "Iteration 9650, loss = 4.36712868\n",
      "Iteration 9651, loss = 4.36295156\n",
      "Iteration 9652, loss = 4.35762296\n",
      "Iteration 9653, loss = 4.35096960\n",
      "Iteration 9654, loss = 4.34436967\n",
      "Iteration 9655, loss = 4.33463553\n",
      "Iteration 9656, loss = 4.32963079\n",
      "Iteration 9657, loss = 4.32677058\n",
      "Iteration 9658, loss = 4.32458550\n",
      "Iteration 9659, loss = 4.30982994\n",
      "Iteration 9660, loss = 4.31151502\n",
      "Iteration 9661, loss = 4.30438840\n",
      "Iteration 9662, loss = 4.29497542\n",
      "Iteration 9663, loss = 4.29398609\n",
      "Iteration 9664, loss = 4.28783982\n",
      "Iteration 9665, loss = 4.29239262\n",
      "Iteration 9666, loss = 4.26743661\n",
      "Iteration 9667, loss = 4.27581374\n",
      "Iteration 9668, loss = 4.26346822\n",
      "Iteration 9669, loss = 4.25828199\n",
      "Iteration 9670, loss = 4.25140235\n",
      "Iteration 9671, loss = 4.25072916\n",
      "Iteration 9672, loss = 4.25179732\n",
      "Iteration 9673, loss = 4.23647910\n",
      "Iteration 9674, loss = 4.23824838\n",
      "Iteration 9675, loss = 4.22329702\n",
      "Iteration 9676, loss = 4.21993022\n",
      "Iteration 9677, loss = 4.22263021\n",
      "Iteration 9678, loss = 4.21022791\n",
      "Iteration 9679, loss = 4.20894949\n",
      "Iteration 9680, loss = 4.19886403\n",
      "Iteration 9681, loss = 4.19799423\n",
      "Iteration 9682, loss = 4.18466166\n",
      "Iteration 9683, loss = 4.19245168\n",
      "Iteration 9684, loss = 4.17607423\n",
      "Iteration 9685, loss = 4.17722314\n",
      "Iteration 9686, loss = 4.16402889\n",
      "Iteration 9687, loss = 4.16409099\n",
      "Iteration 9688, loss = 4.15695486\n",
      "Iteration 9689, loss = 4.15679414\n",
      "Iteration 9690, loss = 4.14755749\n",
      "Iteration 9691, loss = 4.14412181\n",
      "Iteration 9692, loss = 4.14196618\n",
      "Iteration 9693, loss = 4.13569602\n",
      "Iteration 9694, loss = 4.12941642\n",
      "Iteration 9695, loss = 4.12445792\n",
      "Iteration 9696, loss = 4.11430012\n",
      "Iteration 9697, loss = 4.11544612\n",
      "Iteration 9698, loss = 4.10812175\n",
      "Iteration 9699, loss = 4.09910174\n",
      "Iteration 9700, loss = 4.10424956\n",
      "Iteration 9701, loss = 4.09212171\n",
      "Iteration 9702, loss = 4.09243472\n",
      "Iteration 9703, loss = 4.08540684\n",
      "Iteration 9704, loss = 4.07922276\n",
      "Iteration 9705, loss = 4.07407465\n",
      "Iteration 9706, loss = 4.06583278\n",
      "Iteration 9707, loss = 4.06526962\n",
      "Iteration 9708, loss = 4.05853120\n",
      "Iteration 9709, loss = 4.05052901\n",
      "Iteration 9710, loss = 4.05156025\n",
      "Iteration 9711, loss = 4.04154722\n",
      "Iteration 9712, loss = 4.04185814\n",
      "Iteration 9713, loss = 4.03244194\n",
      "Iteration 9714, loss = 4.03149142\n",
      "Iteration 9715, loss = 4.02560675\n",
      "Iteration 9716, loss = 4.02488293\n",
      "Iteration 9717, loss = 4.01633502\n",
      "Iteration 9718, loss = 4.01623628\n",
      "Iteration 9719, loss = 4.01327571\n",
      "Iteration 9720, loss = 3.99823716\n",
      "Iteration 9721, loss = 4.00736275\n",
      "Iteration 9722, loss = 3.99947261\n",
      "Iteration 9723, loss = 3.98572246\n",
      "Iteration 9724, loss = 3.98327126\n",
      "Iteration 9725, loss = 3.97888422\n",
      "Iteration 9726, loss = 3.97350542\n",
      "Iteration 9727, loss = 3.97476335\n",
      "Iteration 9728, loss = 3.97356271\n",
      "Iteration 9729, loss = 3.96745167\n",
      "Iteration 9730, loss = 3.95651582\n",
      "Iteration 9731, loss = 3.95255320\n",
      "Iteration 9732, loss = 3.94795151\n",
      "Iteration 9733, loss = 3.94385272\n",
      "Iteration 9734, loss = 3.94005442\n",
      "Iteration 9735, loss = 3.93493400\n",
      "Iteration 9736, loss = 3.93048763\n",
      "Iteration 9737, loss = 3.92599667\n",
      "Iteration 9738, loss = 3.91817492\n",
      "Iteration 9739, loss = 3.91733773\n",
      "Iteration 9740, loss = 3.91379425\n",
      "Iteration 9741, loss = 3.91048368\n",
      "Iteration 9742, loss = 3.91218972\n",
      "Iteration 9743, loss = 3.89534335\n",
      "Iteration 9744, loss = 3.89406258\n",
      "Iteration 9745, loss = 3.89073128\n",
      "Iteration 9746, loss = 3.89324631\n",
      "Iteration 9747, loss = 3.89183347\n",
      "Iteration 9748, loss = 3.88378274\n",
      "Iteration 9749, loss = 3.88272314\n",
      "Iteration 9750, loss = 3.86921295\n",
      "Iteration 9751, loss = 3.86719952\n",
      "Iteration 9752, loss = 3.86135681\n",
      "Iteration 9753, loss = 3.85902322\n",
      "Iteration 9754, loss = 3.85647414\n",
      "Iteration 9755, loss = 3.84953754\n",
      "Iteration 9756, loss = 3.84411936\n",
      "Iteration 9757, loss = 3.83730869\n",
      "Iteration 9758, loss = 3.83657967\n",
      "Iteration 9759, loss = 3.82977288\n",
      "Iteration 9760, loss = 3.82748388\n",
      "Iteration 9761, loss = 3.82155848\n",
      "Iteration 9762, loss = 3.82170886\n",
      "Iteration 9763, loss = 3.81420304\n",
      "Iteration 9764, loss = 3.81330552\n",
      "Iteration 9765, loss = 3.80338147\n",
      "Iteration 9766, loss = 3.80478528\n",
      "Iteration 9767, loss = 3.79822750\n",
      "Iteration 9768, loss = 3.79253631\n",
      "Iteration 9769, loss = 3.79678612\n",
      "Iteration 9770, loss = 3.79058163\n",
      "Iteration 9771, loss = 3.78387355\n",
      "Iteration 9772, loss = 3.77559414\n",
      "Iteration 9773, loss = 3.77547972\n",
      "Iteration 9774, loss = 3.76662593\n",
      "Iteration 9775, loss = 3.76769126\n",
      "Iteration 9776, loss = 3.76487631\n",
      "Iteration 9777, loss = 3.76173514\n",
      "Iteration 9778, loss = 3.75311230\n",
      "Iteration 9779, loss = 3.74980441\n",
      "Iteration 9780, loss = 3.74545522\n",
      "Iteration 9781, loss = 3.75249524\n",
      "Iteration 9782, loss = 3.74585808\n",
      "Iteration 9783, loss = 3.73690348\n",
      "Iteration 9784, loss = 3.73016039\n",
      "Iteration 9785, loss = 3.73614547\n",
      "Iteration 9786, loss = 3.72630366\n",
      "Iteration 9787, loss = 3.71899551\n",
      "Iteration 9788, loss = 3.71572034\n",
      "Iteration 9789, loss = 3.71404939\n",
      "Iteration 9790, loss = 3.71218033\n",
      "Iteration 9791, loss = 3.70670230\n",
      "Iteration 9792, loss = 3.70090542\n",
      "Iteration 9793, loss = 3.69927475\n",
      "Iteration 9794, loss = 3.69433056\n",
      "Iteration 9795, loss = 3.69203882\n",
      "Iteration 9796, loss = 3.69170150\n",
      "Iteration 9797, loss = 3.68281398\n",
      "Iteration 9798, loss = 3.67882299\n",
      "Iteration 9799, loss = 3.68475476\n",
      "Iteration 9800, loss = 3.67237009\n",
      "Iteration 9801, loss = 3.66565898\n",
      "Iteration 9802, loss = 3.66304852\n",
      "Iteration 9803, loss = 3.66526166\n",
      "Iteration 9804, loss = 3.66598469\n",
      "Iteration 9805, loss = 3.65410709\n",
      "Iteration 9806, loss = 3.64783062\n",
      "Iteration 9807, loss = 3.64789169\n",
      "Iteration 9808, loss = 3.65341498\n",
      "Iteration 9809, loss = 3.63675662\n",
      "Iteration 9810, loss = 3.63431186\n",
      "Iteration 9811, loss = 3.63999602\n",
      "Iteration 9812, loss = 3.62656941\n",
      "Iteration 9813, loss = 3.63013431\n",
      "Iteration 9814, loss = 3.62334452\n",
      "Iteration 9815, loss = 3.61945280\n",
      "Iteration 9816, loss = 3.61779230\n",
      "Iteration 9817, loss = 3.60811553\n",
      "Iteration 9818, loss = 3.61016443\n",
      "Iteration 9819, loss = 3.60274360\n",
      "Iteration 9820, loss = 3.60456194\n",
      "Iteration 9821, loss = 3.60957622\n",
      "Iteration 9822, loss = 3.59409941\n",
      "Iteration 9823, loss = 3.59606518\n",
      "Iteration 9824, loss = 3.58837934\n",
      "Iteration 9825, loss = 3.59207764\n",
      "Iteration 9826, loss = 3.59029900\n",
      "Iteration 9827, loss = 3.57450677\n",
      "Iteration 9828, loss = 3.57284293\n",
      "Iteration 9829, loss = 3.57190976\n",
      "Iteration 9830, loss = 3.57515372\n",
      "Iteration 9831, loss = 3.56368865\n",
      "Iteration 9832, loss = 3.56411128\n",
      "Iteration 9833, loss = 3.56337499\n",
      "Iteration 9834, loss = 3.56752435\n",
      "Iteration 9835, loss = 3.56392552\n",
      "Iteration 9836, loss = 3.55172983\n",
      "Iteration 9837, loss = 3.54449279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9838, loss = 3.54363935\n",
      "Iteration 9839, loss = 3.53973372\n",
      "Iteration 9840, loss = 3.53659900\n",
      "Iteration 9841, loss = 3.53298619\n",
      "Iteration 9842, loss = 3.52866635\n",
      "Iteration 9843, loss = 3.52553529\n",
      "Iteration 9844, loss = 3.52866387\n",
      "Iteration 9845, loss = 3.51708097\n",
      "Iteration 9846, loss = 3.51857609\n",
      "Iteration 9847, loss = 3.51101767\n",
      "Iteration 9848, loss = 3.51921297\n",
      "Iteration 9849, loss = 3.51651499\n",
      "Iteration 9850, loss = 3.50524567\n",
      "Iteration 9851, loss = 3.50095042\n",
      "Iteration 9852, loss = 3.49858264\n",
      "Iteration 9853, loss = 3.49304061\n",
      "Iteration 9854, loss = 3.49672042\n",
      "Iteration 9855, loss = 3.48848929\n",
      "Iteration 9856, loss = 3.48522659\n",
      "Iteration 9857, loss = 3.48091182\n",
      "Iteration 9858, loss = 3.48625720\n",
      "Iteration 9859, loss = 3.47590582\n",
      "Iteration 9860, loss = 3.47598436\n",
      "Iteration 9861, loss = 3.46992285\n",
      "Iteration 9862, loss = 3.46961004\n",
      "Iteration 9863, loss = 3.46582479\n",
      "Iteration 9864, loss = 3.45882204\n",
      "Iteration 9865, loss = 3.46383363\n",
      "Iteration 9866, loss = 3.45140118\n",
      "Iteration 9867, loss = 3.45569475\n",
      "Iteration 9868, loss = 3.44997171\n",
      "Iteration 9869, loss = 3.44808723\n",
      "Iteration 9870, loss = 3.44567310\n",
      "Iteration 9871, loss = 3.43878658\n",
      "Iteration 9872, loss = 3.44685768\n",
      "Iteration 9873, loss = 3.44458237\n",
      "Iteration 9874, loss = 3.43233223\n",
      "Iteration 9875, loss = 3.43207775\n",
      "Iteration 9876, loss = 3.42697340\n",
      "Iteration 9877, loss = 3.42057741\n",
      "Iteration 9878, loss = 3.42175354\n",
      "Iteration 9879, loss = 3.41935044\n",
      "Iteration 9880, loss = 3.41729053\n",
      "Iteration 9881, loss = 3.41116103\n",
      "Iteration 9882, loss = 3.41180460\n",
      "Iteration 9883, loss = 3.40530877\n",
      "Iteration 9884, loss = 3.40309989\n",
      "Iteration 9885, loss = 3.40487959\n",
      "Iteration 9886, loss = 3.39870150\n",
      "Iteration 9887, loss = 3.39426050\n",
      "Iteration 9888, loss = 3.39858468\n",
      "Iteration 9889, loss = 3.39120359\n",
      "Iteration 9890, loss = 3.38641962\n",
      "Iteration 9891, loss = 3.38364737\n",
      "Iteration 9892, loss = 3.38179808\n",
      "Iteration 9893, loss = 3.37801005\n",
      "Iteration 9894, loss = 3.37631970\n",
      "Iteration 9895, loss = 3.38064140\n",
      "Iteration 9896, loss = 3.37095966\n",
      "Iteration 9897, loss = 3.36754880\n",
      "Iteration 9898, loss = 3.36307913\n",
      "Iteration 9899, loss = 3.36734231\n",
      "Iteration 9900, loss = 3.36430141\n",
      "Iteration 9901, loss = 3.36020421\n",
      "Iteration 9902, loss = 3.35493332\n",
      "Iteration 9903, loss = 3.35321958\n",
      "Iteration 9904, loss = 3.35084322\n",
      "Iteration 9905, loss = 3.35199224\n",
      "Iteration 9906, loss = 3.34273314\n",
      "Iteration 9907, loss = 3.34922917\n",
      "Iteration 9908, loss = 3.33827542\n",
      "Iteration 9909, loss = 3.33926537\n",
      "Iteration 9910, loss = 3.33287582\n",
      "Iteration 9911, loss = 3.33654055\n",
      "Iteration 9912, loss = 3.32963508\n",
      "Iteration 9913, loss = 3.32749293\n",
      "Iteration 9914, loss = 3.32485020\n",
      "Iteration 9915, loss = 3.31827144\n",
      "Iteration 9916, loss = 3.32302060\n",
      "Iteration 9917, loss = 3.31939801\n",
      "Iteration 9918, loss = 3.31289378\n",
      "Iteration 9919, loss = 3.31670770\n",
      "Iteration 9920, loss = 3.31448740\n",
      "Iteration 9921, loss = 3.30775363\n",
      "Iteration 9922, loss = 3.30131046\n",
      "Iteration 9923, loss = 3.30666554\n",
      "Iteration 9924, loss = 3.30184879\n",
      "Iteration 9925, loss = 3.30110641\n",
      "Iteration 9926, loss = 3.29206237\n",
      "Iteration 9927, loss = 3.29389871\n",
      "Iteration 9928, loss = 3.29733771\n",
      "Iteration 9929, loss = 3.30477917\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce076b48245d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#http://www.machinelearningtutorial.net/2017/01/28/python-scikit-simple-function-approximation/\n",
    "\n",
    "#don't for get the seed, solutions are differnig without a set seed\n",
    "np.random.seed(3)\n",
    "n = 20\n",
    "x = np.random.uniform(-15, 15, size = n)\n",
    "#y is a predictor for x, the random thing is noise added to the data\n",
    "y = x**2 + 2*np.random.randn(n, )\n",
    "X = np.reshape(x ,[n, 1]) \n",
    "y = np.reshape(y ,[n ,])\n",
    "\n",
    "#if verbose false, results won't show\n",
    "clf = MLPRegressor(alpha=0.001, hidden_layer_sizes = (10,), max_iter = 50000, \n",
    "                 activation = 'logistic', verbose = 'True', learning_rate = 'adaptive')\n",
    "\n",
    "\"\"\"\n",
    "clf = MLPRegressor(alpha=0.1, hidden_layer_sizes = (10,), max_iter = 50000, \n",
    "                 activation = 'logistic', verbose = 'True', learning_rate = 'adaptive')\n",
    "\"\"\"\n",
    "a = clf.fit(X, y)\n",
    "\n",
    "mlp.fit(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>blue is equation, dots are x and y, green is the predictions. </font>\n",
    "\n",
    "### <font color='red'>the green is overfit, it tries to hit each dot</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6480c61c88>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_ = np.linspace(-15, 15, 160) # define axis\n",
    "\n",
    "pred_x = np.reshape(x_, [160, 1]) # [160, ] -> [160, 1]\n",
    "pred_y = clf.predict(pred_x) # predict network output given x_\n",
    "fig = plt.figure() \n",
    "plt.plot(x_, x_**2, color = 'b') # plot original function\n",
    "plt.scatter(x, y, color = 'r') # plot training data\n",
    "plt.plot(pred_x, pred_y, 'g') # plot network output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the above code, play on the hyperparameters, number of samples, and increase noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed n to 200\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#http://www.machinelearningtutorial.net/2017/01/28/python-scikit-simple-function-approximation/\n",
    "\n",
    "#don't for get the seed, solutions are differnig without a set seed\n",
    "np.random.seed(3)\n",
    "n = 2000\n",
    "x = np.random.uniform(-15, 15, size = n)\n",
    "#y is a predictor for x, the random thing is noise added to the data\n",
    "y = x**2 + 200*np.random.randn(n, )\n",
    "X = np.reshape(x ,[n, 1]) \n",
    "y = np.reshape(y ,[n ,])\n",
    "\n",
    "#if verbose false, results won't show\n",
    "clf = MLPRegressor(alpha=0.001, hidden_layer_sizes = (10,), max_iter = 50000, \n",
    "                 activation = 'logistic', verbose = 'True', learning_rate = 'adaptive')\n",
    "\n",
    "\"\"\"\n",
    "clf = MLPRegressor(alpha=0.1, hidden_layer_sizes = (10,), max_iter = 50000, \n",
    "                 activation = 'logistic', verbose = 'True', learning_rate = 'adaptive')\n",
    "\"\"\"\n",
    "a = clf.fit(X, y)\n",
    "\n",
    "mlp.fit(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>change n to 200, converged faster, before max n, then green was correct</font>\n",
    "\n",
    "### <font color='red'>changed n back to 20 then changed noise from 2 to 200*, would not converge, green again overfit</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0c58f807f0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_ = np.linspace(-15, 15, 160) # define axis\n",
    "\n",
    "pred_x = np.reshape(x_, [160, 1]) # [160, ] -> [160, 1]\n",
    "pred_y = clf.predict(pred_x) # predict network output given x_\n",
    "fig = plt.figure() \n",
    "plt.plot(x_, x_**2, color = 'b') # plot original function\n",
    "plt.scatter(x, y, color = 'r') # plot training data\n",
    "plt.plot(pred_x, pred_y, 'g') # plot network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
